{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a0c80d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import sys\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n",
    "from multiprocessing import Process\n",
    "import gc\n",
    "\n",
    "import flwr as fl\n",
    "from flwr.server.strategy import FedAvg\n",
    "import tensorflow as tf\n",
    "import sklearn\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "\n",
    "import collections\n",
    "\n",
    "#!export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX/lib/\n",
    "# demonstration of calculating metrics for a neural network model using sklearn\n",
    "from sklearn.datasets import make_circles\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from flwr.common.logger import log\n",
    "from flwr.server.client_manager import ClientManager\n",
    "from flwr.server.client_proxy import ClientProxy\n",
    "from flwr.common import (\n",
    "    EvaluateIns,\n",
    "    EvaluateRes,\n",
    "    FitIns,\n",
    "    FitRes,\n",
    "    MetricsAggregationFn,\n",
    "    NDArrays,\n",
    "    Parameters,\n",
    "    Scalar,\n",
    "    ndarrays_to_parameters,\n",
    "    parameters_to_ndarrays,\n",
    ")\n",
    "\n",
    "# Make TensorFlow log less verbose\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b2257ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total arguments passed: 3\n",
      "arg: /home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ipykernel_launcher.py\n",
      "arg: -f\n",
      "arg: /home/guilherme/.local/share/jupyter/runtime/kernel-980930ae-3b38-4aa0-8d2a-bcb9e3320608.json\n",
      "iteracoes: 0\n",
      "cycle: 0\n"
     ]
    }
   ],
   "source": [
    "# argumentos\n",
    "n = len(sys.argv)\n",
    "print(\"Total arguments passed:\", n)\n",
    "iteracoes = 0\n",
    "cycle_index = 0\n",
    "finalIterations = 0\n",
    "checkpoint_iteration = 0\n",
    "if(n > 0):\n",
    "    for value in sys.argv:\n",
    "        print(\"arg:\", value)\n",
    "        if(\"iterations=\" in value):\n",
    "            try:\n",
    "                iteracoes = int(value.replace(\"iterations=\",\"\"))\n",
    "            except:\n",
    "                print(\"no\")\n",
    "        \n",
    "        if(\"cycle=\" in value):\n",
    "            try:\n",
    "                cycle_index = int(value.replace(\"cycle=\",\"\"))\n",
    "            except:\n",
    "                print(\"no\")\n",
    "        if(\"checkpoint_iteration=\" in value):\n",
    "            try:\n",
    "                iteracoes = int(value.replace(\"iterations=\",\"\"))\n",
    "            except:\n",
    "                print(\"no\")\n",
    "print(\"iteracoes:\",iteracoes)      \n",
    "print(\"cycle:\",cycle_index)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2a4484bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input folder\n",
    "#inputFolders = \"../02-transformed-data-new-testes/dados2019/\"\n",
    "inputFolderPath = \"../data_2019_processed/\"\n",
    "baseFolderWeek = inputFolderPath+\"w\"\n",
    "\n",
    "WEEKS_TEST= [3]\n",
    "#WEEKS_TEST= [2]\n",
    "#WEEKS_TRAIN = [0,1,2]\n",
    "WEEKS_TRAIN = [0]\n",
    "\n",
    "\n",
    "# General configuration\n",
    "NUMBER_OF_ITERATIONS_FINAL = 200\n",
    "NUMBER_OF_ITERATIONS_FINAL = 10\n",
    "    \n",
    "NUM_EPOCHS = 1\n",
    "BATCH_SIZE = 32\n",
    "VERBOSE = 0\n",
    "\n",
    "# usado para experimentos\n",
    "if(iteracoes > 0):\n",
    "    NUMBER_OF_ITERATIONS_FINAL = iteracoes\n",
    "    \n",
    "NUMBER_OF_ITERATIONS = NUMBER_OF_ITERATIONS_FINAL\n",
    "\n",
    "# output folder\n",
    "outputFolder = \"result_unbalanced_epoch_\"+str(NUM_EPOCHS)+\"_rounds_\"+str(NUMBER_OF_ITERATIONS_FINAL)+\"_cycle_\"+str(cycle_index)\n",
    "\n",
    "#outputFolder = \"test_checkpoint\"\n",
    "checkPointFolder = outputFolder+\"/checkpoints\"\n",
    "\n",
    "# last cycle\n",
    "last_cycle_index = cycle_index - 1\n",
    "lastCycleOutputFolder = \"result_unbalanced_epoch_\"+str(NUM_EPOCHS)+\"_rounds_\"+str(200)+\"_cycle_\"+str(last_cycle_index)\n",
    "lastCycleOutputFolder = \"result_unbalanced_epoch_\"+str(NUM_EPOCHS)+\"_rounds_\"+str(NUMBER_OF_ITERATIONS_FINAL)+\"_cycle_\"+str(last_cycle_index)\n",
    "iferredCycleDataFolder = lastCycleOutputFolder+\"/inferred_datasets\"\n",
    "\n",
    "# train file name modifier\n",
    "fileSufixTrain = \"_transformed\" # _transformed_smote for smote\n",
    "\n",
    "fl.common.logger.configure(identifier=\"myFlowerExperiment\", filename=\"log_\"+outputFolder+\".txt\")\n",
    "\n",
    "# usado para checkpoints\n",
    "if(checkpoint_iteration > 0):\n",
    "    NUMBER_OF_ITERATIONS_FINAL = checkpoint_iteration\n",
    "    \n",
    "NUMBER_OF_ITERATIONS = NUMBER_OF_ITERATIONS_FINAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b0929b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether the folder exists or not\n",
      "The directory exists!\n"
     ]
    }
   ],
   "source": [
    "print(\"Checking whether the folder exists or not\")\n",
    "isExist = os.path.exists(outputFolder)\n",
    "if not isExist:\n",
    "    # Create a new directory because it does not exist\n",
    "    os.makedirs(outputFolder)\n",
    "    print(\"The new directory is created!\")\n",
    "else:\n",
    "    print(\"The directory exists!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5249b935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether the checkpoint folder exists or not\n",
      "The checkpoint directory exists!\n"
     ]
    }
   ],
   "source": [
    "print(\"Checking whether the checkpoint folder exists or not\")\n",
    "isExist = os.path.exists(checkPointFolder)\n",
    "if not isExist:\n",
    "    # Create a new directory because it does not exist\n",
    "    os.makedirs(checkPointFolder)\n",
    "    print(\"The new checkpoint directory is created!\")\n",
    "else:\n",
    "    print(\"The checkpoint directory exists!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "58203650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check whether the cycle is 0 > or not, if so, the folder of inference must exist\n"
     ]
    }
   ],
   "source": [
    "print(\"check whether the cycle is 0 > or not, if so, the folder of inference must exist\")\n",
    "if(cycle_index > 0):\n",
    "    isExist = os.path.exists(iferredCycleDataFolder)\n",
    "    print(iferredCycleDataFolder)\n",
    "    if not isExist:\n",
    "        print(\"The folder of inference not exists!\")\n",
    "        sys.exit(\"The folder of inference not exists!\")\n",
    "    else:\n",
    "        print(\"The checkpoint directory exists!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e75b3967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected features\n",
    "inputFeatures = [\"activity\",\"location\",\"day_of_week\",\"light\",\"phone_lock\",\"proximity\",\"sound\",\"time_to_next_alarm\", \"minutes_day\"]\n",
    "outputClasses = [\"awake\",\"asleep\"]\n",
    "#outputClasses = [\"class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4f7fa6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# client datasets used on the training process (75% of data)\n",
    "trainFolders =  ['0Jf4TH9Zzse0Z1Jjh7SnTOe2MMzeSnFi7feTnkG6vgs',\n",
    "                '0tdmm6rwW3KquQ73ATYYJ5JkpMtvbppJ0VzA2GExdA', \n",
    "                '2cyV53lVyUtlMj0BRwilEWtYJwUiviYoL48cZBPBq0', \n",
    "                '2J22RukYnEbKTk7t+iUVDBkorcyL5NKN6TrLe89ys', \n",
    "                #['5FLZBTVAPwdq9QezHE2sVCJIs7p+r6mCemA2gp9jATk'], #does not have the file\n",
    "                '7EYF5I04EVqisUJCVNHlqn77UAuOmwL2Dahxd3cA', \n",
    "                'a9Qgj8ENWrHvl9QqlXcIPKmyGMKgbfHk9Dbqon1HQP4', \n",
    "                'ae4JJBZDycEcY8McJF+3BxyvZ1619y03BNdCxzpZTc', \n",
    "                'Ch3u5Oaz96VSrQbf0z31X6jEIbeIekkC0mwPzCdeJ1U', \n",
    "                'CH8f0yZkZL13zWuE9ks1CkVJRVrr+jsGdUXHrZ6YeA', \n",
    "                'DHO1K4jgiwZJOfQTrxvKE2vn7hkjamigroGD5IaeRc', \n",
    "                #'DHPqzSqSttiba1L3BD1cptNJPjSxZ8rXxF9mY3za6WA', # does not have asleep data\n",
    "                'dQEFscjqnIlug8Tgq97JohhSQPG2DEOWJqS86wCrcY', \n",
    "                'HFvs2CohmhHte+AaCzFasjzegGzxZKPhkrX23iI6Xo', \n",
    "                'jgB9E8v3Z6PKdTRTCMAijBllA9YEMtrmHbe4qsbmJWw', \n",
    "                'JkY++R7E8myldLN3on6iQ78Ee78zCbrLuggfwGju3I', \n",
    "                'K4SLohf+TN1Ak8Dn8iE3Lme7rEMPISfppB2sXfHX8', \n",
    "                'oGaWetJJJEWHuvYdWYo826SQxfhCExVVQ2da8LE1Y7Q', \n",
    "                'pyt24oiDAHsmgWMvkFKz2fn2pwcHiXchd6KchLM', \n",
    "                #'PZCf1nfvhR+6fk+7+sPNMYOgb8BAMmtQtfoRS83Suc', # does not have asleep data\n",
    "                'QUNCATForxzK0HHw46LrGOMWh0eVA8Y5XWEiUXX+cQ', \n",
    "                #'rIl2UK9+bQ+tzpFdbJAdbBxEa5GbgrgC030yEaENLw', \n",
    "                #'RoBW3cDOO9wWRMPO2twQff83MPc+OXn6gJ+a1DafreI', \n",
    "                'SH3kQeyd5volraxw8vOyhlowNqWBPr1IJ9URNXUL4']\n",
    "                #'VVpwFNMrEglveh6MDN8lrRzTy5OwzglD4FURfM4A2is', \n",
    "                #'Wa1mcNmbh66S7VS6GIzyfCFMD3SGhbtDQyFP1ywJEsw', \n",
    "                #'XCKRE0BWRHxfP1kZIihgtT+jUjSp2GE8v5ZlhcIhVmA', \n",
    "                #'YI5Y79K6GXqAUoGP6PNyII8WKlAoel4urDxWSVVOvBw', \n",
    "                #'ypklj+8GJ15rOIH1lpKQtFJOuK+VdvyCuBPqhY3aoM', \n",
    "                #'ZSsAZ0Pq+MCqFrnjsRFn5Ua09pMCVaOV9c8ZuYb7XQY']\n",
    "            \n",
    "# client datasets used on the training process (25% of data)\n",
    "testFolders =  [#'0Jf4TH9Zzse0Z1Jjh7SnTOe2MMzeSnFi7feTnkG6vgs',\n",
    "                #'0tdmm6rwW3KquQ73ATYYJ5JkpMtvbppJ0VzA2GExdA', \n",
    "                #'2cyV53lVyUtlMj0BRwilEWtYJwUiviYoL48cZBPBq0', \n",
    "                #'2J22RukYnEbKTk7t+iUVDBkorcyL5NKN6TrLe89ys', \n",
    "                #['5FLZBTVAPwdq9QezHE2sVCJIs7p+r6mCemA2gp9jATk'], #does not have the file\n",
    "                #'7EYF5I04EVqisUJCVNHlqn77UAuOmwL2Dahxd3cA', \n",
    "                #'a9Qgj8ENWrHvl9QqlXcIPKmyGMKgbfHk9Dbqon1HQP4', \n",
    "                #'ae4JJBZDycEcY8McJF+3BxyvZ1619y03BNdCxzpZTc', \n",
    "                #'Ch3u5Oaz96VSrQbf0z31X6jEIbeIekkC0mwPzCdeJ1U', \n",
    "                #'CH8f0yZkZL13zWuE9ks1CkVJRVrr+jsGdUXHrZ6YeA', \n",
    "                #'DHO1K4jgiwZJOfQTrxvKE2vn7hkjamigroGD5IaeRc', \n",
    "                #'DHPqzSqSttiba1L3BD1cptNJPjSxZ8rXxF9mY3za6WA', # does not have asleep data\n",
    "                #'dQEFscjqnIlug8Tgq97JohhSQPG2DEOWJqS86wCrcY', \n",
    "                #'HFvs2CohmhHte+AaCzFasjzegGzxZKPhkrX23iI6Xo', \n",
    "                #'jgB9E8v3Z6PKdTRTCMAijBllA9YEMtrmHbe4qsbmJWw', \n",
    "                #'JkY++R7E8myldLN3on6iQ78Ee78zCbrLuggfwGju3I', \n",
    "                #'K4SLohf+TN1Ak8Dn8iE3Lme7rEMPISfppB2sXfHX8', \n",
    "                #'oGaWetJJJEWHuvYdWYo826SQxfhCExVVQ2da8LE1Y7Q', \n",
    "                #'pyt24oiDAHsmgWMvkFKz2fn2pwcHiXchd6KchLM', \n",
    "                #'PZCf1nfvhR+6fk+7+sPNMYOgb8BAMmtQtfoRS83Suc', # does not have asleep data\n",
    "                #'QUNCATForxzK0HHw46LrGOMWh0eVA8Y5XWEiUXX+cQ', \n",
    "                'rIl2UK9+bQ+tzpFdbJAdbBxEa5GbgrgC030yEaENLw', \n",
    "                'RoBW3cDOO9wWRMPO2twQff83MPc+OXn6gJ+a1DafreI', \n",
    "                #'SH3kQeyd5volraxw8vOyhlowNqWBPr1IJ9URNXUL4'] \n",
    "                'VVpwFNMrEglveh6MDN8lrRzTy5OwzglD4FURfM4A2is', \n",
    "                'Wa1mcNmbh66S7VS6GIzyfCFMD3SGhbtDQyFP1ywJEsw', \n",
    "                'XCKRE0BWRHxfP1kZIihgtT+jUjSp2GE8v5ZlhcIhVmA', \n",
    "                'YI5Y79K6GXqAUoGP6PNyII8WKlAoel4urDxWSVVOvBw', \n",
    "                'ypklj+8GJ15rOIH1lpKQtFJOuK+VdvyCuBPqhY3aoM', \n",
    "                'ZSsAZ0Pq+MCqFrnjsRFn5Ua09pMCVaOV9c8ZuYb7XQY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f59ec6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "20a8bdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateMetrics(y_test,yhat_probs):\n",
    "    # predict crisp classes for test set deprecated\n",
    "    #yhat_classes = model.predict_classes(X_test, verbose=0)\n",
    "    #yhat_classes = np.argmax(yhat_probs,axis=1)\n",
    "    yhat_classes = yhat_probs.round()\n",
    "    # accuracy: (tp + tn) / (p + n)\n",
    "    accuracy = accuracy_score(y_test, yhat_classes)\n",
    "    # precision tp / (tp + fp)\n",
    "    precision = precision_score(y_test, yhat_classes)\n",
    "    # recall: tp / (tp + fn)\n",
    "    recall = recall_score(y_test, yhat_classes)\n",
    "    # f1: 2 tp / (2 tp + fp + fn)\n",
    "    f1 = f1_score(y_test, yhat_classes)\n",
    "    # kappa\n",
    "    kappa = cohen_kappa_score(y_test, yhat_classes)\n",
    "    # ROC AUC\n",
    "    auc = roc_auc_score(y_test, yhat_probs)\n",
    "    # confusion matrix\n",
    "    matrix = confusion_matrix(y_test, yhat_classes)\n",
    "    #print(matrix)\n",
    "    \n",
    "    array = []\n",
    "    results = dict()\n",
    "    results['accuracy'] = accuracy\n",
    "    results['precision'] = precision\n",
    "    results['recall'] = recall\n",
    "    results['f1_score'] = f1\n",
    "    results['cohen_kappa_score'] = kappa\n",
    "    results['roc_auc_score'] = auc\n",
    "    results['matrix'] = (\"[[ \" +str(matrix[0][0]) + \" \" +str(matrix[0][1]) +\"][ \" +str(matrix[1][0]) + \" \" + str(matrix[1][1]) +\"]]\") # array.append(np.array(matrix,dtype=object))\n",
    "    results['TP'] = matrix[0][0]\n",
    "    results['FP'] = matrix[0][1]\n",
    "    results['FN'] = matrix[1][0]\n",
    "    results['TN'] = matrix[1][1]\n",
    "    \n",
    "    array.append(accuracy)\n",
    "    array.append(precision)\n",
    "    array.append(recall)\n",
    "    array.append(f1)\n",
    "    array.append(kappa)\n",
    "    array.append(auc)\n",
    "    array.append(\"[[ \" +str(matrix[0][0]) + \" \" +str(matrix[0][1]) +\"][ \" +str(matrix[1][0]) + \" \" + str(matrix[1][1]) +\"]]\") # array.append(np.array(matrix,dtype=object))\n",
    "    array.append(matrix[0][0]) # TP\n",
    "    array.append(matrix[0][1]) # FP\n",
    "    array.append(matrix[1][0]) # FN\n",
    "    array.append(matrix[1][1]) # TN\n",
    "    \n",
    "    return results, array\n",
    "\n",
    "# y_test     = Array with real values\n",
    "# yhat_probs = Array with predicted values\n",
    "def printMetrics(y_test,yhat_probs):\n",
    "    # generate metrics\n",
    "    results, array= generateMetrics(y_test,yhat_probs)\n",
    "\n",
    "    # accuracy: (tp + tn) / (p + n)\n",
    "    accuracy = results['accuracy']\n",
    "    print('Accuracy: %f' % accuracy)\n",
    "    # precision tp / (tp + fp)\n",
    "    precision = results['precision']\n",
    "    print('Precision: %f' % precision)\n",
    "    # recall: tp / (tp + fn)\n",
    "    recall = results['recall'] \n",
    "    print('Recall: %f' % recall)\n",
    "    # f1: 2 tp / (2 tp + fp + fn)\n",
    "    f1 = results['f1_score']\n",
    "    print('F1 score: %f' % f1)\n",
    "    # kappa\n",
    "    kappa = results['cohen_kappa_score']\n",
    "    print('Cohens kappa: %f' % kappa)\n",
    "    # ROC AUC\n",
    "    auc = results['roc_auc_score']\n",
    "    print('ROC AUC: %f' % auc)\n",
    "    # confusion matrix\n",
    "    print(\"Confusion Matrix\")\n",
    "    matrix = results['matrix']\n",
    "    print(matrix)\n",
    "    \n",
    "    return results, array\n",
    "\n",
    "def generateGlobalMetrics(metrics):\n",
    "    accuracy,precision,recall,f1_score,cohen_kappa_score,roc_auc_score = 0,0,0,0,0,0\n",
    "    for metric in metrics:\n",
    "        accuracy = accuracy + metric['accuracy']\n",
    "        precision = precision + metric['precision']\n",
    "        recall = recall + metric['recall']\n",
    "        f1_score = f1_score + metric['f1_score']\n",
    "        cohen_kappa_score = cohen_kappa_score + metric['cohen_kappa_score']\n",
    "        roc_auc_score = roc_auc_score + metric['roc_auc_score']\n",
    "        \n",
    "    # mean\n",
    "    size = len(metrics)\n",
    "    print(size)\n",
    "    accuracy = accuracy / size\n",
    "    precision = precision / size\n",
    "    recall = recall / size\n",
    "    f1_score = f1_score / size\n",
    "    cohen_kappa_score = cohen_kappa_score / size\n",
    "    roc_auc_score = roc_auc_score / size\n",
    "    \n",
    "    return [accuracy,precision,recall,f1_score,cohen_kappa_score,roc_auc_score]\n",
    "\n",
    "def showGlobalMetrics(metrics):\n",
    "    res = generateGlobalMetrics(metrics)\n",
    "    \n",
    "    accuracy = res[0]\n",
    "    precision = res[1]\n",
    "    recall = res[2]\n",
    "    f1_score = res[3]\n",
    "    cohen_kappa_score = res[4]\n",
    "    roc_auc_score = res[5]\n",
    "    \n",
    "    #show:\\\n",
    "    print(\"accuracy: \",accuracy)\n",
    "    print(\"precision: \",precision)\n",
    "    print(\"recall: \",recall)\n",
    "    print(\"f1_score: \",f1_score)\n",
    "    print(\"cohen_kappa_score: \",cohen_kappa_score)\n",
    "    print(\"roc_auc_score: \",roc_auc_score)\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0789f112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the list of directories and concat them\n",
    "def loadDataFromFolders(foldersToLoad,inputFolders,fileType = \"_transformed\"):\n",
    "    print(len(foldersToLoad), \"datasets\")\n",
    "    for i in range(0,len(foldersToLoad)):\n",
    "        currentFolder = foldersToLoad[i]\n",
    "        print(i , \"-\", currentFolder,inputFolders+\"/student_\"+currentFolder+\"\"+fileType+\".csv\")\n",
    "        #print(trainingDataSet[i])\n",
    "        if(i == 0):\n",
    "            temp_data = pd.read_csv(inputFolders+\"/student_\"+currentFolder+\"\"+fileType+\".csv\")\n",
    "        else:\n",
    "            dataset = pd.read_csv(inputFolders+\"/student_\"+currentFolder+\"\"+fileType+\".csv\")\n",
    "            temp_data = pd.concat([temp_data, dataset])\n",
    "    # return the dataset        \n",
    "    return temp_data\n",
    "\n",
    "# take the list of directories and concat them\n",
    "def loadDataFromLastCycleFoldersOnList():\n",
    "    clientList = []\n",
    "    foldersToLoad = trainFolders\n",
    "    print(len(foldersToLoad), \"datasets\")\n",
    "    for i in range(0,len(foldersToLoad)):\n",
    "        currentFolder = foldersToLoad[i]\n",
    "        print(i , \"-\", currentFolder,iferredCycleDataFolder+\"/student_\"+currentFolder+\".csv\")\n",
    "        #print(trainingDataSet[i])\n",
    "        temp_data = pd.read_csv(iferredCycleDataFolder+\"/student_\"+currentFolder+\".csv\")\n",
    "        temp_data['class'] = temp_data['awake']\n",
    "        temp_data['class'] = temp_data['class'].astype('int32')\n",
    "        \n",
    "        mapper = {0: 'asleep', 1: 'awake'}\n",
    "\n",
    "        temp_data['class'] = temp_data['class'].map(mapper)\n",
    "        #temp_data['class'] = temp_data['class'].apply(lambda tpl: [mapper.get(x) for x in tpl])\n",
    "\n",
    "        #temp_data['fullcoursenames'] = [[mapper.get(x) for x in tpl] for tpl in temp_data['itemsets']]\n",
    "\n",
    "        #del temp_data['awake']\n",
    "        #del temp_data['asleep']\n",
    "        \n",
    "        print(\"Adding to the list: \", temp_data.shape)\n",
    "        clientList.append(temp_data)\n",
    "    # return the dataset        \n",
    "    return clientList\n",
    "\n",
    "# take the list of directories and concat them\n",
    "def loadDataFromFoldersOnList(foldersToLoad,inputFolders,fileType = \"_transformed\"):\n",
    "    clientList = []\n",
    "    print(len(foldersToLoad), \"datasets\")\n",
    "    for i in range(0,len(foldersToLoad)):\n",
    "        currentFolder = foldersToLoad[i]\n",
    "        print(i , \"-\", currentFolder,inputFolders+\"student_\"+currentFolder+fileType+\".csv\")\n",
    "        #print(trainingDataSet[i])\n",
    "        temp_data = pd.read_csv(inputFolders+\"student_\"+currentFolder+fileType+\".csv\")\n",
    "        print(\"Adding to the list: \", temp_data.shape)\n",
    "        clientList.append(temp_data)\n",
    "    # return the dataset        \n",
    "    return clientList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e0170ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing test data\n",
      "8 datasets\n",
      "0 - rIl2UK9+bQ+tzpFdbJAdbBxEa5GbgrgC030yEaENLw ../data_2019_processed//student_rIl2UK9+bQ+tzpFdbJAdbBxEa5GbgrgC030yEaENLw_transformed.csv\n",
      "1 - RoBW3cDOO9wWRMPO2twQff83MPc+OXn6gJ+a1DafreI ../data_2019_processed//student_RoBW3cDOO9wWRMPO2twQff83MPc+OXn6gJ+a1DafreI_transformed.csv\n",
      "2 - VVpwFNMrEglveh6MDN8lrRzTy5OwzglD4FURfM4A2is ../data_2019_processed//student_VVpwFNMrEglveh6MDN8lrRzTy5OwzglD4FURfM4A2is_transformed.csv\n",
      "3 - Wa1mcNmbh66S7VS6GIzyfCFMD3SGhbtDQyFP1ywJEsw ../data_2019_processed//student_Wa1mcNmbh66S7VS6GIzyfCFMD3SGhbtDQyFP1ywJEsw_transformed.csv\n",
      "4 - XCKRE0BWRHxfP1kZIihgtT+jUjSp2GE8v5ZlhcIhVmA ../data_2019_processed//student_XCKRE0BWRHxfP1kZIihgtT+jUjSp2GE8v5ZlhcIhVmA_transformed.csv\n",
      "5 - YI5Y79K6GXqAUoGP6PNyII8WKlAoel4urDxWSVVOvBw ../data_2019_processed//student_YI5Y79K6GXqAUoGP6PNyII8WKlAoel4urDxWSVVOvBw_transformed.csv\n",
      "6 - ypklj+8GJ15rOIH1lpKQtFJOuK+VdvyCuBPqhY3aoM ../data_2019_processed//student_ypklj+8GJ15rOIH1lpKQtFJOuK+VdvyCuBPqhY3aoM_transformed.csv\n",
      "7 - ZSsAZ0Pq+MCqFrnjsRFn5Ua09pMCVaOV9c8ZuYb7XQY ../data_2019_processed//student_ZSsAZ0Pq+MCqFrnjsRFn5Ua09pMCVaOV9c8ZuYb7XQY_transformed.csv\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 110603 entries, 0 to 13212\n",
      "Data columns (total 12 columns):\n",
      " #   Column              Non-Null Count   Dtype  \n",
      "---  ------              --------------   -----  \n",
      " 0   activity            110603 non-null  float64\n",
      " 1   location            110603 non-null  float64\n",
      " 2   timestamp           110603 non-null  float64\n",
      " 3   time_to_next_alarm  110603 non-null  float64\n",
      " 4   sound               110603 non-null  float64\n",
      " 5   proximity           110603 non-null  float64\n",
      " 6   phone_lock          110603 non-null  float64\n",
      " 7   light               110603 non-null  float64\n",
      " 8   day_of_week         110603 non-null  float64\n",
      " 9   minutes_day         110603 non-null  float64\n",
      " 10  timestamp_text      110603 non-null  object \n",
      " 11  class               110603 non-null  object \n",
      "dtypes: float64(10), object(2)\n",
      "memory usage: 11.0+ MB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activity</th>\n",
       "      <th>location</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>time_to_next_alarm</th>\n",
       "      <th>sound</th>\n",
       "      <th>proximity</th>\n",
       "      <th>phone_lock</th>\n",
       "      <th>light</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>minutes_day</th>\n",
       "      <th>timestamp_text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.75</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.515992</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.678249</td>\n",
       "      <td>2018-05-14 16:16:08+00:00</td>\n",
       "      <td>awake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.211282e-07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.542171</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.678944</td>\n",
       "      <td>2018-05-14 16:17:39+00:00</td>\n",
       "      <td>awake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.422564e-07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.515992</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.679639</td>\n",
       "      <td>2018-05-14 16:18:39+00:00</td>\n",
       "      <td>awake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.422564e-07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.515992</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.680334</td>\n",
       "      <td>2018-05-14 16:19:09+00:00</td>\n",
       "      <td>awake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.422564e-07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.531341</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.681028</td>\n",
       "      <td>2018-05-14 16:20:09+00:00</td>\n",
       "      <td>awake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13208</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.981050e-03</td>\n",
       "      <td>0.923909</td>\n",
       "      <td>0.577338</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000266</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.035441</td>\n",
       "      <td>2018-06-05 00:51:21+00:00</td>\n",
       "      <td>awake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13209</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.981050e-03</td>\n",
       "      <td>0.923809</td>\n",
       "      <td>0.612109</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000236</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.036136</td>\n",
       "      <td>2018-06-05 00:52:21+00:00</td>\n",
       "      <td>awake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13210</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.981371e-03</td>\n",
       "      <td>0.923611</td>\n",
       "      <td>0.573415</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.037526</td>\n",
       "      <td>2018-06-05 00:54:11+00:00</td>\n",
       "      <td>awake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13211</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.981693e-03</td>\n",
       "      <td>0.923611</td>\n",
       "      <td>0.538685</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.037526</td>\n",
       "      <td>2018-06-05 00:54:41+00:00</td>\n",
       "      <td>awake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13212</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.981693e-03</td>\n",
       "      <td>0.923512</td>\n",
       "      <td>0.542238</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.038221</td>\n",
       "      <td>2018-06-05 00:55:42+00:00</td>\n",
       "      <td>awake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>110603 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       activity  location     timestamp  time_to_next_alarm     sound  \\\n",
       "0          0.75       1.0  0.000000e+00            0.000000  0.515992   \n",
       "1          0.25       1.0  3.211282e-07            0.000000  0.542171   \n",
       "2          0.25       1.0  6.422564e-07            0.000000  0.515992   \n",
       "3          0.00       1.0  6.422564e-07            0.000000  0.515992   \n",
       "4          0.25       1.0  6.422564e-07            0.000000  0.531341   \n",
       "...         ...       ...           ...                 ...       ...   \n",
       "13208      1.00       0.0  3.981050e-03            0.923909  0.577338   \n",
       "13209      1.00       0.0  3.981050e-03            0.923809  0.612109   \n",
       "13210      0.00       0.0  3.981371e-03            0.923611  0.573415   \n",
       "13211      0.75       0.0  3.981693e-03            0.923611  0.538685   \n",
       "13212      0.75       0.0  3.981693e-03            0.923512  0.542238   \n",
       "\n",
       "       proximity  phone_lock     light  day_of_week  minutes_day  \\\n",
       "0            1.0         0.0  0.000000          1.0     0.678249   \n",
       "1            0.0         1.0  0.000007          1.0     0.678944   \n",
       "2            0.0         1.0  0.000000          1.0     0.679639   \n",
       "3            0.0         1.0  0.000000          1.0     0.680334   \n",
       "4            0.0         1.0  0.000000          1.0     0.681028   \n",
       "...          ...         ...       ...          ...          ...   \n",
       "13208        1.0         0.0  0.000266          1.0     0.035441   \n",
       "13209        1.0         0.0  0.000236          1.0     0.036136   \n",
       "13210        1.0         0.0  0.000118          1.0     0.037526   \n",
       "13211        1.0         0.0  0.000089          1.0     0.037526   \n",
       "13212        1.0         0.0  0.000030          1.0     0.038221   \n",
       "\n",
       "                  timestamp_text  class  \n",
       "0      2018-05-14 16:16:08+00:00  awake  \n",
       "1      2018-05-14 16:17:39+00:00  awake  \n",
       "2      2018-05-14 16:18:39+00:00  awake  \n",
       "3      2018-05-14 16:19:09+00:00  awake  \n",
       "4      2018-05-14 16:20:09+00:00  awake  \n",
       "...                          ...    ...  \n",
       "13208  2018-06-05 00:51:21+00:00  awake  \n",
       "13209  2018-06-05 00:52:21+00:00  awake  \n",
       "13210  2018-06-05 00:54:11+00:00  awake  \n",
       "13211  2018-06-05 00:54:41+00:00  awake  \n",
       "13212  2018-06-05 00:55:42+00:00  awake  \n",
       "\n",
       "[110603 rows x 12 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Preparing test data\")\n",
    " \n",
    "# test data comprising 25% of the data. It must be fixed to all models being evaluated\n",
    "#X_test  = pd.read_csv(inputFolders+\"test/allData-classification-numeric-normalized.csv\")\n",
    "X_test = loadDataFromFolders(testFolders,inputFolderPath,\"_transformed\")\n",
    "\n",
    "print()\n",
    "# undestand the dataset by looking on their infos\n",
    "print(X_test.info())\n",
    "\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "77bed070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing X_train data\n",
      "Gething data from training folder\n",
      "19 datasets\n",
      "0 - 0Jf4TH9Zzse0Z1Jjh7SnTOe2MMzeSnFi7feTnkG6vgs ../data_2019_processed/w0/student_0Jf4TH9Zzse0Z1Jjh7SnTOe2MMzeSnFi7feTnkG6vgs_transformed.csv\n",
      "Adding to the list:  (1331, 12)\n",
      "1 - 0tdmm6rwW3KquQ73ATYYJ5JkpMtvbppJ0VzA2GExdA ../data_2019_processed/w0/student_0tdmm6rwW3KquQ73ATYYJ5JkpMtvbppJ0VzA2GExdA_transformed.csv\n",
      "Adding to the list:  (1331, 12)\n",
      "2 - 2cyV53lVyUtlMj0BRwilEWtYJwUiviYoL48cZBPBq0 ../data_2019_processed/w0/student_2cyV53lVyUtlMj0BRwilEWtYJwUiviYoL48cZBPBq0_transformed.csv\n",
      "Adding to the list:  (1331, 12)\n",
      "3 - 2J22RukYnEbKTk7t+iUVDBkorcyL5NKN6TrLe89ys ../data_2019_processed/w0/student_2J22RukYnEbKTk7t+iUVDBkorcyL5NKN6TrLe89ys_transformed.csv\n",
      "Adding to the list:  (1331, 12)\n",
      "4 - 7EYF5I04EVqisUJCVNHlqn77UAuOmwL2Dahxd3cA ../data_2019_processed/w0/student_7EYF5I04EVqisUJCVNHlqn77UAuOmwL2Dahxd3cA_transformed.csv\n",
      "Adding to the list:  (1331, 12)\n",
      "5 - a9Qgj8ENWrHvl9QqlXcIPKmyGMKgbfHk9Dbqon1HQP4 ../data_2019_processed/w0/student_a9Qgj8ENWrHvl9QqlXcIPKmyGMKgbfHk9Dbqon1HQP4_transformed.csv\n",
      "Adding to the list:  (1331, 12)\n",
      "6 - ae4JJBZDycEcY8McJF+3BxyvZ1619y03BNdCxzpZTc ../data_2019_processed/w0/student_ae4JJBZDycEcY8McJF+3BxyvZ1619y03BNdCxzpZTc_transformed.csv\n",
      "Adding to the list:  (1331, 12)\n",
      "7 - Ch3u5Oaz96VSrQbf0z31X6jEIbeIekkC0mwPzCdeJ1U ../data_2019_processed/w0/student_Ch3u5Oaz96VSrQbf0z31X6jEIbeIekkC0mwPzCdeJ1U_transformed.csv\n",
      "Adding to the list:  (1331, 12)\n",
      "8 - CH8f0yZkZL13zWuE9ks1CkVJRVrr+jsGdUXHrZ6YeA ../data_2019_processed/w0/student_CH8f0yZkZL13zWuE9ks1CkVJRVrr+jsGdUXHrZ6YeA_transformed.csv\n",
      "Adding to the list:  (1331, 12)\n",
      "9 - DHO1K4jgiwZJOfQTrxvKE2vn7hkjamigroGD5IaeRc ../data_2019_processed/w0/student_DHO1K4jgiwZJOfQTrxvKE2vn7hkjamigroGD5IaeRc_transformed.csv\n",
      "Adding to the list:  (1331, 12)\n",
      "10 - dQEFscjqnIlug8Tgq97JohhSQPG2DEOWJqS86wCrcY ../data_2019_processed/w0/student_dQEFscjqnIlug8Tgq97JohhSQPG2DEOWJqS86wCrcY_transformed.csv\n",
      "Adding to the list:  (1331, 12)\n",
      "11 - HFvs2CohmhHte+AaCzFasjzegGzxZKPhkrX23iI6Xo ../data_2019_processed/w0/student_HFvs2CohmhHte+AaCzFasjzegGzxZKPhkrX23iI6Xo_transformed.csv\n",
      "Adding to the list:  (1331, 12)\n",
      "12 - jgB9E8v3Z6PKdTRTCMAijBllA9YEMtrmHbe4qsbmJWw ../data_2019_processed/w0/student_jgB9E8v3Z6PKdTRTCMAijBllA9YEMtrmHbe4qsbmJWw_transformed.csv\n",
      "Adding to the list:  (1331, 12)\n",
      "13 - JkY++R7E8myldLN3on6iQ78Ee78zCbrLuggfwGju3I ../data_2019_processed/w0/student_JkY++R7E8myldLN3on6iQ78Ee78zCbrLuggfwGju3I_transformed.csv\n",
      "Adding to the list:  (1331, 12)\n",
      "14 - K4SLohf+TN1Ak8Dn8iE3Lme7rEMPISfppB2sXfHX8 ../data_2019_processed/w0/student_K4SLohf+TN1Ak8Dn8iE3Lme7rEMPISfppB2sXfHX8_transformed.csv\n",
      "Adding to the list:  (1331, 12)\n",
      "15 - oGaWetJJJEWHuvYdWYo826SQxfhCExVVQ2da8LE1Y7Q ../data_2019_processed/w0/student_oGaWetJJJEWHuvYdWYo826SQxfhCExVVQ2da8LE1Y7Q_transformed.csv\n",
      "Adding to the list:  (1331, 12)\n",
      "16 - pyt24oiDAHsmgWMvkFKz2fn2pwcHiXchd6KchLM ../data_2019_processed/w0/student_pyt24oiDAHsmgWMvkFKz2fn2pwcHiXchd6KchLM_transformed.csv\n",
      "Adding to the list:  (1331, 12)\n",
      "17 - QUNCATForxzK0HHw46LrGOMWh0eVA8Y5XWEiUXX+cQ ../data_2019_processed/w0/student_QUNCATForxzK0HHw46LrGOMWh0eVA8Y5XWEiUXX+cQ_transformed.csv\n",
      "Adding to the list:  (1331, 12)\n",
      "18 - SH3kQeyd5volraxw8vOyhlowNqWBPr1IJ9URNXUL4 ../data_2019_processed/w0/student_SH3kQeyd5volraxw8vOyhlowNqWBPr1IJ9URNXUL4_transformed.csv\n",
      "Adding to the list:  (1331, 12)\n",
      "Total 19\n"
     ]
    }
   ],
   "source": [
    "print(\"Preparing X_train data\")\n",
    "# load cliend data\n",
    "if(cycle_index == 0):\n",
    "    print(\"Gething data from training folder\")\n",
    "    inputTempFolder = baseFolderWeek + \"0/\"\n",
    "    clientList = loadDataFromFoldersOnList(trainFolders,inputTempFolder,fileSufixTrain)\n",
    "    #clientList = loadDataFromFoldersOnList(trainFolders,inputFolderPath,fileSufixTrain)\n",
    "else:\n",
    "    print(\"Gething data from human inference folder\")\n",
    "    clientList = loadDataFromLastCycleFoldersOnList()\n",
    "\n",
    "NUMBER_OF_CLIENTS = len(clientList)\n",
    "print(\"Total\",(len(clientList)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e12c8ce7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'_transformed'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fileSufixTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e7f3d997",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clientList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2de66611",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for t in clientList:\n",
    "#    print(collections.Counter(t['class']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "65fe16dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train =  loadDataFromFoldersOnList(trainFolders,inputFolderPath,fileSufixTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f0e7f2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1c86ff2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 110603 entries, 0 to 13212\n",
      "Data columns (total 14 columns):\n",
      " #   Column              Non-Null Count   Dtype  \n",
      "---  ------              --------------   -----  \n",
      " 0   activity            110603 non-null  float64\n",
      " 1   location            110603 non-null  float64\n",
      " 2   timestamp           110603 non-null  float64\n",
      " 3   time_to_next_alarm  110603 non-null  float64\n",
      " 4   sound               110603 non-null  float64\n",
      " 5   proximity           110603 non-null  float64\n",
      " 6   phone_lock          110603 non-null  float64\n",
      " 7   light               110603 non-null  float64\n",
      " 8   day_of_week         110603 non-null  float64\n",
      " 9   minutes_day         110603 non-null  float64\n",
      " 10  timestamp_text      110603 non-null  object \n",
      " 11  class               110603 non-null  object \n",
      " 12  awake               110603 non-null  bool   \n",
      " 13  asleep              110603 non-null  bool   \n",
      "dtypes: bool(2), float64(10), object(2)\n",
      "memory usage: 11.2+ MB\n"
     ]
    }
   ],
   "source": [
    "# one-hot encoding function\n",
    "def transform_output_nominal_class_into_one_hot_encoding(dataset):\n",
    "    # create two classes based on the single class\n",
    "    one_hot_encoded_data = pd.get_dummies(dataset['class'])\n",
    "    #print(one_hot_encoded_data)\n",
    "    dataset['awake'] = one_hot_encoded_data['awake']\n",
    "    dataset['asleep'] = one_hot_encoded_data['asleep']\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# one-hot encoding function\n",
    "def transform_output_numerical_class_into_one_hot_encoding(dataset):\n",
    "    # create two classes based on the single class\n",
    "    one_hot_encoded_data = pd.get_dummies(dataset['class'])\n",
    "    #print(one_hot_encoded_data)\n",
    "    dataset['awake'] = one_hot_encoded_data[0]\n",
    "    dataset['asleep'] = one_hot_encoded_data[1]\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# transform output to one_hot_encoding for the testing dataset\n",
    "X_test = transform_output_nominal_class_into_one_hot_encoding(X_test)\n",
    "\n",
    "# transform output to one_hot_encoding for the input dataset\n",
    "for i in range(0,len(clientList)):\n",
    "    clientList[i] = transform_output_nominal_class_into_one_hot_encoding(clientList[i])\n",
    "    #print (clientList[i])\n",
    "    \n",
    "\n",
    "X_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a39a58ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 110603 entries, 0 to 13212\n",
      "Data columns (total 14 columns):\n",
      " #   Column              Non-Null Count   Dtype  \n",
      "---  ------              --------------   -----  \n",
      " 0   activity            110603 non-null  float32\n",
      " 1   location            110603 non-null  float32\n",
      " 2   timestamp           110603 non-null  float64\n",
      " 3   time_to_next_alarm  110603 non-null  float32\n",
      " 4   sound               110603 non-null  float32\n",
      " 5   proximity           110603 non-null  float32\n",
      " 6   phone_lock          110603 non-null  float32\n",
      " 7   light               110603 non-null  float32\n",
      " 8   day_of_week         110603 non-null  float32\n",
      " 9   minutes_day         110603 non-null  float32\n",
      " 10  timestamp_text      110603 non-null  object \n",
      " 11  class               110603 non-null  object \n",
      " 12  awake               110603 non-null  float32\n",
      " 13  asleep              110603 non-null  float32\n",
      "dtypes: float32(11), float64(1), object(2)\n",
      "memory usage: 8.0+ MB\n"
     ]
    }
   ],
   "source": [
    "def transform_data_type(dataframe):\n",
    "    \n",
    "    # transform inputs\n",
    "    for column in inputFeatures:\n",
    "        dataframe[column] = dataframe[column].astype('float32')\n",
    "    \n",
    "    # transform outputs\n",
    "    for column in outputClasses:\n",
    "        dataframe[column] = dataframe[column].astype('float32')\n",
    "    \n",
    "    return dataframe\n",
    "\n",
    "# transforms the data\n",
    "X_test = transform_data_type(X_test)\n",
    "\n",
    "X_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ec93564f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepering the test dataset\n"
     ]
    }
   ],
   "source": [
    "print(\"Prepering the test dataset\")\n",
    "# selects the data to train and test\n",
    "X_test_data = X_test[inputFeatures]\n",
    "y_test_label = X_test[outputClasses]\n",
    "\n",
    "# transtorm data to tensor slices\n",
    "#client_test_dataset = tf.data.Dataset.from_tensor_slices((X_test_data.values, y_test_label.values))\n",
    "\n",
    "#client_test_dataset = client_test_dataset.repeat(NUM_EPOCHS).batch(BATCH_SIZE, drop_remainder=True)\n",
    "#client_test_dataset = client_test_dataset.repeat(NUM_EPOCHS).batch(BATCH_SIZE)\n",
    "\n",
    "#print(client_test_dataset.element_spec)\n",
    "#client_test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2da70f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"preparing the training datasets\")\n",
    "#federated_training_data = []\n",
    "# transform the data\n",
    "#for i in range(0,len(clientList)):\n",
    "#    # selects the data to train and test\n",
    "#    data   = clientList[i][inputFeatures]\n",
    "#    labels = clientList[i][outputClasses]\n",
    "#    # transform the data to tensor slices\n",
    "#    client_train_dataset = tf.data.Dataset.from_tensor_slices((data.values, labels.values))\n",
    "    # apply the configs\n",
    "#    client_train_dataset = client_train_dataset.repeat(NUM_EPOCHS).batch(BATCH_SIZE)\n",
    "    # transform the data to\n",
    " #   federated_training_data.append(client_train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2a9b73de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating model\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_3 (Dense)             (None, 16)                160       \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 8)                 136       \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 2)                 18        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 314\n",
      "Trainable params: 314\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print(\"creating model\")\n",
    "\n",
    "def create_keras_model():\n",
    "    return tf.keras.models.Sequential([\n",
    "      tf.keras.layers.InputLayer(input_shape=(9,)),\n",
    "      #tf.keras.layers.Dense(9, activation=tf.keras.activations.relu), \n",
    "      tf.keras.layers.Dense(16, activation=tf.keras.activations.relu),\n",
    "      tf.keras.layers.Dense(8, activation=tf.keras.activations.relu),\n",
    "      tf.keras.layers.Dense(2, activation=tf.keras.activations.softmax)\n",
    "      #tf.keras.layers.Dense(2, activation=tf.nn.sigmoid)\n",
    "    ])\n",
    "\n",
    "keras_model = create_keras_model()\n",
    "#keras_model.summary()\n",
    "keras_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "35447b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and data (MobileNetV2, CIFAR-10)\n",
    "#model = keras_model\n",
    "#model.compile(\"adam\", \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "#(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "362b0f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_and_save_results(keras_model,X_test_data, y_test_label, current_round_index, \n",
    "                              clientId, prefix_string = \"Results\", lossValue = -1):\n",
    "     # predict values\n",
    "    yhat_probs = keras_model.predict(X_test_data,verbose=VERBOSE)\n",
    "    \n",
    "    # as we deal with a classification problem with one hot encoding, we must round the values to 0 and 1.\n",
    "    yhat_probs_rounded = yhat_probs.round()\n",
    "    \n",
    "    # create a dataframe with the predicted data\n",
    "    y_predicted_df = pd.DataFrame(data=yhat_probs_rounded,columns=['awake','asleep']) \n",
    "    #y_test_label_label = pd.DataFrame(data=y_test_label,columns=['awake','asleep']) \n",
    "    \n",
    "    roundData = []\n",
    "\n",
    "    columns = ['client','round','loss','class','accuracy','precision','recall', \n",
    "               'f1_score','cohen_kappa_score','roc_auc_score','confusion_matrix',\n",
    "               'TP','FP','FN','TN']\n",
    "    \n",
    "    # Instantiate the list that will contain the results\n",
    "    listOfMetrics = list()\n",
    "    \n",
    "    #print('awake')    \n",
    "    #res,resA = printMetrics(y_test_label['awake'],y_predicted_df['awake'])\n",
    "    res,resA = generateMetrics(y_test_label['awake'],y_predicted_df['awake'])\n",
    "    listOfMetrics.append(res)\n",
    "    \n",
    "    classData = np.concatenate(([clientId,current_round_index,lossValue,'awake'], resA))\n",
    "    roundData.append(classData)\n",
    "    \n",
    "    #print('')\n",
    "    #print('asleep')\n",
    "    #res,resA = printMetrics(y_test_label['asleep'],y_predicted_df['asleep'])\n",
    "    res,resA = generateMetrics(y_test_label['asleep'],y_predicted_df['asleep'])\n",
    "    listOfMetrics.append(res)\n",
    "    # new data\n",
    "    classData = np.concatenate(([clientId,current_round_index,lossValue,'asleep'], resA))\n",
    "    roundData.append(classData)\n",
    "    \n",
    "    #print('Global')\n",
    "    #resA = showGlobalMetrics(listOfMetrics) #return [accuracy,precision,recall,f1_score,cohen_kappa_score,roc_auc_score\n",
    "    resA = generateGlobalMetrics(listOfMetrics) #return [accuracy,precision,recall,f1_score,cohen_kappa_score,roc_auc_score\n",
    "    # new data\n",
    "    classData = np.concatenate(([clientId,current_round_index,lossValue,'avg'], resA))\n",
    "    roundData.append(classData)\n",
    "    \n",
    "    dataMetrics = pd.DataFrame(data=roundData,columns=columns) \n",
    "    # write file\n",
    "    if(clientId >= 0):\n",
    "        outputMetricFile = outputFolder+\"/\"+prefix_string+\"_MLP_client_\" + str(clientId) + \"_round_\" + str(current_round_index) + \".csv\"\n",
    "    else:\n",
    "        outputMetricFile = outputFolder+\"/global_model_MLP_metrics.csv\"\n",
    "        outputMetricFile = outputFolder+\"/\"+prefix_string+\".csv\"\n",
    "        # print global model results\n",
    "        if(os.path.isfile(outputMetricFile)):\n",
    "            dataset = pd.read_csv(outputMetricFile)\n",
    "            dataMetrics = pd.concat([dataset, dataMetrics], axis=0)\n",
    "        # Perform garbage collection\n",
    "        gc.collect()\n",
    "        \n",
    "    dataMetrics.to_csv(outputMetricFile, sep=',', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ab1e2479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint model result_unbalanced_epoch_1_rounds_10_cycle_0/checkpoints/round-*\n",
      "No checkpoint found\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading checkpoint model\",checkPointFolder+\"/round-*\")\n",
    "list_of_files = [fname for fname in glob.glob(checkPointFolder+\"/round-*\")]\n",
    "last_round_checkpoint = -1\n",
    "latest_round_file = None\n",
    "model_check_point = None\n",
    "filename_np = None\n",
    "filename_h5 = None\n",
    "\n",
    "if len(list_of_files) > 0:\n",
    "    latest_round_file = max(list_of_files, key=os.path.getctime)\n",
    "    print(\"Loading pre-trained model from: \", latest_round_file)\n",
    "    if(len(latest_round_file) > 0):\n",
    "        # load the name\n",
    "        last_round = latest_round_file.replace(checkPointFolder+\"/round-\",\"\")\n",
    "        last_round = last_round.replace(\"-weights.npz\",\"\")\n",
    "        last_round = last_round.replace(\"-weights.h5\",\"\")\n",
    "        print(\"Last round: \",last_round)\n",
    "    \n",
    "        last_round_checkpoint = int(last_round)\n",
    "        filename_h5 = checkPointFolder+\"/round-\"+last_round+\"-weights.h5\"\n",
    "        filename_np = checkPointFolder+\"/round-\"+last_round+\"-weights.npz\"\n",
    "else:\n",
    "    print(\"No checkpoint found\")\n",
    "\n",
    "    #check_point_model = tf.keras.models.load_model(latest_round_file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7f8aeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "722dabf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_round_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "bf9bb93d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#if latest_round_file is not None:\n",
    "#    keras_model.load_weights(latest_round_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5161a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "35c16138",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUMBER_OF_ITERATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "10217727",
   "metadata": {},
   "outputs": [],
   "source": [
    "if(last_round_checkpoint > -1):\n",
    "    NUMBER_OF_ITERATIONS = NUMBER_OF_ITERATIONS_FINAL - (last_round_checkpoint)\n",
    "\n",
    "    print(\"Number of iteractions after the checkpoint: \",NUMBER_OF_ITERATIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4dc894c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveModelStrategy(fl.server.strategy.FedAvg):\n",
    "    def aggregate_fit(\n",
    "        self,\n",
    "        server_round: int,\n",
    "        results: List[Tuple[fl.server.client_proxy.ClientProxy, fl.common.FitRes]],\n",
    "        failures: List[Union[Tuple[ClientProxy, FitRes], BaseException]],\n",
    "    ) -> Tuple[Optional[Parameters], Dict[str, Scalar]]:\n",
    "\n",
    "        # Call aggregate_fit from base class (FedAvg) to aggregate parameters and metrics\n",
    "        aggregated_parameters, aggregated_metrics = super().aggregate_fit(server_round, results, failures)\n",
    "        \n",
    "        #print(\"TEsteeee\", aggregated_parameters)\n",
    "        if aggregated_parameters is not None:\n",
    "            # Convert `Parameters` to `List[np.ndarray]`\n",
    "            aggregated_ndarrays: List[np.ndarray] = fl.common.parameters_to_ndarrays(aggregated_parameters)\n",
    "\n",
    "            # Save aggregated_ndarrays\n",
    "            print(f\"Saving round {server_round} aggregated_ndarrays...\")\n",
    "            fileName = f\"{checkPointFolder}/round-{server_round}-weights.npz\"\n",
    "            print(fileName)\n",
    "            #print(aggregated_parameters)\n",
    "            print()\n",
    "            np.savez(fileName, *aggregated_ndarrays)\n",
    "            #np.savez(fileName+\"2\", *aggregated_parameters)\n",
    "            #keras_model = create_keras_model()\n",
    "            #keras_model.set_weights(aggregated_parameters)\n",
    "            #keras_model.save_weights(fileName)\n",
    "\n",
    "        return aggregated_parameters, aggregated_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c71c6242",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Declarating client function\n"
     ]
    }
   ],
   "source": [
    "print(\"Declarating client function\")\n",
    "\n",
    "# Define a Flower client\n",
    "class FlowerISABELASleepClient(fl.client.NumPyClient):\n",
    "\n",
    "    def __init__(self, clientId, model, X_train_data, y_train_label,round_index=0):\n",
    "        self.round_index = round_index\n",
    "        self.clientId = clientId\n",
    "        self.model = model\n",
    "        self.X_train_data = X_train_data\n",
    "        self.y_train_label = y_train_label\n",
    "\n",
    "\n",
    "    def get_parameters(self, config):\n",
    "        \"\"\"Return current weights.\"\"\"\n",
    "        return self.model.get_weights()\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "               \n",
    "        \"\"\"Fit model and return new weights as well as number of training examples.\"\"\"\n",
    "        self.model.set_weights(parameters)\n",
    "        \n",
    "        \n",
    "        # Evaluate local model parameters on the local test data\n",
    "        loss, accuracy = self.model.evaluate(self.X_train_data, self.y_train_label,verbose=VERBOSE)       \n",
    "        # print model results (verify the quality from the proxy data)\n",
    "        evaluate_and_save_results(self.model,self.X_train_data, self.y_train_label, self.round_index, self.clientId,\"local_data_before_fit\",loss)\n",
    " \n",
    "        # use the checkpoint if it is not -1\n",
    "        #if(last_round_checkpoint == self.round_index and last_round_checkpoint != -1):\n",
    "        #    print(\"loading checkpoint: \",filename_h5, \" to client \",self.clientId)\n",
    "        #    print(\"loading\", latest_round_file)\n",
    "        #    self.model = tf.keras.models.load_weights(filename_h5)\n",
    "            \n",
    "        self.model.fit(self.X_train_data, self.y_train_label, epochs=NUM_EPOCHS, batch_size=BATCH_SIZE,verbose=VERBOSE)\n",
    "\n",
    "        # Evaluate local model parameters on the local test data\n",
    "        loss, accuracy = self.model.evaluate(X_test_data, y_test_label,verbose=VERBOSE)\n",
    "\n",
    "        # print model results\n",
    "        evaluate_and_save_results(self.model,X_test_data, y_test_label, self.round_index, self.clientId,\"proxy_data_after_fit\",loss)\n",
    "        return self.model.get_weights(), len(self.X_train_data), {}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c336f022",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_evaluate_fn( model):\n",
    "    \n",
    "    def evaluate(\n",
    "        server_round: int, parameters: NDArrays, config: Dict[str, Scalar]\n",
    "    ) -> Optional[Tuple[float, Dict[str, Scalar]]]:\n",
    "        \n",
    "        current_round = server_round\n",
    "        \n",
    "        if(last_round_checkpoint > -1):\n",
    "            current_round = server_round + last_round_checkpoint\n",
    "            \n",
    "        print(\"Evaluating global model round\",current_round)\n",
    "        \n",
    "        model.set_weights(parameters)\n",
    "        \n",
    "        # Evaluate local model parameters on the local test data\n",
    "        loss, accuracy = model.evaluate(X_test_data, y_test_label,verbose=VERBOSE)\n",
    "\n",
    "        # only saves if the server_round + last_round_checkpoint != last_round_checkpoint to avaid duble metrics\n",
    "        if(current_round > last_round_checkpoint):\n",
    "            # print model results\n",
    "            evaluate_and_save_results(model,X_test_data, y_test_label, current_round, -1,\"global_model_metrics_after_agregation\",loss)\n",
    "\n",
    "            #checkpoint\n",
    "            fileName = f\"{checkPointFolder}/round-{current_round}-weights.h5\"\n",
    "            model.save_weights(fileName)\n",
    "        else:\n",
    "            print(\"Round already evaluated\")\n",
    "        \n",
    "        return loss, { 'accuracy': accuracy }\n",
    "    return evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f6e8c638",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO flwr 2024-03-05 15:02:25,532 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)\n",
      "2024-03-05 15:02:28,542\tINFO worker.py:1621 -- Started a local Ray instance.\n",
      "INFO flwr 2024-03-05 15:02:29,950 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:172.30.126.159': 1.0, 'memory': 2047935284.0, 'object_store_memory': 1023967641.0, 'node:__internal_head__': 1.0, 'CPU': 16.0}\n",
      "INFO flwr 2024-03-05 15:02:29,992 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html\n",
      "INFO flwr 2024-03-05 15:02:29,994 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 1}\n",
      "INFO flwr 2024-03-05 15:02:30,011 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 16 actors\n",
      "INFO flwr 2024-03-05 15:02:30,015 | server.py:89 | Initializing global parameters\n",
      "INFO flwr 2024-03-05 15:02:30,018 | server.py:276 | Requesting initial parameters from one random client\n",
      "INFO flwr 2024-03-05 15:02:38,939 | server.py:280 | Received initial parameters from one random client\n",
      "INFO flwr 2024-03-05 15:02:38,942 | server.py:91 | Evaluating initial parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(DefaultActor pid=21516)\u001b[0m starting client: 18 <class 'str'>\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21516)\u001b[0m starting client:  <class 'int'>\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21516)\u001b[0m Creating client model to client: 18\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21516)\u001b[0m Data X: 1331\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21516)\u001b[0m Data Y: 1331\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21516)\u001b[0m Creating client model to client: 18 round 0\n",
      "Evaluating global model round 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(DefaultActor pid=21516)\u001b[0m /home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py:72: DeprecationWarning:  Ensure your client is of type `flwr.client.Client`. Please convert it using the `.to_client()` method before returning it in the `client_fn` you pass to `start_simulation`. We have applied this conversion on your behalf. Not returning a `Client` might trigger an error in future versions of Flower.\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21516)\u001b[0m   client = check_clientfn_returns_client(client_fn(cid))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO flwr 2024-03-05 15:02:46,247 | server.py:94 | initial parameters (loss, other metrics): 0.693403422832489, {'accuracy': 0.5219749808311462}\n",
      "INFO flwr 2024-03-05 15:02:46,249 | server.py:104 | FL starting\n",
      "DEBUG flwr 2024-03-05 15:02:46,250 | server.py:222 | fit_round 1: strategy sampled 19 clients (out of 19)\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21516)\u001b[0m /home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py:72: DeprecationWarning:  Ensure your client is of type `flwr.client.Client`. Please convert it using the `.to_client()` method before returning it in the `client_fn` you pass to `start_simulation`. We have applied this conversion on your behalf. Not returning a `Client` might trigger an error in future versions of Flower.\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21516)\u001b[0m   client = check_clientfn_returns_client(client_fn(cid))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(DefaultActor pid=21516)\u001b[0m starting client: 2 <class 'str'>\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21516)\u001b[0m starting client:  <class 'int'>\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21516)\u001b[0m Creating client model to client: 2\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21516)\u001b[0m Data X: 1331\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21516)\u001b[0m Data Y: 1331\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21516)\u001b[0m Creating client model to client: 2 round 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:02:46,879 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.18\tray::IDLE\n",
      "21516\t0.18\tray::IDLE\n",
      "21505\t0.18\tray::IDLE\n",
      "21507\t0.18\tray::IDLE\n",
      "21499\t0.18\tray::IDLE\n",
      "21504\t0.18\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:02:46,880 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.16\tray::IDLE\n",
      "21516\t0.16\tray::IDLE\n",
      "21511\t0.16\tray::IDLE\n",
      "21507\t0.16\tray::IDLE\n",
      "21504\t0.16\tray::IDLE\n",
      "21505\t0.16\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:02:46,884 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.18\tray::IDLE\n",
      "21516\t0.18\tray::IDLE\n",
      "21505\t0.18\tray::IDLE\n",
      "21507\t0.18\tray::IDLE\n",
      "21499\t0.18\tray::IDLE\n",
      "21504\t0.18\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:02:46,886 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.16\tray::IDLE\n",
      "21516\t0.16\tray::IDLE\n",
      "21511\t0.16\tray::IDLE\n",
      "21507\t0.16\tray::IDLE\n",
      "21504\t0.16\tray::IDLE\n",
      "21505\t0.16\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:02:46,899 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.17\tray::IDLE\n",
      "21512\t0.17\tray::IDLE\n",
      "21507\t0.17\tray::IDLE\n",
      "21505\t0.17\tray::IDLE\n",
      "21511\t0.17\tray::IDLE\n",
      "21504\t0.17\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:02:46,902 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.18\tray::IDLE\n",
      "21516\t0.18\tray::IDLE\n",
      "21505\t0.18\tray::IDLE\n",
      "21507\t0.18\tray::IDLE\n",
      "21499\t0.18\tray::IDLE\n",
      "21504\t0.18\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:02:46,907 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.21\tray::IDLE\n",
      "21512\t0.21\tray::IDLE\n",
      "21507\t0.21\tray::IDLE\n",
      "21505\t0.21\tray::IDLE\n",
      "21504\t0.21\tray::IDLE\n",
      "21515\t0.21\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:02:46,909 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.17\tray::IDLE\n",
      "21512\t0.17\tray::IDLE\n",
      "21507\t0.17\tray::IDLE\n",
      "21505\t0.17\tray::IDLE\n",
      "21511\t0.17\tray::IDLE\n",
      "21504\t0.17\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:02:46,914 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.18\tray::IDLE\n",
      "21516\t0.18\tray::IDLE\n",
      "21505\t0.18\tray::IDLE\n",
      "21507\t0.18\tray::IDLE\n",
      "21499\t0.18\tray::IDLE\n",
      "21504\t0.18\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:02:46,915 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.21\tray::IDLE\n",
      "21512\t0.21\tray::IDLE\n",
      "21507\t0.21\tray::IDLE\n",
      "21505\t0.21\tray::IDLE\n",
      "21504\t0.21\tray::IDLE\n",
      "21515\t0.21\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:02:46,915 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.17\tray::IDLE\n",
      "21512\t0.17\tray::IDLE\n",
      "21507\t0.17\tray::IDLE\n",
      "21505\t0.17\tray::IDLE\n",
      "21511\t0.17\tray::IDLE\n",
      "21504\t0.17\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:02:46,918 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.52\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.26\tray::DefaultActor\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.22\tray::DefaultActor\n",
      "21505\t0.22\tray::DefaultActor\n",
      "21499\t0.22\tray::DefaultActor\n",
      "21508\t0.22\tray::DefaultActor\n",
      "21504\t0.22\tray::DefaultActor\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:02:46,919 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.19\tray::IDLE\n",
      "21516\t0.19\tray::IDLE\n",
      "21507\t0.19\tray::IDLE\n",
      "21505\t0.19\tray::IDLE\n",
      "21504\t0.19\tray::IDLE\n",
      "21499\t0.19\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:02:46,931 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.72\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.27\tray::DefaultActor.run\n",
      "21512\t0.24\tray::DefaultActor.run\n",
      "21515\t0.24\tray::DefaultActor.run\n",
      "21508\t0.23\tray::DefaultActor.run\n",
      "21509\t0.23\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21507\t0.22\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:02:46,931 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.17\tray::IDLE\n",
      "21512\t0.17\tray::IDLE\n",
      "21507\t0.17\tray::IDLE\n",
      "21505\t0.17\tray::IDLE\n",
      "21511\t0.17\tray::IDLE\n",
      "21504\t0.17\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:02:46,934 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.16\tray::IDLE\n",
      "21516\t0.16\tray::IDLE\n",
      "21511\t0.16\tray::IDLE\n",
      "21507\t0.16\tray::IDLE\n",
      "21504\t0.16\tray::IDLE\n",
      "21505\t0.16\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:02:46,936 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.52\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.26\tray::DefaultActor\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.22\tray::DefaultActor\n",
      "21505\t0.22\tray::DefaultActor\n",
      "21499\t0.22\tray::DefaultActor\n",
      "21508\t0.22\tray::DefaultActor\n",
      "21504\t0.22\tray::DefaultActor\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:02:46,936 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.19\tray::IDLE\n",
      "21516\t0.19\tray::IDLE\n",
      "21507\t0.19\tray::IDLE\n",
      "21505\t0.19\tray::IDLE\n",
      "21504\t0.19\tray::IDLE\n",
      "21499\t0.19\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:02:46,940 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.72\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.27\tray::DefaultActor.run\n",
      "21512\t0.24\tray::DefaultActor.run\n",
      "21515\t0.24\tray::DefaultActor.run\n",
      "21508\t0.23\tray::DefaultActor.run\n",
      "21509\t0.23\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21507\t0.22\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:02:46,942 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.16\tray::IDLE\n",
      "21516\t0.16\tray::IDLE\n",
      "21511\t0.16\tray::IDLE\n",
      "21507\t0.16\tray::IDLE\n",
      "21504\t0.16\tray::IDLE\n",
      "21505\t0.16\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(DefaultActor pid=21515)\u001b[0m starting client: 9 <class 'str'>\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21515)\u001b[0m starting client:  <class 'int'>\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21515)\u001b[0m Creating client model to client: 9\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21515)\u001b[0m Data X: 1331\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21515)\u001b[0m Data Y: 1331\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21515)\u001b[0m Creating client model to client: 9 round 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(DefaultActor pid=21515)\u001b[0m /home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py:72: DeprecationWarning:  Ensure your client is of type `flwr.client.Client`. Please convert it using the `.to_client()` method before returning it in the `client_fn` you pass to `start_simulation`. We have applied this conversion on your behalf. Not returning a `Client` might trigger an error in future versions of Flower.\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21515)\u001b[0m   client = check_clientfn_returns_client(client_fn(cid))\n",
      "ERROR flwr 2024-03-05 15:02:47,240 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.78\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.28\tray::DefaultActor.run\n",
      "21515\t0.26\tray::DefaultActor.run\n",
      "21508\t0.26\tray::DefaultActor.run\n",
      "21512\t0.26\tray::DefaultActor.run\n",
      "21507\t0.24\tray::DefaultActor.run\n",
      "21504\t0.24\tray::DefaultActor.run\n",
      "21505\t0.24\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:02:47,243 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.78\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.28\tray::DefaultActor.run\n",
      "21515\t0.26\tray::DefaultActor.run\n",
      "21508\t0.26\tray::DefaultActor.run\n",
      "21512\t0.26\tray::DefaultActor.run\n",
      "21507\t0.24\tray::DefaultActor.run\n",
      "21504\t0.24\tray::DefaultActor.run\n",
      "21505\t0.24\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(DefaultActor pid=21516)\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:02:48,077 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.78\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.29\tray::DefaultActor.run\n",
      "21508\t0.28\tray::DefaultActor.run\n",
      "21507\t0.27\tray::DefaultActor.run\n",
      "21506\t0.27\tray::DefaultActor.run\n",
      "21512\t0.27\tray::DefaultActor.run\n",
      "21505\t0.27\tray::DefaultActor.run\n",
      "21504\t0.26\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:02:48,079 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.78\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.29\tray::DefaultActor.run\n",
      "21508\t0.28\tray::DefaultActor.run\n",
      "21507\t0.27\tray::DefaultActor.run\n",
      "21506\t0.27\tray::DefaultActor.run\n",
      "21512\t0.27\tray::DefaultActor.run\n",
      "21505\t0.27\tray::DefaultActor.run\n",
      "21504\t0.26\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:02:55,421 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.75\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21508\t0.31\tray::DefaultActor.run\n",
      "21505\t0.31\tray::DefaultActor.run\n",
      "21512\t0.31\tray::DefaultActor.run\n",
      "21506\t0.30\tray::DefaultActor.run\n",
      "21516\t0.30\tray::DefaultActor.run\n",
      "21504\t0.30\tray::DefaultActor.run\n",
      "21503\t0.29\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:02:55,426 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.75\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21508\t0.31\tray::DefaultActor.run\n",
      "21505\t0.31\tray::DefaultActor.run\n",
      "21512\t0.31\tray::DefaultActor.run\n",
      "21506\t0.30\tray::DefaultActor.run\n",
      "21516\t0.30\tray::DefaultActor.run\n",
      "21504\t0.30\tray::DefaultActor.run\n",
      "21503\t0.29\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(DefaultActor pid=21503)\u001b[0m starting client: 15 <class 'str'>\u001b[32m [repeated 7x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21503)\u001b[0m starting client:  <class 'int'>\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21503)\u001b[0m Creating client model to client: 15\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21503)\u001b[0m Data X: 1331\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21503)\u001b[0m Data Y: 1331\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21503)\u001b[0m Creating client model to client: 15 round 1\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21516)\u001b[0m 2\u001b[32m [repeated 7x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2024-03-05 15:02:59,293 | server.py:236 | fit_round 1 received 6 results and 13 failures\n",
      "WARNING flwr 2024-03-05 15:02:59,297 | fedavg.py:250 | No fit_metrics_aggregation_fn provided\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving round 1 aggregated_ndarrays...\n",
      "result_unbalanced_epoch_1_rounds_10_cycle_0/checkpoints/round-1-weights.npz\n",
      "\n",
      "Evaluating global model round 1\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO flwr 2024-03-05 15:03:04,984 | server.py:125 | fit progress: (1, 0.7716444134712219, {'accuracy': 0.3431371748447418}, 18.733179419999942)\n",
      "DEBUG flwr 2024-03-05 15:03:04,986 | server.py:173 | evaluate_round 1: strategy sampled 19 clients (out of 19)\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21506)\u001b[0m /home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py:72: DeprecationWarning:  Ensure your client is of type `flwr.client.Client`. Please convert it using the `.to_client()` method before returning it in the `client_fn` you pass to `start_simulation`. We have applied this conversion on your behalf. Not returning a `Client` might trigger an error in future versions of Flower.\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21506)\u001b[0m   client = check_clientfn_returns_client(client_fn(cid))\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "ERROR flwr 2024-03-05 15:03:05,235 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.75\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21508\t0.31\tray::DefaultActor.run\n",
      "21505\t0.31\tray::DefaultActor.run\n",
      "21512\t0.31\tray::DefaultActor.run\n",
      "21506\t0.30\tray::DefaultActor.run\n",
      "21516\t0.30\tray::DefaultActor.run\n",
      "21504\t0.30\tray::DefaultActor.run\n",
      "21503\t0.29\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:03:05,246 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.75\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21508\t0.31\tray::DefaultActor.run\n",
      "21505\t0.31\tray::DefaultActor.run\n",
      "21512\t0.31\tray::DefaultActor.run\n",
      "21506\t0.30\tray::DefaultActor.run\n",
      "21516\t0.30\tray::DefaultActor.run\n",
      "21504\t0.30\tray::DefaultActor.run\n",
      "21503\t0.29\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:03:05,255 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.78\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.28\tray::DefaultActor.run\n",
      "21515\t0.26\tray::DefaultActor.run\n",
      "21508\t0.26\tray::DefaultActor.run\n",
      "21512\t0.26\tray::DefaultActor.run\n",
      "21507\t0.24\tray::DefaultActor.run\n",
      "21504\t0.24\tray::DefaultActor.run\n",
      "21505\t0.24\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:03:05,257 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.16\tray::IDLE\n",
      "21516\t0.16\tray::IDLE\n",
      "21511\t0.16\tray::IDLE\n",
      "21507\t0.16\tray::IDLE\n",
      "21504\t0.16\tray::IDLE\n",
      "21505\t0.16\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:03:05,258 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.17\tray::IDLE\n",
      "21512\t0.17\tray::IDLE\n",
      "21507\t0.17\tray::IDLE\n",
      "21505\t0.17\tray::IDLE\n",
      "21511\t0.17\tray::IDLE\n",
      "21504\t0.17\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:03:05,260 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.52\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.26\tray::DefaultActor\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.22\tray::DefaultActor\n",
      "21505\t0.22\tray::DefaultActor\n",
      "21499\t0.22\tray::DefaultActor\n",
      "21508\t0.22\tray::DefaultActor\n",
      "21504\t0.22\tray::DefaultActor\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:03:05,260 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.78\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.28\tray::DefaultActor.run\n",
      "21515\t0.26\tray::DefaultActor.run\n",
      "21508\t0.26\tray::DefaultActor.run\n",
      "21512\t0.26\tray::DefaultActor.run\n",
      "21507\t0.24\tray::DefaultActor.run\n",
      "21504\t0.24\tray::DefaultActor.run\n",
      "21505\t0.24\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:03:05,261 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.72\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.27\tray::DefaultActor.run\n",
      "21512\t0.24\tray::DefaultActor.run\n",
      "21515\t0.24\tray::DefaultActor.run\n",
      "21508\t0.23\tray::DefaultActor.run\n",
      "21509\t0.23\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21507\t0.22\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:03:05,262 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.16\tray::IDLE\n",
      "21516\t0.16\tray::IDLE\n",
      "21511\t0.16\tray::IDLE\n",
      "21507\t0.16\tray::IDLE\n",
      "21504\t0.16\tray::IDLE\n",
      "21505\t0.16\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:03:05,263 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.17\tray::IDLE\n",
      "21512\t0.17\tray::IDLE\n",
      "21507\t0.17\tray::IDLE\n",
      "21505\t0.17\tray::IDLE\n",
      "21511\t0.17\tray::IDLE\n",
      "21504\t0.17\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:03:05,265 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.52\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.26\tray::DefaultActor\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.22\tray::DefaultActor\n",
      "21505\t0.22\tray::DefaultActor\n",
      "21499\t0.22\tray::DefaultActor\n",
      "21508\t0.22\tray::DefaultActor\n",
      "21504\t0.22\tray::DefaultActor\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:03:05,267 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.72\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.27\tray::DefaultActor.run\n",
      "21512\t0.24\tray::DefaultActor.run\n",
      "21515\t0.24\tray::DefaultActor.run\n",
      "21508\t0.23\tray::DefaultActor.run\n",
      "21509\t0.23\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21507\t0.22\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:03:05,268 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.78\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.29\tray::DefaultActor.run\n",
      "21508\t0.28\tray::DefaultActor.run\n",
      "21507\t0.27\tray::DefaultActor.run\n",
      "21506\t0.27\tray::DefaultActor.run\n",
      "21512\t0.27\tray::DefaultActor.run\n",
      "21505\t0.27\tray::DefaultActor.run\n",
      "21504\t0.26\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:03:05,270 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.19\tray::IDLE\n",
      "21516\t0.19\tray::IDLE\n",
      "21507\t0.19\tray::IDLE\n",
      "21505\t0.19\tray::IDLE\n",
      "21504\t0.19\tray::IDLE\n",
      "21499\t0.19\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:03:05,274 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.18\tray::IDLE\n",
      "21516\t0.18\tray::IDLE\n",
      "21505\t0.18\tray::IDLE\n",
      "21507\t0.18\tray::IDLE\n",
      "21499\t0.18\tray::IDLE\n",
      "21504\t0.18\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:03:05,277 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.78\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.29\tray::DefaultActor.run\n",
      "21508\t0.28\tray::DefaultActor.run\n",
      "21507\t0.27\tray::DefaultActor.run\n",
      "21506\t0.27\tray::DefaultActor.run\n",
      "21512\t0.27\tray::DefaultActor.run\n",
      "21505\t0.27\tray::DefaultActor.run\n",
      "21504\t0.26\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:03:05,278 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.19\tray::IDLE\n",
      "21516\t0.19\tray::IDLE\n",
      "21507\t0.19\tray::IDLE\n",
      "21505\t0.19\tray::IDLE\n",
      "21504\t0.19\tray::IDLE\n",
      "21499\t0.19\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:03:05,278 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.18\tray::IDLE\n",
      "21516\t0.18\tray::IDLE\n",
      "21505\t0.18\tray::IDLE\n",
      "21507\t0.18\tray::IDLE\n",
      "21499\t0.18\tray::IDLE\n",
      "21504\t0.18\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:03:05,278 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.75\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21508\t0.31\tray::DefaultActor.run\n",
      "21505\t0.31\tray::DefaultActor.run\n",
      "21512\t0.31\tray::DefaultActor.run\n",
      "21506\t0.30\tray::DefaultActor.run\n",
      "21516\t0.30\tray::DefaultActor.run\n",
      "21504\t0.30\tray::DefaultActor.run\n",
      "21503\t0.29\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:03:05,279 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.21\tray::IDLE\n",
      "21512\t0.21\tray::IDLE\n",
      "21507\t0.21\tray::IDLE\n",
      "21505\t0.21\tray::IDLE\n",
      "21504\t0.21\tray::IDLE\n",
      "21515\t0.21\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:03:05,284 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.75\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21508\t0.31\tray::DefaultActor.run\n",
      "21505\t0.31\tray::DefaultActor.run\n",
      "21512\t0.31\tray::DefaultActor.run\n",
      "21506\t0.30\tray::DefaultActor.run\n",
      "21516\t0.30\tray::DefaultActor.run\n",
      "21504\t0.30\tray::DefaultActor.run\n",
      "21503\t0.29\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:03:05,285 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.21\tray::IDLE\n",
      "21512\t0.21\tray::IDLE\n",
      "21507\t0.21\tray::IDLE\n",
      "21505\t0.21\tray::IDLE\n",
      "21504\t0.21\tray::IDLE\n",
      "21515\t0.21\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(DefaultActor pid=21506)\u001b[0m starting client: 0 <class 'str'>\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21506)\u001b[0m starting client:  <class 'int'>\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21506)\u001b[0m Creating client model to client: 0\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21506)\u001b[0m Data X: 1331\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21506)\u001b[0m Data Y: 1331\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21506)\u001b[0m Creating client model to client: 0 round 2\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21503)\u001b[0m 2\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21505)\u001b[0m starting client: 12 <class 'str'>\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21505)\u001b[0m starting client:  <class 'int'>\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21505)\u001b[0m Creating client model to client: 12\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21505)\u001b[0m Data X: 1331\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21505)\u001b[0m Data Y: 1331\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21505)\u001b[0m Creating client model to client: 12 round 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2024-03-05 15:03:05,293 | server.py:187 | evaluate_round 1 received 0 results and 19 failures\n",
      "DEBUG flwr 2024-03-05 15:03:05,295 | server.py:222 | fit_round 2: strategy sampled 19 clients (out of 19)\n",
      "ERROR flwr 2024-03-05 15:03:05,633 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.21\tray::IDLE\n",
      "21512\t0.21\tray::IDLE\n",
      "21507\t0.21\tray::IDLE\n",
      "21505\t0.21\tray::IDLE\n",
      "21504\t0.21\tray::IDLE\n",
      "21515\t0.21\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:03:05,647 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.21\tray::IDLE\n",
      "21512\t0.21\tray::IDLE\n",
      "21507\t0.21\tray::IDLE\n",
      "21505\t0.21\tray::IDLE\n",
      "21504\t0.21\tray::IDLE\n",
      "21515\t0.21\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:03:05,666 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.16\tray::IDLE\n",
      "21516\t0.16\tray::IDLE\n",
      "21511\t0.16\tray::IDLE\n",
      "21507\t0.16\tray::IDLE\n",
      "21504\t0.16\tray::IDLE\n",
      "21505\t0.16\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:03:05,669 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.75\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21508\t0.31\tray::DefaultActor.run\n",
      "21505\t0.31\tray::DefaultActor.run\n",
      "21512\t0.31\tray::DefaultActor.run\n",
      "21506\t0.30\tray::DefaultActor.run\n",
      "21516\t0.30\tray::DefaultActor.run\n",
      "21504\t0.30\tray::DefaultActor.run\n",
      "21503\t0.29\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:03:05,671 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.16\tray::IDLE\n",
      "21516\t0.16\tray::IDLE\n",
      "21511\t0.16\tray::IDLE\n",
      "21507\t0.16\tray::IDLE\n",
      "21504\t0.16\tray::IDLE\n",
      "21505\t0.16\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:03:05,673 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.75\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21508\t0.31\tray::DefaultActor.run\n",
      "21505\t0.31\tray::DefaultActor.run\n",
      "21512\t0.31\tray::DefaultActor.run\n",
      "21506\t0.30\tray::DefaultActor.run\n",
      "21516\t0.30\tray::DefaultActor.run\n",
      "21504\t0.30\tray::DefaultActor.run\n",
      "21503\t0.29\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:03:05,674 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.75\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21508\t0.31\tray::DefaultActor.run\n",
      "21505\t0.31\tray::DefaultActor.run\n",
      "21512\t0.31\tray::DefaultActor.run\n",
      "21506\t0.30\tray::DefaultActor.run\n",
      "21516\t0.30\tray::DefaultActor.run\n",
      "21504\t0.30\tray::DefaultActor.run\n",
      "21503\t0.29\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:03:05,674 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.18\tray::IDLE\n",
      "21516\t0.18\tray::IDLE\n",
      "21505\t0.18\tray::IDLE\n",
      "21507\t0.18\tray::IDLE\n",
      "21499\t0.18\tray::IDLE\n",
      "21504\t0.18\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:03:05,675 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.78\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.29\tray::DefaultActor.run\n",
      "21508\t0.28\tray::DefaultActor.run\n",
      "21507\t0.27\tray::DefaultActor.run\n",
      "21506\t0.27\tray::DefaultActor.run\n",
      "21512\t0.27\tray::DefaultActor.run\n",
      "21505\t0.27\tray::DefaultActor.run\n",
      "21504\t0.26\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:03:05,677 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.89\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21506\t0.36\tray::DefaultActor.run\n",
      "21512\t0.35\tray::DefaultActor.run\n",
      "21508\t0.35\tray::DefaultActor\n",
      "21505\t0.35\tray::DefaultActor\n",
      "21516\t0.34\tray::DefaultActor\n",
      "21503\t0.34\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:03:05,763 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.17\tray::IDLE\n",
      "21512\t0.17\tray::IDLE\n",
      "21507\t0.17\tray::IDLE\n",
      "21505\t0.17\tray::IDLE\n",
      "21511\t0.17\tray::IDLE\n",
      "21504\t0.17\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:03:05,764 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.75\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21508\t0.31\tray::DefaultActor.run\n",
      "21505\t0.31\tray::DefaultActor.run\n",
      "21512\t0.31\tray::DefaultActor.run\n",
      "21506\t0.30\tray::DefaultActor.run\n",
      "21516\t0.30\tray::DefaultActor.run\n",
      "21504\t0.30\tray::DefaultActor.run\n",
      "21503\t0.29\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:03:05,765 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.78\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.28\tray::DefaultActor.run\n",
      "21515\t0.26\tray::DefaultActor.run\n",
      "21508\t0.26\tray::DefaultActor.run\n",
      "21512\t0.26\tray::DefaultActor.run\n",
      "21507\t0.24\tray::DefaultActor.run\n",
      "21504\t0.24\tray::DefaultActor.run\n",
      "21505\t0.24\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:03:05,766 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.18\tray::IDLE\n",
      "21516\t0.18\tray::IDLE\n",
      "21505\t0.18\tray::IDLE\n",
      "21507\t0.18\tray::IDLE\n",
      "21499\t0.18\tray::IDLE\n",
      "21504\t0.18\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:03:05,767 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.18\tray::IDLE\n",
      "21516\t0.18\tray::IDLE\n",
      "21505\t0.18\tray::IDLE\n",
      "21507\t0.18\tray::IDLE\n",
      "21499\t0.18\tray::IDLE\n",
      "21504\t0.18\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:03:05,767 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.21\tray::IDLE\n",
      "21512\t0.21\tray::IDLE\n",
      "21507\t0.21\tray::IDLE\n",
      "21505\t0.21\tray::IDLE\n",
      "21504\t0.21\tray::IDLE\n",
      "21515\t0.21\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:03:05,770 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.72\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.27\tray::DefaultActor.run\n",
      "21512\t0.24\tray::DefaultActor.run\n",
      "21515\t0.24\tray::DefaultActor.run\n",
      "21508\t0.23\tray::DefaultActor.run\n",
      "21509\t0.23\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21507\t0.22\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:03:05,770 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.52\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.26\tray::DefaultActor\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.22\tray::DefaultActor\n",
      "21505\t0.22\tray::DefaultActor\n",
      "21499\t0.22\tray::DefaultActor\n",
      "21508\t0.22\tray::DefaultActor\n",
      "21504\t0.22\tray::DefaultActor\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:03:05,771 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.78\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.29\tray::DefaultActor.run\n",
      "21508\t0.28\tray::DefaultActor.run\n",
      "21507\t0.27\tray::DefaultActor.run\n",
      "21506\t0.27\tray::DefaultActor.run\n",
      "21512\t0.27\tray::DefaultActor.run\n",
      "21505\t0.27\tray::DefaultActor.run\n",
      "21504\t0.26\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:03:05,771 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.19\tray::IDLE\n",
      "21516\t0.19\tray::IDLE\n",
      "21507\t0.19\tray::IDLE\n",
      "21505\t0.19\tray::IDLE\n",
      "21504\t0.19\tray::IDLE\n",
      "21499\t0.19\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:03:05,775 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.89\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21506\t0.36\tray::DefaultActor.run\n",
      "21512\t0.35\tray::DefaultActor.run\n",
      "21508\t0.35\tray::DefaultActor\n",
      "21505\t0.35\tray::DefaultActor\n",
      "21516\t0.34\tray::DefaultActor\n",
      "21503\t0.34\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:03:05,776 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.17\tray::IDLE\n",
      "21512\t0.17\tray::IDLE\n",
      "21507\t0.17\tray::IDLE\n",
      "21505\t0.17\tray::IDLE\n",
      "21511\t0.17\tray::IDLE\n",
      "21504\t0.17\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:03:05,780 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.78\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.28\tray::DefaultActor.run\n",
      "21515\t0.26\tray::DefaultActor.run\n",
      "21508\t0.26\tray::DefaultActor.run\n",
      "21512\t0.26\tray::DefaultActor.run\n",
      "21507\t0.24\tray::DefaultActor.run\n",
      "21504\t0.24\tray::DefaultActor.run\n",
      "21505\t0.24\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:03:05,782 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.18\tray::IDLE\n",
      "21516\t0.18\tray::IDLE\n",
      "21505\t0.18\tray::IDLE\n",
      "21507\t0.18\tray::IDLE\n",
      "21499\t0.18\tray::IDLE\n",
      "21504\t0.18\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:03:05,786 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.21\tray::IDLE\n",
      "21512\t0.21\tray::IDLE\n",
      "21507\t0.21\tray::IDLE\n",
      "21505\t0.21\tray::IDLE\n",
      "21504\t0.21\tray::IDLE\n",
      "21515\t0.21\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:03:05,787 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.72\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.27\tray::DefaultActor.run\n",
      "21512\t0.24\tray::DefaultActor.run\n",
      "21515\t0.24\tray::DefaultActor.run\n",
      "21508\t0.23\tray::DefaultActor.run\n",
      "21509\t0.23\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21507\t0.22\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:03:05,787 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.52\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.26\tray::DefaultActor\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.22\tray::DefaultActor\n",
      "21505\t0.22\tray::DefaultActor\n",
      "21499\t0.22\tray::DefaultActor\n",
      "21508\t0.22\tray::DefaultActor\n",
      "21504\t0.22\tray::DefaultActor\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:03:05,791 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.19\tray::IDLE\n",
      "21516\t0.19\tray::IDLE\n",
      "21507\t0.19\tray::IDLE\n",
      "21505\t0.19\tray::IDLE\n",
      "21504\t0.19\tray::IDLE\n",
      "21499\t0.19\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(DefaultActor pid=21506)\u001b[0m 2\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21505)\u001b[0m starting client: 7 <class 'str'>\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21505)\u001b[0m starting client:  <class 'int'>\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21505)\u001b[0m Creating client model to client: 7\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21505)\u001b[0m Data X: 1331\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21505)\u001b[0m Data Y: 1331\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21505)\u001b[0m Creating client model to client: 7 round 2\u001b[32m [repeated 12x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2024-03-05 15:03:14,254 | server.py:236 | fit_round 2 received 5 results and 14 failures\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving round 2 aggregated_ndarrays...\n",
      "result_unbalanced_epoch_1_rounds_10_cycle_0/checkpoints/round-2-weights.npz\n",
      "\n",
      "Evaluating global model round 2\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO flwr 2024-03-05 15:03:19,754 | server.py:125 | fit progress: (2, 0.7965250015258789, {'accuracy': 0.4252054691314697}, 33.50355370499892)\n",
      "DEBUG flwr 2024-03-05 15:03:19,756 | server.py:173 | evaluate_round 2: strategy sampled 19 clients (out of 19)\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21505)\u001b[0m /home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py:72: DeprecationWarning:  Ensure your client is of type `flwr.client.Client`. Please convert it using the `.to_client()` method before returning it in the `client_fn` you pass to `start_simulation`. We have applied this conversion on your behalf. Not returning a `Client` might trigger an error in future versions of Flower.\u001b[32m [repeated 14x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21505)\u001b[0m   client = check_clientfn_returns_client(client_fn(cid))\u001b[32m [repeated 14x across cluster]\u001b[0m\n",
      "ERROR flwr 2024-03-05 15:03:19,993 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.19\tray::IDLE\n",
      "21516\t0.19\tray::IDLE\n",
      "21507\t0.19\tray::IDLE\n",
      "21505\t0.19\tray::IDLE\n",
      "21504\t0.19\tray::IDLE\n",
      "21499\t0.19\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:03:19,996 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.18\tray::IDLE\n",
      "21516\t0.18\tray::IDLE\n",
      "21505\t0.18\tray::IDLE\n",
      "21507\t0.18\tray::IDLE\n",
      "21499\t0.18\tray::IDLE\n",
      "21504\t0.18\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:03:19,996 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.72\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.27\tray::DefaultActor.run\n",
      "21512\t0.24\tray::DefaultActor.run\n",
      "21515\t0.24\tray::DefaultActor.run\n",
      "21508\t0.23\tray::DefaultActor.run\n",
      "21509\t0.23\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21507\t0.22\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:03:19,996 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.21\tray::IDLE\n",
      "21512\t0.21\tray::IDLE\n",
      "21507\t0.21\tray::IDLE\n",
      "21505\t0.21\tray::IDLE\n",
      "21504\t0.21\tray::IDLE\n",
      "21515\t0.21\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:03:20,000 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.19\tray::IDLE\n",
      "21516\t0.19\tray::IDLE\n",
      "21507\t0.19\tray::IDLE\n",
      "21505\t0.19\tray::IDLE\n",
      "21504\t0.19\tray::IDLE\n",
      "21499\t0.19\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:03:20,000 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.89\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21506\t0.36\tray::DefaultActor.run\n",
      "21512\t0.35\tray::DefaultActor.run\n",
      "21508\t0.35\tray::DefaultActor\n",
      "21505\t0.35\tray::DefaultActor\n",
      "21516\t0.34\tray::DefaultActor\n",
      "21503\t0.34\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:03:20,001 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.18\tray::IDLE\n",
      "21516\t0.18\tray::IDLE\n",
      "21505\t0.18\tray::IDLE\n",
      "21507\t0.18\tray::IDLE\n",
      "21499\t0.18\tray::IDLE\n",
      "21504\t0.18\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:03:20,003 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.72\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.27\tray::DefaultActor.run\n",
      "21512\t0.24\tray::DefaultActor.run\n",
      "21515\t0.24\tray::DefaultActor.run\n",
      "21508\t0.23\tray::DefaultActor.run\n",
      "21509\t0.23\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21507\t0.22\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:03:20,003 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.78\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.29\tray::DefaultActor.run\n",
      "21508\t0.28\tray::DefaultActor.run\n",
      "21507\t0.27\tray::DefaultActor.run\n",
      "21506\t0.27\tray::DefaultActor.run\n",
      "21512\t0.27\tray::DefaultActor.run\n",
      "21505\t0.27\tray::DefaultActor.run\n",
      "21504\t0.26\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:03:20,004 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.21\tray::IDLE\n",
      "21512\t0.21\tray::IDLE\n",
      "21507\t0.21\tray::IDLE\n",
      "21505\t0.21\tray::IDLE\n",
      "21504\t0.21\tray::IDLE\n",
      "21515\t0.21\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:03:20,004 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.16\tray::IDLE\n",
      "21516\t0.16\tray::IDLE\n",
      "21511\t0.16\tray::IDLE\n",
      "21507\t0.16\tray::IDLE\n",
      "21504\t0.16\tray::IDLE\n",
      "21505\t0.16\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:03:20,007 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.89\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21506\t0.36\tray::DefaultActor.run\n",
      "21512\t0.35\tray::DefaultActor.run\n",
      "21508\t0.35\tray::DefaultActor\n",
      "21505\t0.35\tray::DefaultActor\n",
      "21516\t0.34\tray::DefaultActor\n",
      "21503\t0.34\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:03:20,009 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.17\tray::IDLE\n",
      "21512\t0.17\tray::IDLE\n",
      "21507\t0.17\tray::IDLE\n",
      "21505\t0.17\tray::IDLE\n",
      "21511\t0.17\tray::IDLE\n",
      "21504\t0.17\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:03:20,010 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.75\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21508\t0.31\tray::DefaultActor.run\n",
      "21505\t0.31\tray::DefaultActor.run\n",
      "21512\t0.31\tray::DefaultActor.run\n",
      "21506\t0.30\tray::DefaultActor.run\n",
      "21516\t0.30\tray::DefaultActor.run\n",
      "21504\t0.30\tray::DefaultActor.run\n",
      "21503\t0.29\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(DefaultActor pid=21505)\u001b[0m 2\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21505)\u001b[0m starting client: 0 <class 'str'>\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21505)\u001b[0m starting client:  <class 'int'>\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21505)\u001b[0m Creating client model to client: 0\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21505)\u001b[0m Data X: 1331\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21505)\u001b[0m Data Y: 1331\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21505)\u001b[0m Creating client model to client: 0 round 3\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21516)\u001b[0m starting client: 13 <class 'str'>\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21516)\u001b[0m starting client:  <class 'int'>\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21516)\u001b[0m Creating client model to client: 13\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21516)\u001b[0m Data X: 1331\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21516)\u001b[0m Data Y: 1331\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21516)\u001b[0m Creating client model to client: 13 round 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:03:20,012 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.52\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.26\tray::DefaultActor\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.22\tray::DefaultActor\n",
      "21505\t0.22\tray::DefaultActor\n",
      "21499\t0.22\tray::DefaultActor\n",
      "21508\t0.22\tray::DefaultActor\n",
      "21504\t0.22\tray::DefaultActor\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:03:20,015 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.78\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.29\tray::DefaultActor.run\n",
      "21508\t0.28\tray::DefaultActor.run\n",
      "21507\t0.27\tray::DefaultActor.run\n",
      "21506\t0.27\tray::DefaultActor.run\n",
      "21512\t0.27\tray::DefaultActor.run\n",
      "21505\t0.27\tray::DefaultActor.run\n",
      "21504\t0.26\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:03:20,015 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.78\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.28\tray::DefaultActor.run\n",
      "21515\t0.26\tray::DefaultActor.run\n",
      "21508\t0.26\tray::DefaultActor.run\n",
      "21512\t0.26\tray::DefaultActor.run\n",
      "21507\t0.24\tray::DefaultActor.run\n",
      "21504\t0.24\tray::DefaultActor.run\n",
      "21505\t0.24\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:03:20,018 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.16\tray::IDLE\n",
      "21516\t0.16\tray::IDLE\n",
      "21511\t0.16\tray::IDLE\n",
      "21507\t0.16\tray::IDLE\n",
      "21504\t0.16\tray::IDLE\n",
      "21505\t0.16\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:03:20,020 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.17\tray::IDLE\n",
      "21512\t0.17\tray::IDLE\n",
      "21507\t0.17\tray::IDLE\n",
      "21505\t0.17\tray::IDLE\n",
      "21511\t0.17\tray::IDLE\n",
      "21504\t0.17\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:03:20,022 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.75\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21508\t0.31\tray::DefaultActor.run\n",
      "21505\t0.31\tray::DefaultActor.run\n",
      "21512\t0.31\tray::DefaultActor.run\n",
      "21506\t0.30\tray::DefaultActor.run\n",
      "21516\t0.30\tray::DefaultActor.run\n",
      "21504\t0.30\tray::DefaultActor.run\n",
      "21503\t0.29\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:03:20,024 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.52\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.26\tray::DefaultActor\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.22\tray::DefaultActor\n",
      "21505\t0.22\tray::DefaultActor\n",
      "21499\t0.22\tray::DefaultActor\n",
      "21508\t0.22\tray::DefaultActor\n",
      "21504\t0.22\tray::DefaultActor\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:03:20,027 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.78\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.28\tray::DefaultActor.run\n",
      "21515\t0.26\tray::DefaultActor.run\n",
      "21508\t0.26\tray::DefaultActor.run\n",
      "21512\t0.26\tray::DefaultActor.run\n",
      "21507\t0.24\tray::DefaultActor.run\n",
      "21504\t0.24\tray::DefaultActor.run\n",
      "21505\t0.24\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2024-03-05 15:03:20,033 | server.py:187 | evaluate_round 2 received 0 results and 19 failures\n",
      "DEBUG flwr 2024-03-05 15:03:20,034 | server.py:222 | fit_round 3: strategy sampled 19 clients (out of 19)\n",
      "ERROR flwr 2024-03-05 15:03:20,283 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.78\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.28\tray::DefaultActor.run\n",
      "21515\t0.26\tray::DefaultActor.run\n",
      "21508\t0.26\tray::DefaultActor.run\n",
      "21512\t0.26\tray::DefaultActor.run\n",
      "21507\t0.24\tray::DefaultActor.run\n",
      "21504\t0.24\tray::DefaultActor.run\n",
      "21505\t0.24\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:03:20,301 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.78\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.28\tray::DefaultActor.run\n",
      "21515\t0.26\tray::DefaultActor.run\n",
      "21508\t0.26\tray::DefaultActor.run\n",
      "21512\t0.26\tray::DefaultActor.run\n",
      "21507\t0.24\tray::DefaultActor.run\n",
      "21504\t0.24\tray::DefaultActor.run\n",
      "21505\t0.24\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:03:20,308 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.75\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21508\t0.31\tray::DefaultActor.run\n",
      "21505\t0.31\tray::DefaultActor.run\n",
      "21512\t0.31\tray::DefaultActor.run\n",
      "21506\t0.30\tray::DefaultActor.run\n",
      "21516\t0.30\tray::DefaultActor.run\n",
      "21504\t0.30\tray::DefaultActor.run\n",
      "21503\t0.29\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:03:20,325 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.75\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21508\t0.31\tray::DefaultActor.run\n",
      "21505\t0.31\tray::DefaultActor.run\n",
      "21512\t0.31\tray::DefaultActor.run\n",
      "21506\t0.30\tray::DefaultActor.run\n",
      "21516\t0.30\tray::DefaultActor.run\n",
      "21504\t0.30\tray::DefaultActor.run\n",
      "21503\t0.29\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:03:20,334 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.18\tray::IDLE\n",
      "21516\t0.18\tray::IDLE\n",
      "21505\t0.18\tray::IDLE\n",
      "21507\t0.18\tray::IDLE\n",
      "21499\t0.18\tray::IDLE\n",
      "21504\t0.18\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:03:20,336 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.19\tray::IDLE\n",
      "21516\t0.19\tray::IDLE\n",
      "21507\t0.19\tray::IDLE\n",
      "21505\t0.19\tray::IDLE\n",
      "21504\t0.19\tray::IDLE\n",
      "21499\t0.19\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:03:20,340 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.18\tray::IDLE\n",
      "21516\t0.18\tray::IDLE\n",
      "21505\t0.18\tray::IDLE\n",
      "21507\t0.18\tray::IDLE\n",
      "21499\t0.18\tray::IDLE\n",
      "21504\t0.18\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:03:20,343 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.52\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.26\tray::DefaultActor\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.22\tray::DefaultActor\n",
      "21505\t0.22\tray::DefaultActor\n",
      "21499\t0.22\tray::DefaultActor\n",
      "21508\t0.22\tray::DefaultActor\n",
      "21504\t0.22\tray::DefaultActor\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:03:20,343 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.19\tray::IDLE\n",
      "21516\t0.19\tray::IDLE\n",
      "21507\t0.19\tray::IDLE\n",
      "21505\t0.19\tray::IDLE\n",
      "21504\t0.19\tray::IDLE\n",
      "21499\t0.19\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:03:20,345 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.17\tray::IDLE\n",
      "21512\t0.17\tray::IDLE\n",
      "21507\t0.17\tray::IDLE\n",
      "21505\t0.17\tray::IDLE\n",
      "21511\t0.17\tray::IDLE\n",
      "21504\t0.17\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:03:20,349 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.72\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.27\tray::DefaultActor.run\n",
      "21512\t0.24\tray::DefaultActor.run\n",
      "21515\t0.24\tray::DefaultActor.run\n",
      "21508\t0.23\tray::DefaultActor.run\n",
      "21509\t0.23\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21507\t0.22\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:03:20,349 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.78\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.29\tray::DefaultActor.run\n",
      "21508\t0.28\tray::DefaultActor.run\n",
      "21507\t0.27\tray::DefaultActor.run\n",
      "21506\t0.27\tray::DefaultActor.run\n",
      "21512\t0.27\tray::DefaultActor.run\n",
      "21505\t0.27\tray::DefaultActor.run\n",
      "21504\t0.26\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:03:20,349 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.52\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.26\tray::DefaultActor\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.22\tray::DefaultActor\n",
      "21505\t0.22\tray::DefaultActor\n",
      "21499\t0.22\tray::DefaultActor\n",
      "21508\t0.22\tray::DefaultActor\n",
      "21504\t0.22\tray::DefaultActor\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:03:20,355 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.17\tray::IDLE\n",
      "21512\t0.17\tray::IDLE\n",
      "21507\t0.17\tray::IDLE\n",
      "21505\t0.17\tray::IDLE\n",
      "21511\t0.17\tray::IDLE\n",
      "21504\t0.17\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:03:20,356 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.75\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21508\t0.31\tray::DefaultActor.run\n",
      "21505\t0.31\tray::DefaultActor.run\n",
      "21512\t0.31\tray::DefaultActor.run\n",
      "21506\t0.30\tray::DefaultActor.run\n",
      "21516\t0.30\tray::DefaultActor.run\n",
      "21504\t0.30\tray::DefaultActor.run\n",
      "21503\t0.29\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:03:20,358 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.17\tray::IDLE\n",
      "21512\t0.17\tray::IDLE\n",
      "21507\t0.17\tray::IDLE\n",
      "21505\t0.17\tray::IDLE\n",
      "21511\t0.17\tray::IDLE\n",
      "21504\t0.17\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:03:20,359 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.72\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.27\tray::DefaultActor.run\n",
      "21512\t0.24\tray::DefaultActor.run\n",
      "21515\t0.24\tray::DefaultActor.run\n",
      "21508\t0.23\tray::DefaultActor.run\n",
      "21509\t0.23\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21507\t0.22\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:03:20,359 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.89\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21506\t0.36\tray::DefaultActor.run\n",
      "21512\t0.35\tray::DefaultActor.run\n",
      "21508\t0.35\tray::DefaultActor\n",
      "21505\t0.35\tray::DefaultActor\n",
      "21516\t0.34\tray::DefaultActor\n",
      "21503\t0.34\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:03:20,359 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.78\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.28\tray::DefaultActor.run\n",
      "21515\t0.26\tray::DefaultActor.run\n",
      "21508\t0.26\tray::DefaultActor.run\n",
      "21512\t0.26\tray::DefaultActor.run\n",
      "21507\t0.24\tray::DefaultActor.run\n",
      "21504\t0.24\tray::DefaultActor.run\n",
      "21505\t0.24\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:03:20,359 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.16\tray::IDLE\n",
      "21516\t0.16\tray::IDLE\n",
      "21511\t0.16\tray::IDLE\n",
      "21507\t0.16\tray::IDLE\n",
      "21504\t0.16\tray::IDLE\n",
      "21505\t0.16\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:03:20,361 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.78\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.29\tray::DefaultActor.run\n",
      "21508\t0.28\tray::DefaultActor.run\n",
      "21507\t0.27\tray::DefaultActor.run\n",
      "21506\t0.27\tray::DefaultActor.run\n",
      "21512\t0.27\tray::DefaultActor.run\n",
      "21505\t0.27\tray::DefaultActor.run\n",
      "21504\t0.26\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:03:20,363 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.21\tray::IDLE\n",
      "21512\t0.21\tray::IDLE\n",
      "21507\t0.21\tray::IDLE\n",
      "21505\t0.21\tray::IDLE\n",
      "21504\t0.21\tray::IDLE\n",
      "21515\t0.21\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:03:20,366 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.17\tray::IDLE\n",
      "21512\t0.17\tray::IDLE\n",
      "21507\t0.17\tray::IDLE\n",
      "21505\t0.17\tray::IDLE\n",
      "21511\t0.17\tray::IDLE\n",
      "21504\t0.17\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:03:20,369 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.75\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21508\t0.31\tray::DefaultActor.run\n",
      "21505\t0.31\tray::DefaultActor.run\n",
      "21512\t0.31\tray::DefaultActor.run\n",
      "21506\t0.30\tray::DefaultActor.run\n",
      "21516\t0.30\tray::DefaultActor.run\n",
      "21504\t0.30\tray::DefaultActor.run\n",
      "21503\t0.29\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:03:20,374 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.89\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21506\t0.36\tray::DefaultActor.run\n",
      "21512\t0.35\tray::DefaultActor.run\n",
      "21508\t0.35\tray::DefaultActor\n",
      "21505\t0.35\tray::DefaultActor\n",
      "21516\t0.34\tray::DefaultActor\n",
      "21503\t0.34\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:03:20,377 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.78\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.28\tray::DefaultActor.run\n",
      "21515\t0.26\tray::DefaultActor.run\n",
      "21508\t0.26\tray::DefaultActor.run\n",
      "21512\t0.26\tray::DefaultActor.run\n",
      "21507\t0.24\tray::DefaultActor.run\n",
      "21504\t0.24\tray::DefaultActor.run\n",
      "21505\t0.24\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:03:20,378 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.16\tray::IDLE\n",
      "21516\t0.16\tray::IDLE\n",
      "21511\t0.16\tray::IDLE\n",
      "21507\t0.16\tray::IDLE\n",
      "21504\t0.16\tray::IDLE\n",
      "21505\t0.16\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:03:20,382 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.21\tray::IDLE\n",
      "21512\t0.21\tray::IDLE\n",
      "21507\t0.21\tray::IDLE\n",
      "21505\t0.21\tray::IDLE\n",
      "21504\t0.21\tray::IDLE\n",
      "21515\t0.21\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:03:20,539 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.99\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21508\t0.38\tray::DefaultActor.run\n",
      "21505\t0.38\tray::DefaultActor.run\n",
      "21506\t0.37\tray::DefaultActor.run\n",
      "21512\t0.37\tray::DefaultActor.run\n",
      "21516\t0.37\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21025\t0.08\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:03:20,541 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.99\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21508\t0.38\tray::DefaultActor.run\n",
      "21505\t0.38\tray::DefaultActor.run\n",
      "21506\t0.37\tray::DefaultActor.run\n",
      "21512\t0.37\tray::DefaultActor.run\n",
      "21516\t0.37\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21025\t0.08\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(DefaultActor pid=21516)\u001b[0m 2\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21506)\u001b[0m starting client: 9 <class 'str'>\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21506)\u001b[0m starting client:  <class 'int'>\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21506)\u001b[0m Creating client model to client: 9\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21506)\u001b[0m Data X: 1331\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21506)\u001b[0m Data Y: 1331\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21512)\u001b[0m Creating client model to client: 6 round 3\u001b[32m [repeated 11x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2024-03-05 15:03:28,302 | server.py:236 | fit_round 3 received 4 results and 15 failures\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving round 3 aggregated_ndarrays...\n",
      "result_unbalanced_epoch_1_rounds_10_cycle_0/checkpoints/round-3-weights.npz\n",
      "\n",
      "Evaluating global model round 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2024-03-05 15:03:28,478 E 21098 21098] (raylet) node_manager.cc:3084: 12 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa, IP: 172.30.126.159) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.30.126.159`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "INFO flwr 2024-03-05 15:03:33,766 | server.py:125 | fit progress: (3, 0.7837085127830505, {'accuracy': 0.46832364797592163}, 47.5152722319981)\n",
      "DEBUG flwr 2024-03-05 15:03:33,768 | server.py:173 | evaluate_round 3: strategy sampled 19 clients (out of 19)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(DefaultActor pid=21506)\u001b[0m /home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py:72: DeprecationWarning:  Ensure your client is of type `flwr.client.Client`. Please convert it using the `.to_client()` method before returning it in the `client_fn` you pass to `start_simulation`. We have applied this conversion on your behalf. Not returning a `Client` might trigger an error in future versions of Flower.\u001b[32m [repeated 13x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21506)\u001b[0m   client = check_clientfn_returns_client(client_fn(cid))\u001b[32m [repeated 13x across cluster]\u001b[0m\n",
      "ERROR flwr 2024-03-05 15:03:33,977 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.99\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21508\t0.38\tray::DefaultActor.run\n",
      "21505\t0.38\tray::DefaultActor.run\n",
      "21506\t0.37\tray::DefaultActor.run\n",
      "21512\t0.37\tray::DefaultActor.run\n",
      "21516\t0.37\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21025\t0.08\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:03:33,987 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.99\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21508\t0.38\tray::DefaultActor.run\n",
      "21505\t0.38\tray::DefaultActor.run\n",
      "21506\t0.37\tray::DefaultActor.run\n",
      "21512\t0.37\tray::DefaultActor.run\n",
      "21516\t0.37\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21025\t0.08\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:03:33,995 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.21\tray::IDLE\n",
      "21512\t0.21\tray::IDLE\n",
      "21507\t0.21\tray::IDLE\n",
      "21505\t0.21\tray::IDLE\n",
      "21504\t0.21\tray::IDLE\n",
      "21515\t0.21\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:03:33,997 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.89\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21506\t0.36\tray::DefaultActor.run\n",
      "21512\t0.35\tray::DefaultActor.run\n",
      "21508\t0.35\tray::DefaultActor\n",
      "21505\t0.35\tray::DefaultActor\n",
      "21516\t0.34\tray::DefaultActor\n",
      "21503\t0.34\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:03:33,999 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.21\tray::IDLE\n",
      "21512\t0.21\tray::IDLE\n",
      "21507\t0.21\tray::IDLE\n",
      "21505\t0.21\tray::IDLE\n",
      "21504\t0.21\tray::IDLE\n",
      "21515\t0.21\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:03:34,000 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.75\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21508\t0.31\tray::DefaultActor.run\n",
      "21505\t0.31\tray::DefaultActor.run\n",
      "21512\t0.31\tray::DefaultActor.run\n",
      "21506\t0.30\tray::DefaultActor.run\n",
      "21516\t0.30\tray::DefaultActor.run\n",
      "21504\t0.30\tray::DefaultActor.run\n",
      "21503\t0.29\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:03:34,001 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.16\tray::IDLE\n",
      "21516\t0.16\tray::IDLE\n",
      "21511\t0.16\tray::IDLE\n",
      "21507\t0.16\tray::IDLE\n",
      "21504\t0.16\tray::IDLE\n",
      "21505\t0.16\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:03:34,001 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.89\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21506\t0.36\tray::DefaultActor.run\n",
      "21512\t0.35\tray::DefaultActor.run\n",
      "21508\t0.35\tray::DefaultActor\n",
      "21505\t0.35\tray::DefaultActor\n",
      "21516\t0.34\tray::DefaultActor\n",
      "21503\t0.34\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:03:34,001 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.52\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.26\tray::DefaultActor\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.22\tray::DefaultActor\n",
      "21505\t0.22\tray::DefaultActor\n",
      "21499\t0.22\tray::DefaultActor\n",
      "21508\t0.22\tray::DefaultActor\n",
      "21504\t0.22\tray::DefaultActor\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:03:34,003 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.19\tray::IDLE\n",
      "21516\t0.19\tray::IDLE\n",
      "21507\t0.19\tray::IDLE\n",
      "21505\t0.19\tray::IDLE\n",
      "21504\t0.19\tray::IDLE\n",
      "21499\t0.19\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:03:34,004 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.72\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.27\tray::DefaultActor.run\n",
      "21512\t0.24\tray::DefaultActor.run\n",
      "21515\t0.24\tray::DefaultActor.run\n",
      "21508\t0.23\tray::DefaultActor.run\n",
      "21509\t0.23\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21507\t0.22\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:03:34,004 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.18\tray::IDLE\n",
      "21516\t0.18\tray::IDLE\n",
      "21505\t0.18\tray::IDLE\n",
      "21507\t0.18\tray::IDLE\n",
      "21499\t0.18\tray::IDLE\n",
      "21504\t0.18\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:03:34,005 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.75\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21508\t0.31\tray::DefaultActor.run\n",
      "21505\t0.31\tray::DefaultActor.run\n",
      "21512\t0.31\tray::DefaultActor.run\n",
      "21506\t0.30\tray::DefaultActor.run\n",
      "21516\t0.30\tray::DefaultActor.run\n",
      "21504\t0.30\tray::DefaultActor.run\n",
      "21503\t0.29\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:03:34,006 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.78\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.29\tray::DefaultActor.run\n",
      "21508\t0.28\tray::DefaultActor.run\n",
      "21507\t0.27\tray::DefaultActor.run\n",
      "21506\t0.27\tray::DefaultActor.run\n",
      "21512\t0.27\tray::DefaultActor.run\n",
      "21505\t0.27\tray::DefaultActor.run\n",
      "21504\t0.26\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:03:34,007 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.99\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21508\t0.38\tray::DefaultActor.run\n",
      "21505\t0.38\tray::DefaultActor.run\n",
      "21506\t0.37\tray::DefaultActor.run\n",
      "21512\t0.37\tray::DefaultActor.run\n",
      "21516\t0.37\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21025\t0.08\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:03:34,007 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.78\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.28\tray::DefaultActor.run\n",
      "21515\t0.26\tray::DefaultActor.run\n",
      "21508\t0.26\tray::DefaultActor.run\n",
      "21512\t0.26\tray::DefaultActor.run\n",
      "21507\t0.24\tray::DefaultActor.run\n",
      "21504\t0.24\tray::DefaultActor.run\n",
      "21505\t0.24\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:03:34,008 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.16\tray::IDLE\n",
      "21516\t0.16\tray::IDLE\n",
      "21511\t0.16\tray::IDLE\n",
      "21507\t0.16\tray::IDLE\n",
      "21504\t0.16\tray::IDLE\n",
      "21505\t0.16\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:03:34,011 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.52\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.26\tray::DefaultActor\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.22\tray::DefaultActor\n",
      "21505\t0.22\tray::DefaultActor\n",
      "21499\t0.22\tray::DefaultActor\n",
      "21508\t0.22\tray::DefaultActor\n",
      "21504\t0.22\tray::DefaultActor\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:03:34,013 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.17\tray::IDLE\n",
      "21512\t0.17\tray::IDLE\n",
      "21507\t0.17\tray::IDLE\n",
      "21505\t0.17\tray::IDLE\n",
      "21511\t0.17\tray::IDLE\n",
      "21504\t0.17\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:03:34,016 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.19\tray::IDLE\n",
      "21516\t0.19\tray::IDLE\n",
      "21507\t0.19\tray::IDLE\n",
      "21505\t0.19\tray::IDLE\n",
      "21504\t0.19\tray::IDLE\n",
      "21499\t0.19\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:03:34,018 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.72\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.27\tray::DefaultActor.run\n",
      "21512\t0.24\tray::DefaultActor.run\n",
      "21515\t0.24\tray::DefaultActor.run\n",
      "21508\t0.23\tray::DefaultActor.run\n",
      "21509\t0.23\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21507\t0.22\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:03:34,018 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.18\tray::IDLE\n",
      "21516\t0.18\tray::IDLE\n",
      "21505\t0.18\tray::IDLE\n",
      "21507\t0.18\tray::IDLE\n",
      "21499\t0.18\tray::IDLE\n",
      "21504\t0.18\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:03:34,023 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.78\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.29\tray::DefaultActor.run\n",
      "21508\t0.28\tray::DefaultActor.run\n",
      "21507\t0.27\tray::DefaultActor.run\n",
      "21506\t0.27\tray::DefaultActor.run\n",
      "21512\t0.27\tray::DefaultActor.run\n",
      "21505\t0.27\tray::DefaultActor.run\n",
      "21504\t0.26\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:03:34,024 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.99\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21508\t0.38\tray::DefaultActor.run\n",
      "21505\t0.38\tray::DefaultActor.run\n",
      "21506\t0.37\tray::DefaultActor.run\n",
      "21512\t0.37\tray::DefaultActor.run\n",
      "21516\t0.37\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21025\t0.08\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:03:34,024 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.78\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.28\tray::DefaultActor.run\n",
      "21515\t0.26\tray::DefaultActor.run\n",
      "21508\t0.26\tray::DefaultActor.run\n",
      "21512\t0.26\tray::DefaultActor.run\n",
      "21507\t0.24\tray::DefaultActor.run\n",
      "21504\t0.24\tray::DefaultActor.run\n",
      "21505\t0.24\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:03:34,028 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.17\tray::IDLE\n",
      "21512\t0.17\tray::IDLE\n",
      "21507\t0.17\tray::IDLE\n",
      "21505\t0.17\tray::IDLE\n",
      "21511\t0.17\tray::IDLE\n",
      "21504\t0.17\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(DefaultActor pid=21506)\u001b[0m 2\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21506)\u001b[0m starting client: 16 <class 'str'>\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21506)\u001b[0m starting client:  <class 'int'>\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21506)\u001b[0m Creating client model to client: 16\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21506)\u001b[0m Data X: 1331\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21506)\u001b[0m Data Y: 1331\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21506)\u001b[0m Creating client model to client: 16 round 4\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21505)\u001b[0m starting client: 2 <class 'str'>\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21505)\u001b[0m starting client:  <class 'int'>\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21505)\u001b[0m Creating client model to client: 2\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21505)\u001b[0m Data X: 1331\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21505)\u001b[0m Data Y: 1331\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21505)\u001b[0m Creating client model to client: 2 round 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2024-03-05 15:03:34,035 | server.py:187 | evaluate_round 3 received 0 results and 19 failures\n",
      "DEBUG flwr 2024-03-05 15:03:34,036 | server.py:222 | fit_round 4: strategy sampled 19 clients (out of 19)\n",
      "ERROR flwr 2024-03-05 15:03:34,244 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.17\tray::IDLE\n",
      "21512\t0.17\tray::IDLE\n",
      "21507\t0.17\tray::IDLE\n",
      "21505\t0.17\tray::IDLE\n",
      "21511\t0.17\tray::IDLE\n",
      "21504\t0.17\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:03:34,259 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.17\tray::IDLE\n",
      "21512\t0.17\tray::IDLE\n",
      "21507\t0.17\tray::IDLE\n",
      "21505\t0.17\tray::IDLE\n",
      "21511\t0.17\tray::IDLE\n",
      "21504\t0.17\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:03:34,262 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.78\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.28\tray::DefaultActor.run\n",
      "21515\t0.26\tray::DefaultActor.run\n",
      "21508\t0.26\tray::DefaultActor.run\n",
      "21512\t0.26\tray::DefaultActor.run\n",
      "21507\t0.24\tray::DefaultActor.run\n",
      "21504\t0.24\tray::DefaultActor.run\n",
      "21505\t0.24\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:03:34,276 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.78\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.28\tray::DefaultActor.run\n",
      "21515\t0.26\tray::DefaultActor.run\n",
      "21508\t0.26\tray::DefaultActor.run\n",
      "21512\t0.26\tray::DefaultActor.run\n",
      "21507\t0.24\tray::DefaultActor.run\n",
      "21504\t0.24\tray::DefaultActor.run\n",
      "21505\t0.24\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:03:34,280 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.72\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.27\tray::DefaultActor.run\n",
      "21512\t0.24\tray::DefaultActor.run\n",
      "21515\t0.24\tray::DefaultActor.run\n",
      "21508\t0.23\tray::DefaultActor.run\n",
      "21509\t0.23\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21507\t0.22\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:03:34,283 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.72\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.27\tray::DefaultActor.run\n",
      "21512\t0.24\tray::DefaultActor.run\n",
      "21515\t0.24\tray::DefaultActor.run\n",
      "21508\t0.23\tray::DefaultActor.run\n",
      "21509\t0.23\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21507\t0.22\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:03:34,285 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.21\tray::IDLE\n",
      "21512\t0.21\tray::IDLE\n",
      "21507\t0.21\tray::IDLE\n",
      "21505\t0.21\tray::IDLE\n",
      "21504\t0.21\tray::IDLE\n",
      "21515\t0.21\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:03:34,288 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.21\tray::IDLE\n",
      "21512\t0.21\tray::IDLE\n",
      "21507\t0.21\tray::IDLE\n",
      "21505\t0.21\tray::IDLE\n",
      "21504\t0.21\tray::IDLE\n",
      "21515\t0.21\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:03:34,290 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.19\tray::IDLE\n",
      "21516\t0.19\tray::IDLE\n",
      "21507\t0.19\tray::IDLE\n",
      "21505\t0.19\tray::IDLE\n",
      "21504\t0.19\tray::IDLE\n",
      "21499\t0.19\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:03:34,292 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.52\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.26\tray::DefaultActor\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.22\tray::DefaultActor\n",
      "21505\t0.22\tray::DefaultActor\n",
      "21499\t0.22\tray::DefaultActor\n",
      "21508\t0.22\tray::DefaultActor\n",
      "21504\t0.22\tray::DefaultActor\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:03:34,296 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.19\tray::IDLE\n",
      "21516\t0.19\tray::IDLE\n",
      "21507\t0.19\tray::IDLE\n",
      "21505\t0.19\tray::IDLE\n",
      "21504\t0.19\tray::IDLE\n",
      "21499\t0.19\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:03:34,296 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.52\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.26\tray::DefaultActor\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.22\tray::DefaultActor\n",
      "21505\t0.22\tray::DefaultActor\n",
      "21499\t0.22\tray::DefaultActor\n",
      "21508\t0.22\tray::DefaultActor\n",
      "21504\t0.22\tray::DefaultActor\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:03:34,298 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.89\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21506\t0.36\tray::DefaultActor.run\n",
      "21512\t0.35\tray::DefaultActor.run\n",
      "21508\t0.35\tray::DefaultActor\n",
      "21505\t0.35\tray::DefaultActor\n",
      "21516\t0.34\tray::DefaultActor\n",
      "21503\t0.34\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:03:34,302 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.17\tray::IDLE\n",
      "21512\t0.17\tray::IDLE\n",
      "21507\t0.17\tray::IDLE\n",
      "21505\t0.17\tray::IDLE\n",
      "21511\t0.17\tray::IDLE\n",
      "21504\t0.17\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:03:34,302 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.16\tray::IDLE\n",
      "21516\t0.16\tray::IDLE\n",
      "21511\t0.16\tray::IDLE\n",
      "21507\t0.16\tray::IDLE\n",
      "21504\t0.16\tray::IDLE\n",
      "21505\t0.16\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:03:34,305 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.18\tray::IDLE\n",
      "21516\t0.18\tray::IDLE\n",
      "21505\t0.18\tray::IDLE\n",
      "21507\t0.18\tray::IDLE\n",
      "21499\t0.18\tray::IDLE\n",
      "21504\t0.18\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:03:34,305 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.19\tray::IDLE\n",
      "21516\t0.19\tray::IDLE\n",
      "21507\t0.19\tray::IDLE\n",
      "21505\t0.19\tray::IDLE\n",
      "21504\t0.19\tray::IDLE\n",
      "21499\t0.19\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:03:34,307 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.89\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21506\t0.36\tray::DefaultActor.run\n",
      "21512\t0.35\tray::DefaultActor.run\n",
      "21508\t0.35\tray::DefaultActor\n",
      "21505\t0.35\tray::DefaultActor\n",
      "21516\t0.34\tray::DefaultActor\n",
      "21503\t0.34\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:03:34,309 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.99\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21508\t0.38\tray::DefaultActor.run\n",
      "21505\t0.38\tray::DefaultActor.run\n",
      "21506\t0.37\tray::DefaultActor.run\n",
      "21512\t0.37\tray::DefaultActor.run\n",
      "21516\t0.37\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21025\t0.08\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:03:34,310 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.75\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21508\t0.31\tray::DefaultActor.run\n",
      "21505\t0.31\tray::DefaultActor.run\n",
      "21512\t0.31\tray::DefaultActor.run\n",
      "21506\t0.30\tray::DefaultActor.run\n",
      "21516\t0.30\tray::DefaultActor.run\n",
      "21504\t0.30\tray::DefaultActor.run\n",
      "21503\t0.29\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:03:34,312 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.17\tray::IDLE\n",
      "21512\t0.17\tray::IDLE\n",
      "21507\t0.17\tray::IDLE\n",
      "21505\t0.17\tray::IDLE\n",
      "21511\t0.17\tray::IDLE\n",
      "21504\t0.17\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:03:34,312 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.78\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.29\tray::DefaultActor.run\n",
      "21508\t0.28\tray::DefaultActor.run\n",
      "21507\t0.27\tray::DefaultActor.run\n",
      "21506\t0.27\tray::DefaultActor.run\n",
      "21512\t0.27\tray::DefaultActor.run\n",
      "21505\t0.27\tray::DefaultActor.run\n",
      "21504\t0.26\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:03:34,313 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.16\tray::IDLE\n",
      "21516\t0.16\tray::IDLE\n",
      "21511\t0.16\tray::IDLE\n",
      "21507\t0.16\tray::IDLE\n",
      "21504\t0.16\tray::IDLE\n",
      "21505\t0.16\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:03:34,314 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.18\tray::IDLE\n",
      "21516\t0.18\tray::IDLE\n",
      "21505\t0.18\tray::IDLE\n",
      "21507\t0.18\tray::IDLE\n",
      "21499\t0.18\tray::IDLE\n",
      "21504\t0.18\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:03:34,315 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.78\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.28\tray::DefaultActor.run\n",
      "21515\t0.26\tray::DefaultActor.run\n",
      "21508\t0.26\tray::DefaultActor.run\n",
      "21512\t0.26\tray::DefaultActor.run\n",
      "21507\t0.24\tray::DefaultActor.run\n",
      "21504\t0.24\tray::DefaultActor.run\n",
      "21505\t0.24\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:03:34,317 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.19\tray::IDLE\n",
      "21516\t0.19\tray::IDLE\n",
      "21507\t0.19\tray::IDLE\n",
      "21505\t0.19\tray::IDLE\n",
      "21504\t0.19\tray::IDLE\n",
      "21499\t0.19\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:03:34,321 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.99\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21508\t0.38\tray::DefaultActor.run\n",
      "21505\t0.38\tray::DefaultActor.run\n",
      "21506\t0.37\tray::DefaultActor.run\n",
      "21512\t0.37\tray::DefaultActor.run\n",
      "21516\t0.37\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21025\t0.08\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:03:34,322 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.75\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21508\t0.31\tray::DefaultActor.run\n",
      "21505\t0.31\tray::DefaultActor.run\n",
      "21512\t0.31\tray::DefaultActor.run\n",
      "21506\t0.30\tray::DefaultActor.run\n",
      "21516\t0.30\tray::DefaultActor.run\n",
      "21504\t0.30\tray::DefaultActor.run\n",
      "21503\t0.29\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:03:34,325 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.78\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.29\tray::DefaultActor.run\n",
      "21508\t0.28\tray::DefaultActor.run\n",
      "21507\t0.27\tray::DefaultActor.run\n",
      "21506\t0.27\tray::DefaultActor.run\n",
      "21512\t0.27\tray::DefaultActor.run\n",
      "21505\t0.27\tray::DefaultActor.run\n",
      "21504\t0.26\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:03:34,334 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.78\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.28\tray::DefaultActor.run\n",
      "21515\t0.26\tray::DefaultActor.run\n",
      "21508\t0.26\tray::DefaultActor.run\n",
      "21512\t0.26\tray::DefaultActor.run\n",
      "21507\t0.24\tray::DefaultActor.run\n",
      "21504\t0.24\tray::DefaultActor.run\n",
      "21505\t0.24\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(DefaultActor pid=21506)\u001b[0m 2\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21516)\u001b[0m starting client: 14 <class 'str'>\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21516)\u001b[0m starting client:  <class 'int'>\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21516)\u001b[0m Creating client model to client: 14\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21516)\u001b[0m Data X: 1331\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21516)\u001b[0m Data Y: 1331\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21516)\u001b[0m Creating client model to client: 14 round 4\u001b[32m [repeated 8x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2024-03-05 15:03:42,258 | server.py:236 | fit_round 4 received 4 results and 15 failures\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving round 4 aggregated_ndarrays...\n",
      "result_unbalanced_epoch_1_rounds_10_cycle_0/checkpoints/round-4-weights.npz\n",
      "\n",
      "Evaluating global model round 4\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO flwr 2024-03-05 15:03:47,778 | server.py:125 | fit progress: (4, 0.785237729549408, {'accuracy': 0.48932668566703796}, 61.527633637997496)\n",
      "DEBUG flwr 2024-03-05 15:03:47,780 | server.py:173 | evaluate_round 4: strategy sampled 19 clients (out of 19)\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21505)\u001b[0m /home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py:72: DeprecationWarning:  Ensure your client is of type `flwr.client.Client`. Please convert it using the `.to_client()` method before returning it in the `client_fn` you pass to `start_simulation`. We have applied this conversion on your behalf. Not returning a `Client` might trigger an error in future versions of Flower.\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21505)\u001b[0m   client = check_clientfn_returns_client(client_fn(cid))\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "ERROR flwr 2024-03-05 15:03:47,989 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.78\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.28\tray::DefaultActor.run\n",
      "21515\t0.26\tray::DefaultActor.run\n",
      "21508\t0.26\tray::DefaultActor.run\n",
      "21512\t0.26\tray::DefaultActor.run\n",
      "21507\t0.24\tray::DefaultActor.run\n",
      "21504\t0.24\tray::DefaultActor.run\n",
      "21505\t0.24\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:03:47,999 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.78\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.28\tray::DefaultActor.run\n",
      "21515\t0.26\tray::DefaultActor.run\n",
      "21508\t0.26\tray::DefaultActor.run\n",
      "21512\t0.26\tray::DefaultActor.run\n",
      "21507\t0.24\tray::DefaultActor.run\n",
      "21504\t0.24\tray::DefaultActor.run\n",
      "21505\t0.24\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:03:48,005 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.78\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.29\tray::DefaultActor.run\n",
      "21508\t0.28\tray::DefaultActor.run\n",
      "21507\t0.27\tray::DefaultActor.run\n",
      "21506\t0.27\tray::DefaultActor.run\n",
      "21512\t0.27\tray::DefaultActor.run\n",
      "21505\t0.27\tray::DefaultActor.run\n",
      "21504\t0.26\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:03:48,007 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.75\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21508\t0.31\tray::DefaultActor.run\n",
      "21505\t0.31\tray::DefaultActor.run\n",
      "21512\t0.31\tray::DefaultActor.run\n",
      "21506\t0.30\tray::DefaultActor.run\n",
      "21516\t0.30\tray::DefaultActor.run\n",
      "21504\t0.30\tray::DefaultActor.run\n",
      "21503\t0.29\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:03:48,008 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.99\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21508\t0.38\tray::DefaultActor.run\n",
      "21505\t0.38\tray::DefaultActor.run\n",
      "21506\t0.37\tray::DefaultActor.run\n",
      "21512\t0.37\tray::DefaultActor.run\n",
      "21516\t0.37\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21025\t0.08\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:03:48,009 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.78\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.29\tray::DefaultActor.run\n",
      "21508\t0.28\tray::DefaultActor.run\n",
      "21507\t0.27\tray::DefaultActor.run\n",
      "21506\t0.27\tray::DefaultActor.run\n",
      "21512\t0.27\tray::DefaultActor.run\n",
      "21505\t0.27\tray::DefaultActor.run\n",
      "21504\t0.26\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:03:48,011 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.75\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21508\t0.31\tray::DefaultActor.run\n",
      "21505\t0.31\tray::DefaultActor.run\n",
      "21512\t0.31\tray::DefaultActor.run\n",
      "21506\t0.30\tray::DefaultActor.run\n",
      "21516\t0.30\tray::DefaultActor.run\n",
      "21504\t0.30\tray::DefaultActor.run\n",
      "21503\t0.29\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:03:48,012 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.99\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21508\t0.38\tray::DefaultActor.run\n",
      "21505\t0.38\tray::DefaultActor.run\n",
      "21506\t0.37\tray::DefaultActor.run\n",
      "21512\t0.37\tray::DefaultActor.run\n",
      "21516\t0.37\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21025\t0.08\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:03:48,015 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.21\tray::IDLE\n",
      "21512\t0.21\tray::IDLE\n",
      "21507\t0.21\tray::IDLE\n",
      "21505\t0.21\tray::IDLE\n",
      "21504\t0.21\tray::IDLE\n",
      "21515\t0.21\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:03:48,015 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.52\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.26\tray::DefaultActor\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.22\tray::DefaultActor\n",
      "21505\t0.22\tray::DefaultActor\n",
      "21499\t0.22\tray::DefaultActor\n",
      "21508\t0.22\tray::DefaultActor\n",
      "21504\t0.22\tray::DefaultActor\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:03:48,018 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.21\tray::IDLE\n",
      "21512\t0.21\tray::IDLE\n",
      "21507\t0.21\tray::IDLE\n",
      "21505\t0.21\tray::IDLE\n",
      "21504\t0.21\tray::IDLE\n",
      "21515\t0.21\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:03:48,019 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.52\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.26\tray::DefaultActor\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.22\tray::DefaultActor\n",
      "21505\t0.22\tray::DefaultActor\n",
      "21499\t0.22\tray::DefaultActor\n",
      "21508\t0.22\tray::DefaultActor\n",
      "21504\t0.22\tray::DefaultActor\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:03:48,021 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.19\tray::IDLE\n",
      "21516\t0.19\tray::IDLE\n",
      "21507\t0.19\tray::IDLE\n",
      "21505\t0.19\tray::IDLE\n",
      "21504\t0.19\tray::IDLE\n",
      "21499\t0.19\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:03:48,023 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.89\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21506\t0.36\tray::DefaultActor.run\n",
      "21512\t0.35\tray::DefaultActor.run\n",
      "21508\t0.35\tray::DefaultActor\n",
      "21505\t0.35\tray::DefaultActor\n",
      "21516\t0.34\tray::DefaultActor\n",
      "21503\t0.34\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:03:48,024 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.16\tray::IDLE\n",
      "21516\t0.16\tray::IDLE\n",
      "21511\t0.16\tray::IDLE\n",
      "21507\t0.16\tray::IDLE\n",
      "21504\t0.16\tray::IDLE\n",
      "21505\t0.16\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:03:48,025 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.72\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.27\tray::DefaultActor.run\n",
      "21512\t0.24\tray::DefaultActor.run\n",
      "21515\t0.24\tray::DefaultActor.run\n",
      "21508\t0.23\tray::DefaultActor.run\n",
      "21509\t0.23\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21507\t0.22\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:03:48,026 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.19\tray::IDLE\n",
      "21516\t0.19\tray::IDLE\n",
      "21507\t0.19\tray::IDLE\n",
      "21505\t0.19\tray::IDLE\n",
      "21504\t0.19\tray::IDLE\n",
      "21499\t0.19\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:03:48,029 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.89\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21506\t0.36\tray::DefaultActor.run\n",
      "21512\t0.35\tray::DefaultActor.run\n",
      "21508\t0.35\tray::DefaultActor\n",
      "21505\t0.35\tray::DefaultActor\n",
      "21516\t0.34\tray::DefaultActor\n",
      "21503\t0.34\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:03:48,029 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.18\tray::IDLE\n",
      "21516\t0.18\tray::IDLE\n",
      "21505\t0.18\tray::IDLE\n",
      "21507\t0.18\tray::IDLE\n",
      "21499\t0.18\tray::IDLE\n",
      "21504\t0.18\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:03:48,030 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.16\tray::IDLE\n",
      "21516\t0.16\tray::IDLE\n",
      "21511\t0.16\tray::IDLE\n",
      "21507\t0.16\tray::IDLE\n",
      "21504\t0.16\tray::IDLE\n",
      "21505\t0.16\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:03:48,030 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.78\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.28\tray::DefaultActor.run\n",
      "21515\t0.26\tray::DefaultActor.run\n",
      "21508\t0.26\tray::DefaultActor.run\n",
      "21512\t0.26\tray::DefaultActor.run\n",
      "21507\t0.24\tray::DefaultActor.run\n",
      "21504\t0.24\tray::DefaultActor.run\n",
      "21505\t0.24\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:03:48,031 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.17\tray::IDLE\n",
      "21512\t0.17\tray::IDLE\n",
      "21507\t0.17\tray::IDLE\n",
      "21505\t0.17\tray::IDLE\n",
      "21511\t0.17\tray::IDLE\n",
      "21504\t0.17\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:03:48,031 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.72\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.27\tray::DefaultActor.run\n",
      "21512\t0.24\tray::DefaultActor.run\n",
      "21515\t0.24\tray::DefaultActor.run\n",
      "21508\t0.23\tray::DefaultActor.run\n",
      "21509\t0.23\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21507\t0.22\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:03:48,032 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.78\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.29\tray::DefaultActor.run\n",
      "21508\t0.28\tray::DefaultActor.run\n",
      "21507\t0.27\tray::DefaultActor.run\n",
      "21506\t0.27\tray::DefaultActor.run\n",
      "21512\t0.27\tray::DefaultActor.run\n",
      "21505\t0.27\tray::DefaultActor.run\n",
      "21504\t0.26\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:03:48,035 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.18\tray::IDLE\n",
      "21516\t0.18\tray::IDLE\n",
      "21505\t0.18\tray::IDLE\n",
      "21507\t0.18\tray::IDLE\n",
      "21499\t0.18\tray::IDLE\n",
      "21504\t0.18\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:03:48,037 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.78\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.28\tray::DefaultActor.run\n",
      "21515\t0.26\tray::DefaultActor.run\n",
      "21508\t0.26\tray::DefaultActor.run\n",
      "21512\t0.26\tray::DefaultActor.run\n",
      "21507\t0.24\tray::DefaultActor.run\n",
      "21504\t0.24\tray::DefaultActor.run\n",
      "21505\t0.24\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:03:48,039 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.17\tray::IDLE\n",
      "21512\t0.17\tray::IDLE\n",
      "21507\t0.17\tray::IDLE\n",
      "21505\t0.17\tray::IDLE\n",
      "21511\t0.17\tray::IDLE\n",
      "21504\t0.17\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:03:48,042 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.78\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.29\tray::DefaultActor.run\n",
      "21508\t0.28\tray::DefaultActor.run\n",
      "21507\t0.27\tray::DefaultActor.run\n",
      "21506\t0.27\tray::DefaultActor.run\n",
      "21512\t0.27\tray::DefaultActor.run\n",
      "21505\t0.27\tray::DefaultActor.run\n",
      "21504\t0.26\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(DefaultActor pid=21516)\u001b[0m 2\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21506)\u001b[0m starting client: 7 <class 'str'>\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21506)\u001b[0m starting client:  <class 'int'>\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21505)\u001b[0m starting client: 12 <class 'str'>\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21505)\u001b[0m starting client:  <class 'int'>\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21505)\u001b[0m Creating client model to client: 12\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21505)\u001b[0m Data X: 1331\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21505)\u001b[0m Data Y: 1331\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21505)\u001b[0m Creating client model to client: 12 round 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2024-03-05 15:03:48,046 | server.py:187 | evaluate_round 4 received 0 results and 19 failures\n",
      "DEBUG flwr 2024-03-05 15:03:48,047 | server.py:222 | fit_round 5: strategy sampled 19 clients (out of 19)\n",
      "ERROR flwr 2024-03-05 15:03:48,262 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.78\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.29\tray::DefaultActor.run\n",
      "21508\t0.28\tray::DefaultActor.run\n",
      "21507\t0.27\tray::DefaultActor.run\n",
      "21506\t0.27\tray::DefaultActor.run\n",
      "21512\t0.27\tray::DefaultActor.run\n",
      "21505\t0.27\tray::DefaultActor.run\n",
      "21504\t0.26\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:03:48,264 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.78\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.29\tray::DefaultActor.run\n",
      "21508\t0.28\tray::DefaultActor.run\n",
      "21507\t0.27\tray::DefaultActor.run\n",
      "21506\t0.27\tray::DefaultActor.run\n",
      "21512\t0.27\tray::DefaultActor.run\n",
      "21505\t0.27\tray::DefaultActor.run\n",
      "21504\t0.26\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:03:48,296 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.78\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.28\tray::DefaultActor.run\n",
      "21515\t0.26\tray::DefaultActor.run\n",
      "21508\t0.26\tray::DefaultActor.run\n",
      "21512\t0.26\tray::DefaultActor.run\n",
      "21507\t0.24\tray::DefaultActor.run\n",
      "21504\t0.24\tray::DefaultActor.run\n",
      "21505\t0.24\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:03:48,298 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.17\tray::IDLE\n",
      "21512\t0.17\tray::IDLE\n",
      "21507\t0.17\tray::IDLE\n",
      "21505\t0.17\tray::IDLE\n",
      "21511\t0.17\tray::IDLE\n",
      "21504\t0.17\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:03:48,300 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.18\tray::IDLE\n",
      "21516\t0.18\tray::IDLE\n",
      "21505\t0.18\tray::IDLE\n",
      "21507\t0.18\tray::IDLE\n",
      "21499\t0.18\tray::IDLE\n",
      "21504\t0.18\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:03:48,302 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.78\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.28\tray::DefaultActor.run\n",
      "21515\t0.26\tray::DefaultActor.run\n",
      "21508\t0.26\tray::DefaultActor.run\n",
      "21512\t0.26\tray::DefaultActor.run\n",
      "21507\t0.24\tray::DefaultActor.run\n",
      "21504\t0.24\tray::DefaultActor.run\n",
      "21505\t0.24\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:03:48,304 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.72\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.27\tray::DefaultActor.run\n",
      "21512\t0.24\tray::DefaultActor.run\n",
      "21515\t0.24\tray::DefaultActor.run\n",
      "21508\t0.23\tray::DefaultActor.run\n",
      "21509\t0.23\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21507\t0.22\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:03:48,305 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.17\tray::IDLE\n",
      "21512\t0.17\tray::IDLE\n",
      "21507\t0.17\tray::IDLE\n",
      "21505\t0.17\tray::IDLE\n",
      "21511\t0.17\tray::IDLE\n",
      "21504\t0.17\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:03:48,306 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.18\tray::IDLE\n",
      "21516\t0.18\tray::IDLE\n",
      "21505\t0.18\tray::IDLE\n",
      "21507\t0.18\tray::IDLE\n",
      "21499\t0.18\tray::IDLE\n",
      "21504\t0.18\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:03:48,307 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.16\tray::IDLE\n",
      "21516\t0.16\tray::IDLE\n",
      "21511\t0.16\tray::IDLE\n",
      "21507\t0.16\tray::IDLE\n",
      "21504\t0.16\tray::IDLE\n",
      "21505\t0.16\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:03:48,310 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.72\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.27\tray::DefaultActor.run\n",
      "21512\t0.24\tray::DefaultActor.run\n",
      "21515\t0.24\tray::DefaultActor.run\n",
      "21508\t0.23\tray::DefaultActor.run\n",
      "21509\t0.23\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21507\t0.22\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:03:48,311 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.99\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21508\t0.38\tray::DefaultActor.run\n",
      "21505\t0.38\tray::DefaultActor.run\n",
      "21506\t0.37\tray::DefaultActor.run\n",
      "21512\t0.37\tray::DefaultActor.run\n",
      "21516\t0.37\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21025\t0.08\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:03:48,313 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.75\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21508\t0.31\tray::DefaultActor.run\n",
      "21505\t0.31\tray::DefaultActor.run\n",
      "21512\t0.31\tray::DefaultActor.run\n",
      "21506\t0.30\tray::DefaultActor.run\n",
      "21516\t0.30\tray::DefaultActor.run\n",
      "21504\t0.30\tray::DefaultActor.run\n",
      "21503\t0.29\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:03:48,313 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.78\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.29\tray::DefaultActor.run\n",
      "21508\t0.28\tray::DefaultActor.run\n",
      "21507\t0.27\tray::DefaultActor.run\n",
      "21506\t0.27\tray::DefaultActor.run\n",
      "21512\t0.27\tray::DefaultActor.run\n",
      "21505\t0.27\tray::DefaultActor.run\n",
      "21504\t0.26\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:03:48,314 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.78\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.28\tray::DefaultActor.run\n",
      "21515\t0.26\tray::DefaultActor.run\n",
      "21508\t0.26\tray::DefaultActor.run\n",
      "21512\t0.26\tray::DefaultActor.run\n",
      "21507\t0.24\tray::DefaultActor.run\n",
      "21504\t0.24\tray::DefaultActor.run\n",
      "21505\t0.24\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:03:48,316 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.16\tray::IDLE\n",
      "21516\t0.16\tray::IDLE\n",
      "21511\t0.16\tray::IDLE\n",
      "21507\t0.16\tray::IDLE\n",
      "21504\t0.16\tray::IDLE\n",
      "21505\t0.16\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:03:48,318 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.72\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.27\tray::DefaultActor.run\n",
      "21512\t0.24\tray::DefaultActor.run\n",
      "21515\t0.24\tray::DefaultActor.run\n",
      "21508\t0.23\tray::DefaultActor.run\n",
      "21509\t0.23\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21507\t0.22\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:03:48,319 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.19\tray::IDLE\n",
      "21516\t0.19\tray::IDLE\n",
      "21507\t0.19\tray::IDLE\n",
      "21505\t0.19\tray::IDLE\n",
      "21504\t0.19\tray::IDLE\n",
      "21499\t0.19\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:03:48,321 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.52\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.26\tray::DefaultActor\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.22\tray::DefaultActor\n",
      "21505\t0.22\tray::DefaultActor\n",
      "21499\t0.22\tray::DefaultActor\n",
      "21508\t0.22\tray::DefaultActor\n",
      "21504\t0.22\tray::DefaultActor\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:03:48,322 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.99\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21508\t0.38\tray::DefaultActor.run\n",
      "21505\t0.38\tray::DefaultActor.run\n",
      "21506\t0.37\tray::DefaultActor.run\n",
      "21512\t0.37\tray::DefaultActor.run\n",
      "21516\t0.37\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21025\t0.08\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:03:48,325 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.75\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21508\t0.31\tray::DefaultActor.run\n",
      "21505\t0.31\tray::DefaultActor.run\n",
      "21512\t0.31\tray::DefaultActor.run\n",
      "21506\t0.30\tray::DefaultActor.run\n",
      "21516\t0.30\tray::DefaultActor.run\n",
      "21504\t0.30\tray::DefaultActor.run\n",
      "21503\t0.29\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:03:48,327 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.21\tray::IDLE\n",
      "21512\t0.21\tray::IDLE\n",
      "21507\t0.21\tray::IDLE\n",
      "21505\t0.21\tray::IDLE\n",
      "21504\t0.21\tray::IDLE\n",
      "21515\t0.21\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:03:48,327 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.78\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.29\tray::DefaultActor.run\n",
      "21508\t0.28\tray::DefaultActor.run\n",
      "21507\t0.27\tray::DefaultActor.run\n",
      "21506\t0.27\tray::DefaultActor.run\n",
      "21512\t0.27\tray::DefaultActor.run\n",
      "21505\t0.27\tray::DefaultActor.run\n",
      "21504\t0.26\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:03:48,328 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.89\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21506\t0.36\tray::DefaultActor.run\n",
      "21512\t0.35\tray::DefaultActor.run\n",
      "21508\t0.35\tray::DefaultActor\n",
      "21505\t0.35\tray::DefaultActor\n",
      "21516\t0.34\tray::DefaultActor\n",
      "21503\t0.34\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:03:48,329 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.78\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.28\tray::DefaultActor.run\n",
      "21515\t0.26\tray::DefaultActor.run\n",
      "21508\t0.26\tray::DefaultActor.run\n",
      "21512\t0.26\tray::DefaultActor.run\n",
      "21507\t0.24\tray::DefaultActor.run\n",
      "21504\t0.24\tray::DefaultActor.run\n",
      "21505\t0.24\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:03:48,332 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.72\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.27\tray::DefaultActor.run\n",
      "21512\t0.24\tray::DefaultActor.run\n",
      "21515\t0.24\tray::DefaultActor.run\n",
      "21508\t0.23\tray::DefaultActor.run\n",
      "21509\t0.23\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21507\t0.22\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:03:48,333 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.19\tray::IDLE\n",
      "21516\t0.19\tray::IDLE\n",
      "21507\t0.19\tray::IDLE\n",
      "21505\t0.19\tray::IDLE\n",
      "21504\t0.19\tray::IDLE\n",
      "21499\t0.19\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:03:48,335 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.52\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.26\tray::DefaultActor\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.22\tray::DefaultActor\n",
      "21505\t0.22\tray::DefaultActor\n",
      "21499\t0.22\tray::DefaultActor\n",
      "21508\t0.22\tray::DefaultActor\n",
      "21504\t0.22\tray::DefaultActor\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:03:48,344 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.21\tray::IDLE\n",
      "21512\t0.21\tray::IDLE\n",
      "21507\t0.21\tray::IDLE\n",
      "21505\t0.21\tray::IDLE\n",
      "21504\t0.21\tray::IDLE\n",
      "21515\t0.21\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:03:48,346 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.89\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21506\t0.36\tray::DefaultActor.run\n",
      "21512\t0.35\tray::DefaultActor.run\n",
      "21508\t0.35\tray::DefaultActor\n",
      "21505\t0.35\tray::DefaultActor\n",
      "21516\t0.34\tray::DefaultActor\n",
      "21503\t0.34\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(DefaultActor pid=21505)\u001b[0m 2\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21512)\u001b[0m starting client: 13 <class 'str'>\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21512)\u001b[0m starting client:  <class 'int'>\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21512)\u001b[0m Creating client model to client: 13\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21512)\u001b[0m Data X: 1331\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21512)\u001b[0m Data Y: 1331\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21512)\u001b[0m Creating client model to client: 13 round 5\u001b[32m [repeated 8x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2024-03-05 15:03:55,976 | server.py:236 | fit_round 5 received 4 results and 15 failures\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving round 5 aggregated_ndarrays...\n",
      "result_unbalanced_epoch_1_rounds_10_cycle_0/checkpoints/round-5-weights.npz\n",
      "\n",
      "Evaluating global model round 5\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO flwr 2024-03-05 15:04:01,498 | server.py:125 | fit progress: (5, 0.7544503808021545, {'accuracy': 0.5235030055046082}, 75.24743984000088)\n",
      "DEBUG flwr 2024-03-05 15:04:01,500 | server.py:173 | evaluate_round 5: strategy sampled 19 clients (out of 19)\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21506)\u001b[0m /home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py:72: DeprecationWarning:  Ensure your client is of type `flwr.client.Client`. Please convert it using the `.to_client()` method before returning it in the `client_fn` you pass to `start_simulation`. We have applied this conversion on your behalf. Not returning a `Client` might trigger an error in future versions of Flower.\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21506)\u001b[0m   client = check_clientfn_returns_client(client_fn(cid))\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "ERROR flwr 2024-03-05 15:04:01,701 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.21\tray::IDLE\n",
      "21512\t0.21\tray::IDLE\n",
      "21507\t0.21\tray::IDLE\n",
      "21505\t0.21\tray::IDLE\n",
      "21504\t0.21\tray::IDLE\n",
      "21515\t0.21\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:04:01,712 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.21\tray::IDLE\n",
      "21512\t0.21\tray::IDLE\n",
      "21507\t0.21\tray::IDLE\n",
      "21505\t0.21\tray::IDLE\n",
      "21504\t0.21\tray::IDLE\n",
      "21515\t0.21\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:01,715 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.52\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.26\tray::DefaultActor\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.22\tray::DefaultActor\n",
      "21505\t0.22\tray::DefaultActor\n",
      "21499\t0.22\tray::DefaultActor\n",
      "21508\t0.22\tray::DefaultActor\n",
      "21504\t0.22\tray::DefaultActor\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:04:01,717 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.52\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.26\tray::DefaultActor\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.22\tray::DefaultActor\n",
      "21505\t0.22\tray::DefaultActor\n",
      "21499\t0.22\tray::DefaultActor\n",
      "21508\t0.22\tray::DefaultActor\n",
      "21504\t0.22\tray::DefaultActor\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:01,721 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.78\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.28\tray::DefaultActor.run\n",
      "21515\t0.26\tray::DefaultActor.run\n",
      "21508\t0.26\tray::DefaultActor.run\n",
      "21512\t0.26\tray::DefaultActor.run\n",
      "21507\t0.24\tray::DefaultActor.run\n",
      "21504\t0.24\tray::DefaultActor.run\n",
      "21505\t0.24\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:04:01,723 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.72\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.27\tray::DefaultActor.run\n",
      "21512\t0.24\tray::DefaultActor.run\n",
      "21515\t0.24\tray::DefaultActor.run\n",
      "21508\t0.23\tray::DefaultActor.run\n",
      "21509\t0.23\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21507\t0.22\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:04:01,724 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.19\tray::IDLE\n",
      "21516\t0.19\tray::IDLE\n",
      "21507\t0.19\tray::IDLE\n",
      "21505\t0.19\tray::IDLE\n",
      "21504\t0.19\tray::IDLE\n",
      "21499\t0.19\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:04:01,726 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.75\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21508\t0.31\tray::DefaultActor.run\n",
      "21505\t0.31\tray::DefaultActor.run\n",
      "21512\t0.31\tray::DefaultActor.run\n",
      "21506\t0.30\tray::DefaultActor.run\n",
      "21516\t0.30\tray::DefaultActor.run\n",
      "21504\t0.30\tray::DefaultActor.run\n",
      "21503\t0.29\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:04:01,726 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.78\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.28\tray::DefaultActor.run\n",
      "21515\t0.26\tray::DefaultActor.run\n",
      "21508\t0.26\tray::DefaultActor.run\n",
      "21512\t0.26\tray::DefaultActor.run\n",
      "21507\t0.24\tray::DefaultActor.run\n",
      "21504\t0.24\tray::DefaultActor.run\n",
      "21505\t0.24\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:04:01,727 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.72\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.27\tray::DefaultActor.run\n",
      "21512\t0.24\tray::DefaultActor.run\n",
      "21515\t0.24\tray::DefaultActor.run\n",
      "21508\t0.23\tray::DefaultActor.run\n",
      "21509\t0.23\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21507\t0.22\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:01,727 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.18\tray::IDLE\n",
      "21516\t0.18\tray::IDLE\n",
      "21505\t0.18\tray::IDLE\n",
      "21507\t0.18\tray::IDLE\n",
      "21499\t0.18\tray::IDLE\n",
      "21504\t0.18\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:04:01,728 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.52\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.26\tray::DefaultActor\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.22\tray::DefaultActor\n",
      "21505\t0.22\tray::DefaultActor\n",
      "21499\t0.22\tray::DefaultActor\n",
      "21508\t0.22\tray::DefaultActor\n",
      "21504\t0.22\tray::DefaultActor\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:04:01,728 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.19\tray::IDLE\n",
      "21516\t0.19\tray::IDLE\n",
      "21507\t0.19\tray::IDLE\n",
      "21505\t0.19\tray::IDLE\n",
      "21504\t0.19\tray::IDLE\n",
      "21499\t0.19\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(DefaultActor pid=21512)\u001b[0m 2\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21516)\u001b[0m starting client: 17 <class 'str'>\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21516)\u001b[0m starting client:  <class 'int'>\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21516)\u001b[0m Creating client model to client: 17\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21516)\u001b[0m Data X: 1331\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21516)\u001b[0m Data Y: 1331\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21512)\u001b[0m starting client: 9 <class 'str'>\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21512)\u001b[0m starting client:  <class 'int'>\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21512)\u001b[0m Creating client model to client: 9\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21512)\u001b[0m Data X: 1331\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21512)\u001b[0m Data Y: 1331\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21512)\u001b[0m Creating client model to client: 9 round 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:04:01,733 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.75\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21508\t0.31\tray::DefaultActor.run\n",
      "21505\t0.31\tray::DefaultActor.run\n",
      "21512\t0.31\tray::DefaultActor.run\n",
      "21506\t0.30\tray::DefaultActor.run\n",
      "21516\t0.30\tray::DefaultActor.run\n",
      "21504\t0.30\tray::DefaultActor.run\n",
      "21503\t0.29\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:01,733 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.89\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21506\t0.36\tray::DefaultActor.run\n",
      "21512\t0.35\tray::DefaultActor.run\n",
      "21508\t0.35\tray::DefaultActor\n",
      "21505\t0.35\tray::DefaultActor\n",
      "21516\t0.34\tray::DefaultActor\n",
      "21503\t0.34\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:04:01,736 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.18\tray::IDLE\n",
      "21516\t0.18\tray::IDLE\n",
      "21505\t0.18\tray::IDLE\n",
      "21507\t0.18\tray::IDLE\n",
      "21499\t0.18\tray::IDLE\n",
      "21504\t0.18\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:01,737 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.52\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.26\tray::DefaultActor\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.22\tray::DefaultActor\n",
      "21505\t0.22\tray::DefaultActor\n",
      "21499\t0.22\tray::DefaultActor\n",
      "21508\t0.22\tray::DefaultActor\n",
      "21504\t0.22\tray::DefaultActor\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:04:01,738 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.99\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21508\t0.38\tray::DefaultActor.run\n",
      "21505\t0.38\tray::DefaultActor.run\n",
      "21506\t0.37\tray::DefaultActor.run\n",
      "21512\t0.37\tray::DefaultActor.run\n",
      "21516\t0.37\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21025\t0.08\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:04:01,740 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.78\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.29\tray::DefaultActor.run\n",
      "21508\t0.28\tray::DefaultActor.run\n",
      "21507\t0.27\tray::DefaultActor.run\n",
      "21506\t0.27\tray::DefaultActor.run\n",
      "21512\t0.27\tray::DefaultActor.run\n",
      "21505\t0.27\tray::DefaultActor.run\n",
      "21504\t0.26\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:04:01,741 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.17\tray::IDLE\n",
      "21512\t0.17\tray::IDLE\n",
      "21507\t0.17\tray::IDLE\n",
      "21505\t0.17\tray::IDLE\n",
      "21511\t0.17\tray::IDLE\n",
      "21504\t0.17\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:04:01,742 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.89\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21506\t0.36\tray::DefaultActor.run\n",
      "21512\t0.35\tray::DefaultActor.run\n",
      "21508\t0.35\tray::DefaultActor\n",
      "21505\t0.35\tray::DefaultActor\n",
      "21516\t0.34\tray::DefaultActor\n",
      "21503\t0.34\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:01,744 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.16\tray::IDLE\n",
      "21516\t0.16\tray::IDLE\n",
      "21511\t0.16\tray::IDLE\n",
      "21507\t0.16\tray::IDLE\n",
      "21504\t0.16\tray::IDLE\n",
      "21505\t0.16\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:04:01,744 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.21\tray::IDLE\n",
      "21512\t0.21\tray::IDLE\n",
      "21507\t0.21\tray::IDLE\n",
      "21505\t0.21\tray::IDLE\n",
      "21504\t0.21\tray::IDLE\n",
      "21515\t0.21\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:04:01,747 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.99\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21508\t0.38\tray::DefaultActor.run\n",
      "21505\t0.38\tray::DefaultActor.run\n",
      "21506\t0.37\tray::DefaultActor.run\n",
      "21512\t0.37\tray::DefaultActor.run\n",
      "21516\t0.37\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21025\t0.08\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:01,748 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.78\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.29\tray::DefaultActor.run\n",
      "21508\t0.28\tray::DefaultActor.run\n",
      "21507\t0.27\tray::DefaultActor.run\n",
      "21506\t0.27\tray::DefaultActor.run\n",
      "21512\t0.27\tray::DefaultActor.run\n",
      "21505\t0.27\tray::DefaultActor.run\n",
      "21504\t0.26\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:01,750 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.17\tray::IDLE\n",
      "21512\t0.17\tray::IDLE\n",
      "21507\t0.17\tray::IDLE\n",
      "21505\t0.17\tray::IDLE\n",
      "21511\t0.17\tray::IDLE\n",
      "21504\t0.17\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:01,752 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.16\tray::IDLE\n",
      "21516\t0.16\tray::IDLE\n",
      "21511\t0.16\tray::IDLE\n",
      "21507\t0.16\tray::IDLE\n",
      "21504\t0.16\tray::IDLE\n",
      "21505\t0.16\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:01,753 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.21\tray::IDLE\n",
      "21512\t0.21\tray::IDLE\n",
      "21507\t0.21\tray::IDLE\n",
      "21505\t0.21\tray::IDLE\n",
      "21504\t0.21\tray::IDLE\n",
      "21515\t0.21\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2024-03-05 15:04:01,758 | server.py:187 | evaluate_round 5 received 0 results and 19 failures\n",
      "DEBUG flwr 2024-03-05 15:04:01,760 | server.py:222 | fit_round 6: strategy sampled 19 clients (out of 19)\n",
      "ERROR flwr 2024-03-05 15:04:01,963 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.21\tray::IDLE\n",
      "21512\t0.21\tray::IDLE\n",
      "21507\t0.21\tray::IDLE\n",
      "21505\t0.21\tray::IDLE\n",
      "21504\t0.21\tray::IDLE\n",
      "21515\t0.21\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:04:01,974 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.21\tray::IDLE\n",
      "21512\t0.21\tray::IDLE\n",
      "21507\t0.21\tray::IDLE\n",
      "21505\t0.21\tray::IDLE\n",
      "21504\t0.21\tray::IDLE\n",
      "21515\t0.21\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:01,991 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.89\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21506\t0.36\tray::DefaultActor.run\n",
      "21512\t0.35\tray::DefaultActor.run\n",
      "21508\t0.35\tray::DefaultActor\n",
      "21505\t0.35\tray::DefaultActor\n",
      "21516\t0.34\tray::DefaultActor\n",
      "21503\t0.34\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:04:01,993 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.89\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21506\t0.36\tray::DefaultActor.run\n",
      "21512\t0.35\tray::DefaultActor.run\n",
      "21508\t0.35\tray::DefaultActor\n",
      "21505\t0.35\tray::DefaultActor\n",
      "21516\t0.34\tray::DefaultActor\n",
      "21503\t0.34\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:01,996 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.99\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21508\t0.38\tray::DefaultActor.run\n",
      "21505\t0.38\tray::DefaultActor.run\n",
      "21506\t0.37\tray::DefaultActor.run\n",
      "21512\t0.37\tray::DefaultActor.run\n",
      "21516\t0.37\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21025\t0.08\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:04:01,998 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.78\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.29\tray::DefaultActor.run\n",
      "21508\t0.28\tray::DefaultActor.run\n",
      "21507\t0.27\tray::DefaultActor.run\n",
      "21506\t0.27\tray::DefaultActor.run\n",
      "21512\t0.27\tray::DefaultActor.run\n",
      "21505\t0.27\tray::DefaultActor.run\n",
      "21504\t0.26\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:04:01,999 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.99\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21508\t0.38\tray::DefaultActor.run\n",
      "21505\t0.38\tray::DefaultActor.run\n",
      "21506\t0.37\tray::DefaultActor.run\n",
      "21512\t0.37\tray::DefaultActor.run\n",
      "21516\t0.37\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21025\t0.08\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:02,000 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.52\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.26\tray::DefaultActor\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.22\tray::DefaultActor\n",
      "21505\t0.22\tray::DefaultActor\n",
      "21499\t0.22\tray::DefaultActor\n",
      "21508\t0.22\tray::DefaultActor\n",
      "21504\t0.22\tray::DefaultActor\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:04:02,002 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.75\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21508\t0.31\tray::DefaultActor.run\n",
      "21505\t0.31\tray::DefaultActor.run\n",
      "21512\t0.31\tray::DefaultActor.run\n",
      "21506\t0.30\tray::DefaultActor.run\n",
      "21516\t0.30\tray::DefaultActor.run\n",
      "21504\t0.30\tray::DefaultActor.run\n",
      "21503\t0.29\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:04:02,002 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.19\tray::IDLE\n",
      "21516\t0.19\tray::IDLE\n",
      "21507\t0.19\tray::IDLE\n",
      "21505\t0.19\tray::IDLE\n",
      "21504\t0.19\tray::IDLE\n",
      "21499\t0.19\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:04:02,004 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.78\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.29\tray::DefaultActor.run\n",
      "21508\t0.28\tray::DefaultActor.run\n",
      "21507\t0.27\tray::DefaultActor.run\n",
      "21506\t0.27\tray::DefaultActor.run\n",
      "21512\t0.27\tray::DefaultActor.run\n",
      "21505\t0.27\tray::DefaultActor.run\n",
      "21504\t0.26\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:02,004 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.78\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.28\tray::DefaultActor.run\n",
      "21515\t0.26\tray::DefaultActor.run\n",
      "21508\t0.26\tray::DefaultActor.run\n",
      "21512\t0.26\tray::DefaultActor.run\n",
      "21507\t0.24\tray::DefaultActor.run\n",
      "21504\t0.24\tray::DefaultActor.run\n",
      "21505\t0.24\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:04:02,007 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.72\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.27\tray::DefaultActor.run\n",
      "21512\t0.24\tray::DefaultActor.run\n",
      "21515\t0.24\tray::DefaultActor.run\n",
      "21508\t0.23\tray::DefaultActor.run\n",
      "21509\t0.23\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21507\t0.22\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:04:02,007 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.52\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.26\tray::DefaultActor\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.22\tray::DefaultActor\n",
      "21505\t0.22\tray::DefaultActor\n",
      "21499\t0.22\tray::DefaultActor\n",
      "21508\t0.22\tray::DefaultActor\n",
      "21504\t0.22\tray::DefaultActor\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:02,008 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.21\tray::IDLE\n",
      "21512\t0.21\tray::IDLE\n",
      "21507\t0.21\tray::IDLE\n",
      "21505\t0.21\tray::IDLE\n",
      "21504\t0.21\tray::IDLE\n",
      "21515\t0.21\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:04:02,010 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.75\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21508\t0.31\tray::DefaultActor.run\n",
      "21505\t0.31\tray::DefaultActor.run\n",
      "21512\t0.31\tray::DefaultActor.run\n",
      "21506\t0.30\tray::DefaultActor.run\n",
      "21516\t0.30\tray::DefaultActor.run\n",
      "21504\t0.30\tray::DefaultActor.run\n",
      "21503\t0.29\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:02,012 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.99\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21508\t0.38\tray::DefaultActor.run\n",
      "21505\t0.38\tray::DefaultActor.run\n",
      "21506\t0.37\tray::DefaultActor.run\n",
      "21512\t0.37\tray::DefaultActor.run\n",
      "21516\t0.37\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21025\t0.08\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:04:02,012 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.17\tray::IDLE\n",
      "21512\t0.17\tray::IDLE\n",
      "21507\t0.17\tray::IDLE\n",
      "21505\t0.17\tray::IDLE\n",
      "21511\t0.17\tray::IDLE\n",
      "21504\t0.17\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:04:02,013 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.19\tray::IDLE\n",
      "21516\t0.19\tray::IDLE\n",
      "21507\t0.19\tray::IDLE\n",
      "21505\t0.19\tray::IDLE\n",
      "21504\t0.19\tray::IDLE\n",
      "21499\t0.19\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:02,015 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.18\tray::IDLE\n",
      "21516\t0.18\tray::IDLE\n",
      "21505\t0.18\tray::IDLE\n",
      "21507\t0.18\tray::IDLE\n",
      "21499\t0.18\tray::IDLE\n",
      "21504\t0.18\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:04:02,018 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.89\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21506\t0.36\tray::DefaultActor.run\n",
      "21512\t0.35\tray::DefaultActor.run\n",
      "21508\t0.35\tray::DefaultActor\n",
      "21505\t0.35\tray::DefaultActor\n",
      "21516\t0.34\tray::DefaultActor\n",
      "21503\t0.34\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:04:02,018 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.78\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.28\tray::DefaultActor.run\n",
      "21515\t0.26\tray::DefaultActor.run\n",
      "21508\t0.26\tray::DefaultActor.run\n",
      "21512\t0.26\tray::DefaultActor.run\n",
      "21507\t0.24\tray::DefaultActor.run\n",
      "21504\t0.24\tray::DefaultActor.run\n",
      "21505\t0.24\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:02,018 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.16\tray::IDLE\n",
      "21516\t0.16\tray::IDLE\n",
      "21511\t0.16\tray::IDLE\n",
      "21507\t0.16\tray::IDLE\n",
      "21504\t0.16\tray::IDLE\n",
      "21505\t0.16\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:04:02,019 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.72\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.27\tray::DefaultActor.run\n",
      "21512\t0.24\tray::DefaultActor.run\n",
      "21515\t0.24\tray::DefaultActor.run\n",
      "21508\t0.23\tray::DefaultActor.run\n",
      "21509\t0.23\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21507\t0.22\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:02,022 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.21\tray::IDLE\n",
      "21512\t0.21\tray::IDLE\n",
      "21507\t0.21\tray::IDLE\n",
      "21505\t0.21\tray::IDLE\n",
      "21504\t0.21\tray::IDLE\n",
      "21515\t0.21\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:04:02,027 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.99\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21508\t0.38\tray::DefaultActor.run\n",
      "21505\t0.38\tray::DefaultActor.run\n",
      "21506\t0.37\tray::DefaultActor.run\n",
      "21512\t0.37\tray::DefaultActor.run\n",
      "21516\t0.37\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21025\t0.08\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:02,029 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.17\tray::IDLE\n",
      "21512\t0.17\tray::IDLE\n",
      "21507\t0.17\tray::IDLE\n",
      "21505\t0.17\tray::IDLE\n",
      "21511\t0.17\tray::IDLE\n",
      "21504\t0.17\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:02,033 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.18\tray::IDLE\n",
      "21516\t0.18\tray::IDLE\n",
      "21505\t0.18\tray::IDLE\n",
      "21507\t0.18\tray::IDLE\n",
      "21499\t0.18\tray::IDLE\n",
      "21504\t0.18\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:02,034 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.89\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21506\t0.36\tray::DefaultActor.run\n",
      "21512\t0.35\tray::DefaultActor.run\n",
      "21508\t0.35\tray::DefaultActor\n",
      "21505\t0.35\tray::DefaultActor\n",
      "21516\t0.34\tray::DefaultActor\n",
      "21503\t0.34\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:02,037 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.16\tray::IDLE\n",
      "21516\t0.16\tray::IDLE\n",
      "21511\t0.16\tray::IDLE\n",
      "21507\t0.16\tray::IDLE\n",
      "21504\t0.16\tray::IDLE\n",
      "21505\t0.16\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2024-03-05 15:04:09,614 | server.py:236 | fit_round 6 received 4 results and 15 failures\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(DefaultActor pid=21506)\u001b[0m 2\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21506)\u001b[0m starting client: 18 <class 'str'>\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21506)\u001b[0m starting client:  <class 'int'>\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21506)\u001b[0m Creating client model to client: 18\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21506)\u001b[0m Data X: 1331\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21506)\u001b[0m Data Y: 1331\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21506)\u001b[0m Creating client model to client: 18 round 6\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "Saving round 6 aggregated_ndarrays...\n",
      "result_unbalanced_epoch_1_rounds_10_cycle_0/checkpoints/round-6-weights.npz\n",
      "\n",
      "Evaluating global model round 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO flwr 2024-03-05 15:04:15,116 | server.py:125 | fit progress: (6, 0.7448371648788452, {'accuracy': 0.5410522222518921}, 88.86510824699872)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2024-03-05 15:04:15,118 | server.py:173 | evaluate_round 6: strategy sampled 19 clients (out of 19)\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21506)\u001b[0m /home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py:72: DeprecationWarning:  Ensure your client is of type `flwr.client.Client`. Please convert it using the `.to_client()` method before returning it in the `client_fn` you pass to `start_simulation`. We have applied this conversion on your behalf. Not returning a `Client` might trigger an error in future versions of Flower.\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21506)\u001b[0m   client = check_clientfn_returns_client(client_fn(cid))\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "ERROR flwr 2024-03-05 15:04:15,319 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.89\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21506\t0.36\tray::DefaultActor.run\n",
      "21512\t0.35\tray::DefaultActor.run\n",
      "21508\t0.35\tray::DefaultActor\n",
      "21505\t0.35\tray::DefaultActor\n",
      "21516\t0.34\tray::DefaultActor\n",
      "21503\t0.34\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:04:15,330 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.89\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21506\t0.36\tray::DefaultActor.run\n",
      "21512\t0.35\tray::DefaultActor.run\n",
      "21508\t0.35\tray::DefaultActor\n",
      "21505\t0.35\tray::DefaultActor\n",
      "21516\t0.34\tray::DefaultActor\n",
      "21503\t0.34\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:15,335 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.21\tray::IDLE\n",
      "21512\t0.21\tray::IDLE\n",
      "21507\t0.21\tray::IDLE\n",
      "21505\t0.21\tray::IDLE\n",
      "21504\t0.21\tray::IDLE\n",
      "21515\t0.21\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:04:15,337 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.21\tray::IDLE\n",
      "21512\t0.21\tray::IDLE\n",
      "21507\t0.21\tray::IDLE\n",
      "21505\t0.21\tray::IDLE\n",
      "21504\t0.21\tray::IDLE\n",
      "21515\t0.21\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:15,341 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.99\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21508\t0.38\tray::DefaultActor.run\n",
      "21505\t0.38\tray::DefaultActor.run\n",
      "21506\t0.37\tray::DefaultActor.run\n",
      "21512\t0.37\tray::DefaultActor.run\n",
      "21516\t0.37\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21025\t0.08\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:04:15,342 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.18\tray::IDLE\n",
      "21516\t0.18\tray::IDLE\n",
      "21505\t0.18\tray::IDLE\n",
      "21507\t0.18\tray::IDLE\n",
      "21499\t0.18\tray::IDLE\n",
      "21504\t0.18\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:04:15,343 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.72\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.27\tray::DefaultActor.run\n",
      "21512\t0.24\tray::DefaultActor.run\n",
      "21515\t0.24\tray::DefaultActor.run\n",
      "21508\t0.23\tray::DefaultActor.run\n",
      "21509\t0.23\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21507\t0.22\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:04:15,343 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.17\tray::IDLE\n",
      "21512\t0.17\tray::IDLE\n",
      "21507\t0.17\tray::IDLE\n",
      "21505\t0.17\tray::IDLE\n",
      "21511\t0.17\tray::IDLE\n",
      "21504\t0.17\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:04:15,344 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.19\tray::IDLE\n",
      "21516\t0.19\tray::IDLE\n",
      "21507\t0.19\tray::IDLE\n",
      "21505\t0.19\tray::IDLE\n",
      "21504\t0.19\tray::IDLE\n",
      "21499\t0.19\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:04:15,345 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.99\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21508\t0.38\tray::DefaultActor.run\n",
      "21505\t0.38\tray::DefaultActor.run\n",
      "21506\t0.37\tray::DefaultActor.run\n",
      "21512\t0.37\tray::DefaultActor.run\n",
      "21516\t0.37\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21025\t0.08\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:15,348 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.52\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.26\tray::DefaultActor\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.22\tray::DefaultActor\n",
      "21505\t0.22\tray::DefaultActor\n",
      "21499\t0.22\tray::DefaultActor\n",
      "21508\t0.22\tray::DefaultActor\n",
      "21504\t0.22\tray::DefaultActor\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:04:15,349 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.18\tray::IDLE\n",
      "21516\t0.18\tray::IDLE\n",
      "21505\t0.18\tray::IDLE\n",
      "21507\t0.18\tray::IDLE\n",
      "21499\t0.18\tray::IDLE\n",
      "21504\t0.18\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:15,350 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.72\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.27\tray::DefaultActor.run\n",
      "21512\t0.24\tray::DefaultActor.run\n",
      "21515\t0.24\tray::DefaultActor.run\n",
      "21508\t0.23\tray::DefaultActor.run\n",
      "21509\t0.23\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21507\t0.22\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:04:15,354 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.16\tray::IDLE\n",
      "21516\t0.16\tray::IDLE\n",
      "21511\t0.16\tray::IDLE\n",
      "21507\t0.16\tray::IDLE\n",
      "21504\t0.16\tray::IDLE\n",
      "21505\t0.16\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:04:15,355 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.17\tray::IDLE\n",
      "21512\t0.17\tray::IDLE\n",
      "21507\t0.17\tray::IDLE\n",
      "21505\t0.17\tray::IDLE\n",
      "21511\t0.17\tray::IDLE\n",
      "21504\t0.17\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:15,357 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.78\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.29\tray::DefaultActor.run\n",
      "21508\t0.28\tray::DefaultActor.run\n",
      "21507\t0.27\tray::DefaultActor.run\n",
      "21506\t0.27\tray::DefaultActor.run\n",
      "21512\t0.27\tray::DefaultActor.run\n",
      "21505\t0.27\tray::DefaultActor.run\n",
      "21504\t0.26\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:04:15,357 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.19\tray::IDLE\n",
      "21516\t0.19\tray::IDLE\n",
      "21507\t0.19\tray::IDLE\n",
      "21505\t0.19\tray::IDLE\n",
      "21504\t0.19\tray::IDLE\n",
      "21499\t0.19\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:15,358 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.75\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21508\t0.31\tray::DefaultActor.run\n",
      "21505\t0.31\tray::DefaultActor.run\n",
      "21512\t0.31\tray::DefaultActor.run\n",
      "21506\t0.30\tray::DefaultActor.run\n",
      "21516\t0.30\tray::DefaultActor.run\n",
      "21504\t0.30\tray::DefaultActor.run\n",
      "21503\t0.29\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:04:15,359 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.78\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.28\tray::DefaultActor.run\n",
      "21515\t0.26\tray::DefaultActor.run\n",
      "21508\t0.26\tray::DefaultActor.run\n",
      "21512\t0.26\tray::DefaultActor.run\n",
      "21507\t0.24\tray::DefaultActor.run\n",
      "21504\t0.24\tray::DefaultActor.run\n",
      "21505\t0.24\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:04:15,360 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.52\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.26\tray::DefaultActor\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.22\tray::DefaultActor\n",
      "21505\t0.22\tray::DefaultActor\n",
      "21499\t0.22\tray::DefaultActor\n",
      "21508\t0.22\tray::DefaultActor\n",
      "21504\t0.22\tray::DefaultActor\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:15,365 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.16\tray::IDLE\n",
      "21516\t0.16\tray::IDLE\n",
      "21511\t0.16\tray::IDLE\n",
      "21507\t0.16\tray::IDLE\n",
      "21504\t0.16\tray::IDLE\n",
      "21505\t0.16\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:15,367 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.89\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21506\t0.36\tray::DefaultActor.run\n",
      "21512\t0.35\tray::DefaultActor.run\n",
      "21508\t0.35\tray::DefaultActor\n",
      "21505\t0.35\tray::DefaultActor\n",
      "21516\t0.34\tray::DefaultActor\n",
      "21503\t0.34\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:04:15,369 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.78\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.29\tray::DefaultActor.run\n",
      "21508\t0.28\tray::DefaultActor.run\n",
      "21507\t0.27\tray::DefaultActor.run\n",
      "21506\t0.27\tray::DefaultActor.run\n",
      "21512\t0.27\tray::DefaultActor.run\n",
      "21505\t0.27\tray::DefaultActor.run\n",
      "21504\t0.26\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:04:15,370 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.75\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21508\t0.31\tray::DefaultActor.run\n",
      "21505\t0.31\tray::DefaultActor.run\n",
      "21512\t0.31\tray::DefaultActor.run\n",
      "21506\t0.30\tray::DefaultActor.run\n",
      "21516\t0.30\tray::DefaultActor.run\n",
      "21504\t0.30\tray::DefaultActor.run\n",
      "21503\t0.29\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:15,371 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.78\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.28\tray::DefaultActor.run\n",
      "21515\t0.26\tray::DefaultActor.run\n",
      "21508\t0.26\tray::DefaultActor.run\n",
      "21512\t0.26\tray::DefaultActor.run\n",
      "21507\t0.24\tray::DefaultActor.run\n",
      "21504\t0.24\tray::DefaultActor.run\n",
      "21505\t0.24\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:15,373 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.89\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21506\t0.36\tray::DefaultActor.run\n",
      "21512\t0.35\tray::DefaultActor.run\n",
      "21508\t0.35\tray::DefaultActor\n",
      "21505\t0.35\tray::DefaultActor\n",
      "21516\t0.34\tray::DefaultActor\n",
      "21503\t0.34\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "DEBUG flwr 2024-03-05 15:04:15,379 | server.py:187 | evaluate_round 6 received 0 results and 19 failures\n",
      "DEBUG flwr 2024-03-05 15:04:15,381 | server.py:222 | fit_round 7: strategy sampled 19 clients (out of 19)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(DefaultActor pid=21512)\u001b[0m 2\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21506)\u001b[0m starting client: 2 <class 'str'>\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21506)\u001b[0m starting client:  <class 'int'>\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21506)\u001b[0m Creating client model to client: 2\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21506)\u001b[0m Data X: 1331\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21506)\u001b[0m Data Y: 1331\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21506)\u001b[0m Creating client model to client: 2 round 7\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21505)\u001b[0m starting client: 9 <class 'str'>\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21505)\u001b[0m starting client:  <class 'int'>\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21505)\u001b[0m Creating client model to client: 9\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21505)\u001b[0m Data X: 1331\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21505)\u001b[0m Data Y: 1331\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21505)\u001b[0m Creating client model to client: 9 round 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:04:15,611 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.89\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21506\t0.36\tray::DefaultActor.run\n",
      "21512\t0.35\tray::DefaultActor.run\n",
      "21508\t0.35\tray::DefaultActor\n",
      "21505\t0.35\tray::DefaultActor\n",
      "21516\t0.34\tray::DefaultActor\n",
      "21503\t0.34\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:04:15,629 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.89\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21506\t0.36\tray::DefaultActor.run\n",
      "21512\t0.35\tray::DefaultActor.run\n",
      "21508\t0.35\tray::DefaultActor\n",
      "21505\t0.35\tray::DefaultActor\n",
      "21516\t0.34\tray::DefaultActor\n",
      "21503\t0.34\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:15,648 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.78\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.29\tray::DefaultActor.run\n",
      "21508\t0.28\tray::DefaultActor.run\n",
      "21507\t0.27\tray::DefaultActor.run\n",
      "21506\t0.27\tray::DefaultActor.run\n",
      "21512\t0.27\tray::DefaultActor.run\n",
      "21505\t0.27\tray::DefaultActor.run\n",
      "21504\t0.26\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:04:15,650 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.78\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.28\tray::DefaultActor.run\n",
      "21515\t0.26\tray::DefaultActor.run\n",
      "21508\t0.26\tray::DefaultActor.run\n",
      "21512\t0.26\tray::DefaultActor.run\n",
      "21507\t0.24\tray::DefaultActor.run\n",
      "21504\t0.24\tray::DefaultActor.run\n",
      "21505\t0.24\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:04:15,653 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.16\tray::IDLE\n",
      "21516\t0.16\tray::IDLE\n",
      "21511\t0.16\tray::IDLE\n",
      "21507\t0.16\tray::IDLE\n",
      "21504\t0.16\tray::IDLE\n",
      "21505\t0.16\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:04:15,654 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.78\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.29\tray::DefaultActor.run\n",
      "21508\t0.28\tray::DefaultActor.run\n",
      "21507\t0.27\tray::DefaultActor.run\n",
      "21506\t0.27\tray::DefaultActor.run\n",
      "21512\t0.27\tray::DefaultActor.run\n",
      "21505\t0.27\tray::DefaultActor.run\n",
      "21504\t0.26\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:04:15,654 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.99\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21508\t0.38\tray::DefaultActor.run\n",
      "21505\t0.38\tray::DefaultActor.run\n",
      "21506\t0.37\tray::DefaultActor.run\n",
      "21512\t0.37\tray::DefaultActor.run\n",
      "21516\t0.37\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21025\t0.08\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:04:15,655 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.78\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.28\tray::DefaultActor.run\n",
      "21515\t0.26\tray::DefaultActor.run\n",
      "21508\t0.26\tray::DefaultActor.run\n",
      "21512\t0.26\tray::DefaultActor.run\n",
      "21507\t0.24\tray::DefaultActor.run\n",
      "21504\t0.24\tray::DefaultActor.run\n",
      "21505\t0.24\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:15,656 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.75\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21508\t0.31\tray::DefaultActor.run\n",
      "21505\t0.31\tray::DefaultActor.run\n",
      "21512\t0.31\tray::DefaultActor.run\n",
      "21506\t0.30\tray::DefaultActor.run\n",
      "21516\t0.30\tray::DefaultActor.run\n",
      "21504\t0.30\tray::DefaultActor.run\n",
      "21503\t0.29\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:04:15,656 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.16\tray::IDLE\n",
      "21516\t0.16\tray::IDLE\n",
      "21511\t0.16\tray::IDLE\n",
      "21507\t0.16\tray::IDLE\n",
      "21504\t0.16\tray::IDLE\n",
      "21505\t0.16\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:15,657 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.52\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.26\tray::DefaultActor\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.22\tray::DefaultActor\n",
      "21505\t0.22\tray::DefaultActor\n",
      "21499\t0.22\tray::DefaultActor\n",
      "21508\t0.22\tray::DefaultActor\n",
      "21504\t0.22\tray::DefaultActor\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:04:15,659 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.17\tray::IDLE\n",
      "21512\t0.17\tray::IDLE\n",
      "21507\t0.17\tray::IDLE\n",
      "21505\t0.17\tray::IDLE\n",
      "21511\t0.17\tray::IDLE\n",
      "21504\t0.17\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:04:15,661 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.72\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.27\tray::DefaultActor.run\n",
      "21512\t0.24\tray::DefaultActor.run\n",
      "21515\t0.24\tray::DefaultActor.run\n",
      "21508\t0.23\tray::DefaultActor.run\n",
      "21509\t0.23\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21507\t0.22\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:04:15,661 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.89\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21506\t0.36\tray::DefaultActor.run\n",
      "21512\t0.35\tray::DefaultActor.run\n",
      "21508\t0.35\tray::DefaultActor\n",
      "21505\t0.35\tray::DefaultActor\n",
      "21516\t0.34\tray::DefaultActor\n",
      "21503\t0.34\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:04:15,661 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.21\tray::IDLE\n",
      "21512\t0.21\tray::IDLE\n",
      "21507\t0.21\tray::IDLE\n",
      "21505\t0.21\tray::IDLE\n",
      "21504\t0.21\tray::IDLE\n",
      "21515\t0.21\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:04:15,662 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.18\tray::IDLE\n",
      "21516\t0.18\tray::IDLE\n",
      "21505\t0.18\tray::IDLE\n",
      "21507\t0.18\tray::IDLE\n",
      "21499\t0.18\tray::IDLE\n",
      "21504\t0.18\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:04:15,665 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.75\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21508\t0.31\tray::DefaultActor.run\n",
      "21505\t0.31\tray::DefaultActor.run\n",
      "21512\t0.31\tray::DefaultActor.run\n",
      "21506\t0.30\tray::DefaultActor.run\n",
      "21516\t0.30\tray::DefaultActor.run\n",
      "21504\t0.30\tray::DefaultActor.run\n",
      "21503\t0.29\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:04:15,665 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.99\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21508\t0.38\tray::DefaultActor.run\n",
      "21505\t0.38\tray::DefaultActor.run\n",
      "21506\t0.37\tray::DefaultActor.run\n",
      "21512\t0.37\tray::DefaultActor.run\n",
      "21516\t0.37\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21025\t0.08\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:04:15,667 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.19\tray::IDLE\n",
      "21516\t0.19\tray::IDLE\n",
      "21507\t0.19\tray::IDLE\n",
      "21505\t0.19\tray::IDLE\n",
      "21504\t0.19\tray::IDLE\n",
      "21499\t0.19\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:04:15,667 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.78\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.29\tray::DefaultActor.run\n",
      "21508\t0.28\tray::DefaultActor.run\n",
      "21507\t0.27\tray::DefaultActor.run\n",
      "21506\t0.27\tray::DefaultActor.run\n",
      "21512\t0.27\tray::DefaultActor.run\n",
      "21505\t0.27\tray::DefaultActor.run\n",
      "21504\t0.26\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:04:15,668 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.75\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21508\t0.31\tray::DefaultActor.run\n",
      "21505\t0.31\tray::DefaultActor.run\n",
      "21512\t0.31\tray::DefaultActor.run\n",
      "21506\t0.30\tray::DefaultActor.run\n",
      "21516\t0.30\tray::DefaultActor.run\n",
      "21504\t0.30\tray::DefaultActor.run\n",
      "21503\t0.29\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:04:15,672 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.52\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.26\tray::DefaultActor\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.22\tray::DefaultActor\n",
      "21505\t0.22\tray::DefaultActor\n",
      "21499\t0.22\tray::DefaultActor\n",
      "21508\t0.22\tray::DefaultActor\n",
      "21504\t0.22\tray::DefaultActor\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:15,674 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.17\tray::IDLE\n",
      "21512\t0.17\tray::IDLE\n",
      "21507\t0.17\tray::IDLE\n",
      "21505\t0.17\tray::IDLE\n",
      "21511\t0.17\tray::IDLE\n",
      "21504\t0.17\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:15,675 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.72\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.27\tray::DefaultActor.run\n",
      "21512\t0.24\tray::DefaultActor.run\n",
      "21515\t0.24\tray::DefaultActor.run\n",
      "21508\t0.23\tray::DefaultActor.run\n",
      "21509\t0.23\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21507\t0.22\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:15,678 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.89\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21506\t0.36\tray::DefaultActor.run\n",
      "21512\t0.35\tray::DefaultActor.run\n",
      "21508\t0.35\tray::DefaultActor\n",
      "21505\t0.35\tray::DefaultActor\n",
      "21516\t0.34\tray::DefaultActor\n",
      "21503\t0.34\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:15,678 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.21\tray::IDLE\n",
      "21512\t0.21\tray::IDLE\n",
      "21507\t0.21\tray::IDLE\n",
      "21505\t0.21\tray::IDLE\n",
      "21504\t0.21\tray::IDLE\n",
      "21515\t0.21\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:04:15,681 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.18\tray::IDLE\n",
      "21516\t0.18\tray::IDLE\n",
      "21505\t0.18\tray::IDLE\n",
      "21507\t0.18\tray::IDLE\n",
      "21499\t0.18\tray::IDLE\n",
      "21504\t0.18\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:15,683 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.75\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21508\t0.31\tray::DefaultActor.run\n",
      "21505\t0.31\tray::DefaultActor.run\n",
      "21512\t0.31\tray::DefaultActor.run\n",
      "21506\t0.30\tray::DefaultActor.run\n",
      "21516\t0.30\tray::DefaultActor.run\n",
      "21504\t0.30\tray::DefaultActor.run\n",
      "21503\t0.29\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:15,687 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.19\tray::IDLE\n",
      "21516\t0.19\tray::IDLE\n",
      "21507\t0.19\tray::IDLE\n",
      "21505\t0.19\tray::IDLE\n",
      "21504\t0.19\tray::IDLE\n",
      "21499\t0.19\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:15,689 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.78\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.29\tray::DefaultActor.run\n",
      "21508\t0.28\tray::DefaultActor.run\n",
      "21507\t0.27\tray::DefaultActor.run\n",
      "21506\t0.27\tray::DefaultActor.run\n",
      "21512\t0.27\tray::DefaultActor.run\n",
      "21505\t0.27\tray::DefaultActor.run\n",
      "21504\t0.26\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "DEBUG flwr 2024-03-05 15:04:23,406 | server.py:236 | fit_round 7 received 4 results and 15 failures\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(DefaultActor pid=21505)\u001b[0m 2\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21506)\u001b[0m starting client: 18 <class 'str'>\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21506)\u001b[0m starting client:  <class 'int'>\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21506)\u001b[0m Creating client model to client: 18\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21506)\u001b[0m Data X: 1331\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21506)\u001b[0m Data Y: 1331\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21505)\u001b[0m Creating client model to client: 5 round 7\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "Saving round 7 aggregated_ndarrays...\n",
      "result_unbalanced_epoch_1_rounds_10_cycle_0/checkpoints/round-7-weights.npz\n",
      "\n",
      "Evaluating global model round 7\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO flwr 2024-03-05 15:04:28,988 | server.py:125 | fit progress: (7, 0.7352955341339111, {'accuracy': 0.559216320514679}, 102.73729575899779)\n",
      "DEBUG flwr 2024-03-05 15:04:28,990 | server.py:173 | evaluate_round 7: strategy sampled 19 clients (out of 19)\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21506)\u001b[0m /home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py:72: DeprecationWarning:  Ensure your client is of type `flwr.client.Client`. Please convert it using the `.to_client()` method before returning it in the `client_fn` you pass to `start_simulation`. We have applied this conversion on your behalf. Not returning a `Client` might trigger an error in future versions of Flower.\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21506)\u001b[0m   client = check_clientfn_returns_client(client_fn(cid))\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "ERROR flwr 2024-03-05 15:04:29,191 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.78\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.29\tray::DefaultActor.run\n",
      "21508\t0.28\tray::DefaultActor.run\n",
      "21507\t0.27\tray::DefaultActor.run\n",
      "21506\t0.27\tray::DefaultActor.run\n",
      "21512\t0.27\tray::DefaultActor.run\n",
      "21505\t0.27\tray::DefaultActor.run\n",
      "21504\t0.26\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:04:29,202 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.78\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.29\tray::DefaultActor.run\n",
      "21508\t0.28\tray::DefaultActor.run\n",
      "21507\t0.27\tray::DefaultActor.run\n",
      "21506\t0.27\tray::DefaultActor.run\n",
      "21512\t0.27\tray::DefaultActor.run\n",
      "21505\t0.27\tray::DefaultActor.run\n",
      "21504\t0.26\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:29,210 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.75\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21508\t0.31\tray::DefaultActor.run\n",
      "21505\t0.31\tray::DefaultActor.run\n",
      "21512\t0.31\tray::DefaultActor.run\n",
      "21506\t0.30\tray::DefaultActor.run\n",
      "21516\t0.30\tray::DefaultActor.run\n",
      "21504\t0.30\tray::DefaultActor.run\n",
      "21503\t0.29\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:04:29,210 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.72\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.27\tray::DefaultActor.run\n",
      "21512\t0.24\tray::DefaultActor.run\n",
      "21515\t0.24\tray::DefaultActor.run\n",
      "21508\t0.23\tray::DefaultActor.run\n",
      "21509\t0.23\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21507\t0.22\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:04:29,211 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.19\tray::IDLE\n",
      "21516\t0.19\tray::IDLE\n",
      "21507\t0.19\tray::IDLE\n",
      "21505\t0.19\tray::IDLE\n",
      "21504\t0.19\tray::IDLE\n",
      "21499\t0.19\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:04:29,215 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.75\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21508\t0.31\tray::DefaultActor.run\n",
      "21505\t0.31\tray::DefaultActor.run\n",
      "21512\t0.31\tray::DefaultActor.run\n",
      "21506\t0.30\tray::DefaultActor.run\n",
      "21516\t0.30\tray::DefaultActor.run\n",
      "21504\t0.30\tray::DefaultActor.run\n",
      "21503\t0.29\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:04:29,215 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.99\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21508\t0.38\tray::DefaultActor.run\n",
      "21505\t0.38\tray::DefaultActor.run\n",
      "21506\t0.37\tray::DefaultActor.run\n",
      "21512\t0.37\tray::DefaultActor.run\n",
      "21516\t0.37\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21025\t0.08\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:04:29,216 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.78\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.28\tray::DefaultActor.run\n",
      "21515\t0.26\tray::DefaultActor.run\n",
      "21508\t0.26\tray::DefaultActor.run\n",
      "21512\t0.26\tray::DefaultActor.run\n",
      "21507\t0.24\tray::DefaultActor.run\n",
      "21504\t0.24\tray::DefaultActor.run\n",
      "21505\t0.24\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:04:29,217 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.17\tray::IDLE\n",
      "21512\t0.17\tray::IDLE\n",
      "21507\t0.17\tray::IDLE\n",
      "21505\t0.17\tray::IDLE\n",
      "21511\t0.17\tray::IDLE\n",
      "21504\t0.17\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:04:29,217 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.16\tray::IDLE\n",
      "21516\t0.16\tray::IDLE\n",
      "21511\t0.16\tray::IDLE\n",
      "21507\t0.16\tray::IDLE\n",
      "21504\t0.16\tray::IDLE\n",
      "21505\t0.16\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:04:29,218 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.72\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.27\tray::DefaultActor.run\n",
      "21512\t0.24\tray::DefaultActor.run\n",
      "21515\t0.24\tray::DefaultActor.run\n",
      "21508\t0.23\tray::DefaultActor.run\n",
      "21509\t0.23\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21507\t0.22\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:29,219 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.19\tray::IDLE\n",
      "21516\t0.19\tray::IDLE\n",
      "21507\t0.19\tray::IDLE\n",
      "21505\t0.19\tray::IDLE\n",
      "21504\t0.19\tray::IDLE\n",
      "21499\t0.19\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:29,225 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.18\tray::IDLE\n",
      "21516\t0.18\tray::IDLE\n",
      "21505\t0.18\tray::IDLE\n",
      "21507\t0.18\tray::IDLE\n",
      "21499\t0.18\tray::IDLE\n",
      "21504\t0.18\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:04:29,225 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.99\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21508\t0.38\tray::DefaultActor.run\n",
      "21505\t0.38\tray::DefaultActor.run\n",
      "21506\t0.37\tray::DefaultActor.run\n",
      "21512\t0.37\tray::DefaultActor.run\n",
      "21516\t0.37\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21025\t0.08\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:29,225 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.89\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21506\t0.36\tray::DefaultActor.run\n",
      "21512\t0.35\tray::DefaultActor.run\n",
      "21508\t0.35\tray::DefaultActor\n",
      "21505\t0.35\tray::DefaultActor\n",
      "21516\t0.34\tray::DefaultActor\n",
      "21503\t0.34\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:04:29,226 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.52\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.26\tray::DefaultActor\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.22\tray::DefaultActor\n",
      "21505\t0.22\tray::DefaultActor\n",
      "21499\t0.22\tray::DefaultActor\n",
      "21508\t0.22\tray::DefaultActor\n",
      "21504\t0.22\tray::DefaultActor\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:04:29,227 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.78\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.28\tray::DefaultActor.run\n",
      "21515\t0.26\tray::DefaultActor.run\n",
      "21508\t0.26\tray::DefaultActor.run\n",
      "21512\t0.26\tray::DefaultActor.run\n",
      "21507\t0.24\tray::DefaultActor.run\n",
      "21504\t0.24\tray::DefaultActor.run\n",
      "21505\t0.24\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:29,228 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.78\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.29\tray::DefaultActor.run\n",
      "21508\t0.28\tray::DefaultActor.run\n",
      "21507\t0.27\tray::DefaultActor.run\n",
      "21506\t0.27\tray::DefaultActor.run\n",
      "21512\t0.27\tray::DefaultActor.run\n",
      "21505\t0.27\tray::DefaultActor.run\n",
      "21504\t0.26\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(DefaultActor pid=21506)\u001b[0m 2\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21506)\u001b[0m starting client: 16 <class 'str'>\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21506)\u001b[0m starting client:  <class 'int'>\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21506)\u001b[0m Creating client model to client: 16\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21506)\u001b[0m Data X: 1331\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21506)\u001b[0m Data Y: 1331\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21506)\u001b[0m Creating client model to client: 16 round 8\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21505)\u001b[0m starting client: 12 <class 'str'>\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21505)\u001b[0m starting client:  <class 'int'>\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21505)\u001b[0m Creating client model to client: 12\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21505)\u001b[0m Data X: 1331\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21505)\u001b[0m Data Y: 1331\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21505)\u001b[0m Creating client model to client: 12 round 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:04:29,229 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.17\tray::IDLE\n",
      "21512\t0.17\tray::IDLE\n",
      "21507\t0.17\tray::IDLE\n",
      "21505\t0.17\tray::IDLE\n",
      "21511\t0.17\tray::IDLE\n",
      "21504\t0.17\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:29,230 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.16\tray::IDLE\n",
      "21516\t0.16\tray::IDLE\n",
      "21511\t0.16\tray::IDLE\n",
      "21507\t0.16\tray::IDLE\n",
      "21504\t0.16\tray::IDLE\n",
      "21505\t0.16\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:29,232 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.21\tray::IDLE\n",
      "21512\t0.21\tray::IDLE\n",
      "21507\t0.21\tray::IDLE\n",
      "21505\t0.21\tray::IDLE\n",
      "21504\t0.21\tray::IDLE\n",
      "21515\t0.21\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:04:29,234 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.18\tray::IDLE\n",
      "21516\t0.18\tray::IDLE\n",
      "21505\t0.18\tray::IDLE\n",
      "21507\t0.18\tray::IDLE\n",
      "21499\t0.18\tray::IDLE\n",
      "21504\t0.18\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:04:29,237 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.89\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21506\t0.36\tray::DefaultActor.run\n",
      "21512\t0.35\tray::DefaultActor.run\n",
      "21508\t0.35\tray::DefaultActor\n",
      "21505\t0.35\tray::DefaultActor\n",
      "21516\t0.34\tray::DefaultActor\n",
      "21503\t0.34\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:29,239 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.52\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.26\tray::DefaultActor\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.22\tray::DefaultActor\n",
      "21505\t0.22\tray::DefaultActor\n",
      "21499\t0.22\tray::DefaultActor\n",
      "21508\t0.22\tray::DefaultActor\n",
      "21504\t0.22\tray::DefaultActor\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:29,242 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.78\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.29\tray::DefaultActor.run\n",
      "21508\t0.28\tray::DefaultActor.run\n",
      "21507\t0.27\tray::DefaultActor.run\n",
      "21506\t0.27\tray::DefaultActor.run\n",
      "21512\t0.27\tray::DefaultActor.run\n",
      "21505\t0.27\tray::DefaultActor.run\n",
      "21504\t0.26\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:29,245 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.21\tray::IDLE\n",
      "21512\t0.21\tray::IDLE\n",
      "21507\t0.21\tray::IDLE\n",
      "21505\t0.21\tray::IDLE\n",
      "21504\t0.21\tray::IDLE\n",
      "21515\t0.21\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "DEBUG flwr 2024-03-05 15:04:29,252 | server.py:187 | evaluate_round 7 received 0 results and 19 failures\n",
      "DEBUG flwr 2024-03-05 15:04:29,254 | server.py:222 | fit_round 8: strategy sampled 19 clients (out of 19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:04:29,487 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.21\tray::IDLE\n",
      "21512\t0.21\tray::IDLE\n",
      "21507\t0.21\tray::IDLE\n",
      "21505\t0.21\tray::IDLE\n",
      "21504\t0.21\tray::IDLE\n",
      "21515\t0.21\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:04:29,499 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.21\tray::IDLE\n",
      "21512\t0.21\tray::IDLE\n",
      "21507\t0.21\tray::IDLE\n",
      "21505\t0.21\tray::IDLE\n",
      "21504\t0.21\tray::IDLE\n",
      "21515\t0.21\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:29,526 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.52\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.26\tray::DefaultActor\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.22\tray::DefaultActor\n",
      "21505\t0.22\tray::DefaultActor\n",
      "21499\t0.22\tray::DefaultActor\n",
      "21508\t0.22\tray::DefaultActor\n",
      "21504\t0.22\tray::DefaultActor\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:04:29,528 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.78\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.29\tray::DefaultActor.run\n",
      "21508\t0.28\tray::DefaultActor.run\n",
      "21507\t0.27\tray::DefaultActor.run\n",
      "21506\t0.27\tray::DefaultActor.run\n",
      "21512\t0.27\tray::DefaultActor.run\n",
      "21505\t0.27\tray::DefaultActor.run\n",
      "21504\t0.26\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:04:29,529 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.52\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.26\tray::DefaultActor\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.22\tray::DefaultActor\n",
      "21505\t0.22\tray::DefaultActor\n",
      "21499\t0.22\tray::DefaultActor\n",
      "21508\t0.22\tray::DefaultActor\n",
      "21504\t0.22\tray::DefaultActor\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:29,531 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.78\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.29\tray::DefaultActor.run\n",
      "21508\t0.28\tray::DefaultActor.run\n",
      "21507\t0.27\tray::DefaultActor.run\n",
      "21506\t0.27\tray::DefaultActor.run\n",
      "21512\t0.27\tray::DefaultActor.run\n",
      "21505\t0.27\tray::DefaultActor.run\n",
      "21504\t0.26\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:29,534 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.16\tray::IDLE\n",
      "21516\t0.16\tray::IDLE\n",
      "21511\t0.16\tray::IDLE\n",
      "21507\t0.16\tray::IDLE\n",
      "21504\t0.16\tray::IDLE\n",
      "21505\t0.16\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:04:29,535 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.99\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21508\t0.38\tray::DefaultActor.run\n",
      "21505\t0.38\tray::DefaultActor.run\n",
      "21506\t0.37\tray::DefaultActor.run\n",
      "21512\t0.37\tray::DefaultActor.run\n",
      "21516\t0.37\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21025\t0.08\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:04:29,538 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.21\tray::IDLE\n",
      "21512\t0.21\tray::IDLE\n",
      "21507\t0.21\tray::IDLE\n",
      "21505\t0.21\tray::IDLE\n",
      "21504\t0.21\tray::IDLE\n",
      "21515\t0.21\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:04:29,539 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.78\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.28\tray::DefaultActor.run\n",
      "21515\t0.26\tray::DefaultActor.run\n",
      "21508\t0.26\tray::DefaultActor.run\n",
      "21512\t0.26\tray::DefaultActor.run\n",
      "21507\t0.24\tray::DefaultActor.run\n",
      "21504\t0.24\tray::DefaultActor.run\n",
      "21505\t0.24\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:04:29,539 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.16\tray::IDLE\n",
      "21516\t0.16\tray::IDLE\n",
      "21511\t0.16\tray::IDLE\n",
      "21507\t0.16\tray::IDLE\n",
      "21504\t0.16\tray::IDLE\n",
      "21505\t0.16\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:29,540 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.52\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.26\tray::DefaultActor\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.22\tray::DefaultActor\n",
      "21505\t0.22\tray::DefaultActor\n",
      "21499\t0.22\tray::DefaultActor\n",
      "21508\t0.22\tray::DefaultActor\n",
      "21504\t0.22\tray::DefaultActor\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:04:29,540 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.19\tray::IDLE\n",
      "21516\t0.19\tray::IDLE\n",
      "21507\t0.19\tray::IDLE\n",
      "21505\t0.19\tray::IDLE\n",
      "21504\t0.19\tray::IDLE\n",
      "21499\t0.19\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:04:29,541 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.18\tray::IDLE\n",
      "21516\t0.18\tray::IDLE\n",
      "21505\t0.18\tray::IDLE\n",
      "21507\t0.18\tray::IDLE\n",
      "21499\t0.18\tray::IDLE\n",
      "21504\t0.18\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:04:29,541 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.99\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21508\t0.38\tray::DefaultActor.run\n",
      "21505\t0.38\tray::DefaultActor.run\n",
      "21506\t0.37\tray::DefaultActor.run\n",
      "21512\t0.37\tray::DefaultActor.run\n",
      "21516\t0.37\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21025\t0.08\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:29,541 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.17\tray::IDLE\n",
      "21512\t0.17\tray::IDLE\n",
      "21507\t0.17\tray::IDLE\n",
      "21505\t0.17\tray::IDLE\n",
      "21511\t0.17\tray::IDLE\n",
      "21504\t0.17\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:04:29,543 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.75\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21508\t0.31\tray::DefaultActor.run\n",
      "21505\t0.31\tray::DefaultActor.run\n",
      "21512\t0.31\tray::DefaultActor.run\n",
      "21506\t0.30\tray::DefaultActor.run\n",
      "21516\t0.30\tray::DefaultActor.run\n",
      "21504\t0.30\tray::DefaultActor.run\n",
      "21503\t0.29\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:04:29,546 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.78\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.29\tray::DefaultActor.run\n",
      "21508\t0.28\tray::DefaultActor.run\n",
      "21507\t0.27\tray::DefaultActor.run\n",
      "21506\t0.27\tray::DefaultActor.run\n",
      "21512\t0.27\tray::DefaultActor.run\n",
      "21505\t0.27\tray::DefaultActor.run\n",
      "21504\t0.26\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:04:29,548 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.89\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21506\t0.36\tray::DefaultActor.run\n",
      "21512\t0.35\tray::DefaultActor.run\n",
      "21508\t0.35\tray::DefaultActor\n",
      "21505\t0.35\tray::DefaultActor\n",
      "21516\t0.34\tray::DefaultActor\n",
      "21503\t0.34\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:04:29,548 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.72\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.27\tray::DefaultActor.run\n",
      "21512\t0.24\tray::DefaultActor.run\n",
      "21515\t0.24\tray::DefaultActor.run\n",
      "21508\t0.23\tray::DefaultActor.run\n",
      "21509\t0.23\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21507\t0.22\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:04:29,548 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.21\tray::IDLE\n",
      "21512\t0.21\tray::IDLE\n",
      "21507\t0.21\tray::IDLE\n",
      "21505\t0.21\tray::IDLE\n",
      "21504\t0.21\tray::IDLE\n",
      "21515\t0.21\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:29,549 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.78\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.28\tray::DefaultActor.run\n",
      "21515\t0.26\tray::DefaultActor.run\n",
      "21508\t0.26\tray::DefaultActor.run\n",
      "21512\t0.26\tray::DefaultActor.run\n",
      "21507\t0.24\tray::DefaultActor.run\n",
      "21504\t0.24\tray::DefaultActor.run\n",
      "21505\t0.24\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:29,554 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.52\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.26\tray::DefaultActor\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.22\tray::DefaultActor\n",
      "21505\t0.22\tray::DefaultActor\n",
      "21499\t0.22\tray::DefaultActor\n",
      "21508\t0.22\tray::DefaultActor\n",
      "21504\t0.22\tray::DefaultActor\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:04:29,556 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.19\tray::IDLE\n",
      "21516\t0.19\tray::IDLE\n",
      "21507\t0.19\tray::IDLE\n",
      "21505\t0.19\tray::IDLE\n",
      "21504\t0.19\tray::IDLE\n",
      "21499\t0.19\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:29,559 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.18\tray::IDLE\n",
      "21516\t0.18\tray::IDLE\n",
      "21505\t0.18\tray::IDLE\n",
      "21507\t0.18\tray::IDLE\n",
      "21499\t0.18\tray::IDLE\n",
      "21504\t0.18\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:29,564 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.17\tray::IDLE\n",
      "21512\t0.17\tray::IDLE\n",
      "21507\t0.17\tray::IDLE\n",
      "21505\t0.17\tray::IDLE\n",
      "21511\t0.17\tray::IDLE\n",
      "21504\t0.17\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:29,565 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.75\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21508\t0.31\tray::DefaultActor.run\n",
      "21505\t0.31\tray::DefaultActor.run\n",
      "21512\t0.31\tray::DefaultActor.run\n",
      "21506\t0.30\tray::DefaultActor.run\n",
      "21516\t0.30\tray::DefaultActor.run\n",
      "21504\t0.30\tray::DefaultActor.run\n",
      "21503\t0.29\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:29,566 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.78\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.29\tray::DefaultActor.run\n",
      "21508\t0.28\tray::DefaultActor.run\n",
      "21507\t0.27\tray::DefaultActor.run\n",
      "21506\t0.27\tray::DefaultActor.run\n",
      "21512\t0.27\tray::DefaultActor.run\n",
      "21505\t0.27\tray::DefaultActor.run\n",
      "21504\t0.26\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:04:29,567 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.89\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21506\t0.36\tray::DefaultActor.run\n",
      "21512\t0.35\tray::DefaultActor.run\n",
      "21508\t0.35\tray::DefaultActor\n",
      "21505\t0.35\tray::DefaultActor\n",
      "21516\t0.34\tray::DefaultActor\n",
      "21503\t0.34\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:29,568 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.72\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.27\tray::DefaultActor.run\n",
      "21512\t0.24\tray::DefaultActor.run\n",
      "21515\t0.24\tray::DefaultActor.run\n",
      "21508\t0.23\tray::DefaultActor.run\n",
      "21509\t0.23\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21507\t0.22\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:29,904 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 0ec556f93983fd754e49558001000000, name=DefaultActor.__init__, pid=21516, memory used=0.43GB) was running was 7.07GB / 7.44GB (0.950049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01e894dafc14c58acc1d58301dbd318d734ed5b0ee61f337692679a3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-01e894dafc14c58acc1d58301dbd318d734ed5b0ee61f337692679a3*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.98\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21506\t0.45\tray::DefaultActor.run\n",
      "21505\t0.44\tray::DefaultActor.run\n",
      "21512\t0.44\tray::DefaultActor.run\n",
      "21516\t0.43\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21025\t0.08\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...\n",
      "21145\t0.06\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -u /home/guilherme/cpu-tens...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:04:29,906 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 0ec556f93983fd754e49558001000000, name=DefaultActor.__init__, pid=21516, memory used=0.43GB) was running was 7.07GB / 7.44GB (0.950049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01e894dafc14c58acc1d58301dbd318d734ed5b0ee61f337692679a3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-01e894dafc14c58acc1d58301dbd318d734ed5b0ee61f337692679a3*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.98\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21506\t0.45\tray::DefaultActor.run\n",
      "21505\t0.44\tray::DefaultActor.run\n",
      "21512\t0.44\tray::DefaultActor.run\n",
      "21516\t0.43\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21025\t0.08\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...\n",
      "21145\t0.06\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -u /home/guilherme/cpu-tens...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2024-03-05 15:04:36,632 | server.py:236 | fit_round 8 received 3 results and 16 failures\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(DefaultActor pid=21505)\u001b[0m 2\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21516)\u001b[0m starting client: 3 <class 'str'>\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21516)\u001b[0m starting client:  <class 'int'>\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21516)\u001b[0m Creating client model to client: 3\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21516)\u001b[0m Data X: 1331\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21516)\u001b[0m Data Y: 1331\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21516)\u001b[0m Creating client model to client: 3 round 8\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "Saving round 8 aggregated_ndarrays...\n",
      "result_unbalanced_epoch_1_rounds_10_cycle_0/checkpoints/round-8-weights.npz\n",
      "\n",
      "Evaluating global model round 8\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO flwr 2024-03-05 15:04:42,262 | server.py:125 | fit progress: (8, 0.7206368446350098, {'accuracy': 0.5837092995643616}, 116.01149144699957)\n",
      "DEBUG flwr 2024-03-05 15:04:42,264 | server.py:173 | evaluate_round 8: strategy sampled 19 clients (out of 19)\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21506)\u001b[0m /home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py:72: DeprecationWarning:  Ensure your client is of type `flwr.client.Client`. Please convert it using the `.to_client()` method before returning it in the `client_fn` you pass to `start_simulation`. We have applied this conversion on your behalf. Not returning a `Client` might trigger an error in future versions of Flower.\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21506)\u001b[0m   client = check_clientfn_returns_client(client_fn(cid))\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "ERROR flwr 2024-03-05 15:04:42,467 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 0ec556f93983fd754e49558001000000, name=DefaultActor.__init__, pid=21516, memory used=0.43GB) was running was 7.07GB / 7.44GB (0.950049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01e894dafc14c58acc1d58301dbd318d734ed5b0ee61f337692679a3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-01e894dafc14c58acc1d58301dbd318d734ed5b0ee61f337692679a3*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.98\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21506\t0.45\tray::DefaultActor.run\n",
      "21505\t0.44\tray::DefaultActor.run\n",
      "21512\t0.44\tray::DefaultActor.run\n",
      "21516\t0.43\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21025\t0.08\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...\n",
      "21145\t0.06\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -u /home/guilherme/cpu-tens...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:04:42,477 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 0ec556f93983fd754e49558001000000, name=DefaultActor.__init__, pid=21516, memory used=0.43GB) was running was 7.07GB / 7.44GB (0.950049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01e894dafc14c58acc1d58301dbd318d734ed5b0ee61f337692679a3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-01e894dafc14c58acc1d58301dbd318d734ed5b0ee61f337692679a3*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.98\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21506\t0.45\tray::DefaultActor.run\n",
      "21505\t0.44\tray::DefaultActor.run\n",
      "21512\t0.44\tray::DefaultActor.run\n",
      "21516\t0.43\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21025\t0.08\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...\n",
      "21145\t0.06\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -u /home/guilherme/cpu-tens...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:42,494 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.72\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.27\tray::DefaultActor.run\n",
      "21512\t0.24\tray::DefaultActor.run\n",
      "21515\t0.24\tray::DefaultActor.run\n",
      "21508\t0.23\tray::DefaultActor.run\n",
      "21509\t0.23\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21507\t0.22\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:04:42,496 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.72\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.27\tray::DefaultActor.run\n",
      "21512\t0.24\tray::DefaultActor.run\n",
      "21515\t0.24\tray::DefaultActor.run\n",
      "21508\t0.23\tray::DefaultActor.run\n",
      "21509\t0.23\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21507\t0.22\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:42,500 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.18\tray::IDLE\n",
      "21516\t0.18\tray::IDLE\n",
      "21505\t0.18\tray::IDLE\n",
      "21507\t0.18\tray::IDLE\n",
      "21499\t0.18\tray::IDLE\n",
      "21504\t0.18\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:04:42,502 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.78\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.29\tray::DefaultActor.run\n",
      "21508\t0.28\tray::DefaultActor.run\n",
      "21507\t0.27\tray::DefaultActor.run\n",
      "21506\t0.27\tray::DefaultActor.run\n",
      "21512\t0.27\tray::DefaultActor.run\n",
      "21505\t0.27\tray::DefaultActor.run\n",
      "21504\t0.26\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:04:42,502 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.17\tray::IDLE\n",
      "21512\t0.17\tray::IDLE\n",
      "21507\t0.17\tray::IDLE\n",
      "21505\t0.17\tray::IDLE\n",
      "21511\t0.17\tray::IDLE\n",
      "21504\t0.17\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:04:42,504 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.99\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21508\t0.38\tray::DefaultActor.run\n",
      "21505\t0.38\tray::DefaultActor.run\n",
      "21506\t0.37\tray::DefaultActor.run\n",
      "21512\t0.37\tray::DefaultActor.run\n",
      "21516\t0.37\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21025\t0.08\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:04:42,506 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.21\tray::IDLE\n",
      "21512\t0.21\tray::IDLE\n",
      "21507\t0.21\tray::IDLE\n",
      "21505\t0.21\tray::IDLE\n",
      "21504\t0.21\tray::IDLE\n",
      "21515\t0.21\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:04:42,506 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.18\tray::IDLE\n",
      "21516\t0.18\tray::IDLE\n",
      "21505\t0.18\tray::IDLE\n",
      "21507\t0.18\tray::IDLE\n",
      "21499\t0.18\tray::IDLE\n",
      "21504\t0.18\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:42,507 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.78\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.29\tray::DefaultActor.run\n",
      "21508\t0.28\tray::DefaultActor.run\n",
      "21507\t0.27\tray::DefaultActor.run\n",
      "21506\t0.27\tray::DefaultActor.run\n",
      "21512\t0.27\tray::DefaultActor.run\n",
      "21505\t0.27\tray::DefaultActor.run\n",
      "21504\t0.26\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:42,509 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.17\tray::IDLE\n",
      "21512\t0.17\tray::IDLE\n",
      "21507\t0.17\tray::IDLE\n",
      "21505\t0.17\tray::IDLE\n",
      "21511\t0.17\tray::IDLE\n",
      "21504\t0.17\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:42,511 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.99\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21508\t0.38\tray::DefaultActor.run\n",
      "21505\t0.38\tray::DefaultActor.run\n",
      "21506\t0.37\tray::DefaultActor.run\n",
      "21512\t0.37\tray::DefaultActor.run\n",
      "21516\t0.37\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21025\t0.08\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:42,513 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.75\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21508\t0.31\tray::DefaultActor.run\n",
      "21505\t0.31\tray::DefaultActor.run\n",
      "21512\t0.31\tray::DefaultActor.run\n",
      "21506\t0.30\tray::DefaultActor.run\n",
      "21516\t0.30\tray::DefaultActor.run\n",
      "21504\t0.30\tray::DefaultActor.run\n",
      "21503\t0.29\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:04:42,513 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.21\tray::IDLE\n",
      "21512\t0.21\tray::IDLE\n",
      "21507\t0.21\tray::IDLE\n",
      "21505\t0.21\tray::IDLE\n",
      "21504\t0.21\tray::IDLE\n",
      "21515\t0.21\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:42,516 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.19\tray::IDLE\n",
      "21516\t0.19\tray::IDLE\n",
      "21507\t0.19\tray::IDLE\n",
      "21505\t0.19\tray::IDLE\n",
      "21504\t0.19\tray::IDLE\n",
      "21499\t0.19\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(DefaultActor pid=21506)\u001b[0m 2\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21506)\u001b[0m starting client: 16 <class 'str'>\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21506)\u001b[0m starting client:  <class 'int'>\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21506)\u001b[0m Creating client model to client: 16\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21506)\u001b[0m Data X: 1331\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21506)\u001b[0m Data Y: 1331\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21506)\u001b[0m Creating client model to client: 16 round 9\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21505)\u001b[0m starting client: 0 <class 'str'>\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21505)\u001b[0m starting client:  <class 'int'>\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21505)\u001b[0m Creating client model to client: 0\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21505)\u001b[0m Data X: 1331\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21505)\u001b[0m Data Y: 1331\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21505)\u001b[0m Creating client model to client: 0 round 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:04:42,516 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.52\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.26\tray::DefaultActor\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.22\tray::DefaultActor\n",
      "21505\t0.22\tray::DefaultActor\n",
      "21499\t0.22\tray::DefaultActor\n",
      "21508\t0.22\tray::DefaultActor\n",
      "21504\t0.22\tray::DefaultActor\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:04:42,517 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.16\tray::IDLE\n",
      "21516\t0.16\tray::IDLE\n",
      "21511\t0.16\tray::IDLE\n",
      "21507\t0.16\tray::IDLE\n",
      "21504\t0.16\tray::IDLE\n",
      "21505\t0.16\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:04:42,518 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.78\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.28\tray::DefaultActor.run\n",
      "21515\t0.26\tray::DefaultActor.run\n",
      "21508\t0.26\tray::DefaultActor.run\n",
      "21512\t0.26\tray::DefaultActor.run\n",
      "21507\t0.24\tray::DefaultActor.run\n",
      "21504\t0.24\tray::DefaultActor.run\n",
      "21505\t0.24\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:04:42,518 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 0ec556f93983fd754e49558001000000, name=DefaultActor.__init__, pid=21516, memory used=0.43GB) was running was 7.07GB / 7.44GB (0.950049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01e894dafc14c58acc1d58301dbd318d734ed5b0ee61f337692679a3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-01e894dafc14c58acc1d58301dbd318d734ed5b0ee61f337692679a3*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.98\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21506\t0.45\tray::DefaultActor.run\n",
      "21505\t0.44\tray::DefaultActor.run\n",
      "21512\t0.44\tray::DefaultActor.run\n",
      "21516\t0.43\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21025\t0.08\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...\n",
      "21145\t0.06\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -u /home/guilherme/cpu-tens...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:04:42,521 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.89\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21506\t0.36\tray::DefaultActor.run\n",
      "21512\t0.35\tray::DefaultActor.run\n",
      "21508\t0.35\tray::DefaultActor\n",
      "21505\t0.35\tray::DefaultActor\n",
      "21516\t0.34\tray::DefaultActor\n",
      "21503\t0.34\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:04:42,522 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.72\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.27\tray::DefaultActor.run\n",
      "21512\t0.24\tray::DefaultActor.run\n",
      "21515\t0.24\tray::DefaultActor.run\n",
      "21508\t0.23\tray::DefaultActor.run\n",
      "21509\t0.23\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21507\t0.22\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:04:42,523 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.75\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21508\t0.31\tray::DefaultActor.run\n",
      "21505\t0.31\tray::DefaultActor.run\n",
      "21512\t0.31\tray::DefaultActor.run\n",
      "21506\t0.30\tray::DefaultActor.run\n",
      "21516\t0.30\tray::DefaultActor.run\n",
      "21504\t0.30\tray::DefaultActor.run\n",
      "21503\t0.29\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:42,527 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.19\tray::IDLE\n",
      "21516\t0.19\tray::IDLE\n",
      "21507\t0.19\tray::IDLE\n",
      "21505\t0.19\tray::IDLE\n",
      "21504\t0.19\tray::IDLE\n",
      "21499\t0.19\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:42,528 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.52\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.26\tray::DefaultActor\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.22\tray::DefaultActor\n",
      "21505\t0.22\tray::DefaultActor\n",
      "21499\t0.22\tray::DefaultActor\n",
      "21508\t0.22\tray::DefaultActor\n",
      "21504\t0.22\tray::DefaultActor\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:42,529 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.16\tray::IDLE\n",
      "21516\t0.16\tray::IDLE\n",
      "21511\t0.16\tray::IDLE\n",
      "21507\t0.16\tray::IDLE\n",
      "21504\t0.16\tray::IDLE\n",
      "21505\t0.16\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:42,530 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.78\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.28\tray::DefaultActor.run\n",
      "21515\t0.26\tray::DefaultActor.run\n",
      "21508\t0.26\tray::DefaultActor.run\n",
      "21512\t0.26\tray::DefaultActor.run\n",
      "21507\t0.24\tray::DefaultActor.run\n",
      "21504\t0.24\tray::DefaultActor.run\n",
      "21505\t0.24\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:04:42,532 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 0ec556f93983fd754e49558001000000, name=DefaultActor.__init__, pid=21516, memory used=0.43GB) was running was 7.07GB / 7.44GB (0.950049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01e894dafc14c58acc1d58301dbd318d734ed5b0ee61f337692679a3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-01e894dafc14c58acc1d58301dbd318d734ed5b0ee61f337692679a3*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.98\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21506\t0.45\tray::DefaultActor.run\n",
      "21505\t0.44\tray::DefaultActor.run\n",
      "21512\t0.44\tray::DefaultActor.run\n",
      "21516\t0.43\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21025\t0.08\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...\n",
      "21145\t0.06\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -u /home/guilherme/cpu-tens...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:42,532 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.89\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21506\t0.36\tray::DefaultActor.run\n",
      "21512\t0.35\tray::DefaultActor.run\n",
      "21508\t0.35\tray::DefaultActor\n",
      "21505\t0.35\tray::DefaultActor\n",
      "21516\t0.34\tray::DefaultActor\n",
      "21503\t0.34\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:42,533 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.72\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.27\tray::DefaultActor.run\n",
      "21512\t0.24\tray::DefaultActor.run\n",
      "21515\t0.24\tray::DefaultActor.run\n",
      "21508\t0.23\tray::DefaultActor.run\n",
      "21509\t0.23\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21507\t0.22\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "DEBUG flwr 2024-03-05 15:04:42,543 | server.py:187 | evaluate_round 8 received 0 results and 19 failures\n",
      "DEBUG flwr 2024-03-05 15:04:42,545 | server.py:222 | fit_round 9: strategy sampled 19 clients (out of 19)\n",
      "ERROR flwr 2024-03-05 15:04:42,784 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.72\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.27\tray::DefaultActor.run\n",
      "21512\t0.24\tray::DefaultActor.run\n",
      "21515\t0.24\tray::DefaultActor.run\n",
      "21508\t0.23\tray::DefaultActor.run\n",
      "21509\t0.23\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21507\t0.22\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:04:42,799 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.72\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.27\tray::DefaultActor.run\n",
      "21512\t0.24\tray::DefaultActor.run\n",
      "21515\t0.24\tray::DefaultActor.run\n",
      "21508\t0.23\tray::DefaultActor.run\n",
      "21509\t0.23\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21507\t0.22\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:42,803 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.52\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.26\tray::DefaultActor\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.22\tray::DefaultActor\n",
      "21505\t0.22\tray::DefaultActor\n",
      "21499\t0.22\tray::DefaultActor\n",
      "21508\t0.22\tray::DefaultActor\n",
      "21504\t0.22\tray::DefaultActor\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:04:42,817 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.52\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.26\tray::DefaultActor\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.22\tray::DefaultActor\n",
      "21505\t0.22\tray::DefaultActor\n",
      "21499\t0.22\tray::DefaultActor\n",
      "21508\t0.22\tray::DefaultActor\n",
      "21504\t0.22\tray::DefaultActor\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:42,822 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.78\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.28\tray::DefaultActor.run\n",
      "21515\t0.26\tray::DefaultActor.run\n",
      "21508\t0.26\tray::DefaultActor.run\n",
      "21512\t0.26\tray::DefaultActor.run\n",
      "21507\t0.24\tray::DefaultActor.run\n",
      "21504\t0.24\tray::DefaultActor.run\n",
      "21505\t0.24\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:04:42,824 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.78\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.28\tray::DefaultActor.run\n",
      "21515\t0.26\tray::DefaultActor.run\n",
      "21508\t0.26\tray::DefaultActor.run\n",
      "21512\t0.26\tray::DefaultActor.run\n",
      "21507\t0.24\tray::DefaultActor.run\n",
      "21504\t0.24\tray::DefaultActor.run\n",
      "21505\t0.24\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:42,829 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.89\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21506\t0.36\tray::DefaultActor.run\n",
      "21512\t0.35\tray::DefaultActor.run\n",
      "21508\t0.35\tray::DefaultActor\n",
      "21505\t0.35\tray::DefaultActor\n",
      "21516\t0.34\tray::DefaultActor\n",
      "21503\t0.34\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:04:42,829 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 0ec556f93983fd754e49558001000000, name=DefaultActor.__init__, pid=21516, memory used=0.43GB) was running was 7.07GB / 7.44GB (0.950049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01e894dafc14c58acc1d58301dbd318d734ed5b0ee61f337692679a3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-01e894dafc14c58acc1d58301dbd318d734ed5b0ee61f337692679a3*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.98\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21506\t0.45\tray::DefaultActor.run\n",
      "21505\t0.44\tray::DefaultActor.run\n",
      "21512\t0.44\tray::DefaultActor.run\n",
      "21516\t0.43\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21025\t0.08\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...\n",
      "21145\t0.06\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -u /home/guilherme/cpu-tens...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:04:42,831 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.16\tray::IDLE\n",
      "21516\t0.16\tray::IDLE\n",
      "21511\t0.16\tray::IDLE\n",
      "21507\t0.16\tray::IDLE\n",
      "21504\t0.16\tray::IDLE\n",
      "21505\t0.16\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:04:42,833 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.17\tray::IDLE\n",
      "21512\t0.17\tray::IDLE\n",
      "21507\t0.17\tray::IDLE\n",
      "21505\t0.17\tray::IDLE\n",
      "21511\t0.17\tray::IDLE\n",
      "21504\t0.17\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:04:42,834 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.75\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21508\t0.31\tray::DefaultActor.run\n",
      "21505\t0.31\tray::DefaultActor.run\n",
      "21512\t0.31\tray::DefaultActor.run\n",
      "21506\t0.30\tray::DefaultActor.run\n",
      "21516\t0.30\tray::DefaultActor.run\n",
      "21504\t0.30\tray::DefaultActor.run\n",
      "21503\t0.29\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:04:42,835 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.89\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21506\t0.36\tray::DefaultActor.run\n",
      "21512\t0.35\tray::DefaultActor.run\n",
      "21508\t0.35\tray::DefaultActor\n",
      "21505\t0.35\tray::DefaultActor\n",
      "21516\t0.34\tray::DefaultActor\n",
      "21503\t0.34\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:42,836 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 0ec556f93983fd754e49558001000000, name=DefaultActor.__init__, pid=21516, memory used=0.43GB) was running was 7.07GB / 7.44GB (0.950049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01e894dafc14c58acc1d58301dbd318d734ed5b0ee61f337692679a3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-01e894dafc14c58acc1d58301dbd318d734ed5b0ee61f337692679a3*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.98\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21506\t0.45\tray::DefaultActor.run\n",
      "21505\t0.44\tray::DefaultActor.run\n",
      "21512\t0.44\tray::DefaultActor.run\n",
      "21516\t0.43\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21025\t0.08\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...\n",
      "21145\t0.06\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -u /home/guilherme/cpu-tens...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:42,838 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.16\tray::IDLE\n",
      "21516\t0.16\tray::IDLE\n",
      "21511\t0.16\tray::IDLE\n",
      "21507\t0.16\tray::IDLE\n",
      "21504\t0.16\tray::IDLE\n",
      "21505\t0.16\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:42,839 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.78\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.29\tray::DefaultActor.run\n",
      "21508\t0.28\tray::DefaultActor.run\n",
      "21507\t0.27\tray::DefaultActor.run\n",
      "21506\t0.27\tray::DefaultActor.run\n",
      "21512\t0.27\tray::DefaultActor.run\n",
      "21505\t0.27\tray::DefaultActor.run\n",
      "21504\t0.26\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:04:42,839 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.72\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.27\tray::DefaultActor.run\n",
      "21512\t0.24\tray::DefaultActor.run\n",
      "21515\t0.24\tray::DefaultActor.run\n",
      "21508\t0.23\tray::DefaultActor.run\n",
      "21509\t0.23\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21507\t0.22\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:04:42,843 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.17\tray::IDLE\n",
      "21512\t0.17\tray::IDLE\n",
      "21507\t0.17\tray::IDLE\n",
      "21505\t0.17\tray::IDLE\n",
      "21511\t0.17\tray::IDLE\n",
      "21504\t0.17\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:42,843 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.18\tray::IDLE\n",
      "21516\t0.18\tray::IDLE\n",
      "21505\t0.18\tray::IDLE\n",
      "21507\t0.18\tray::IDLE\n",
      "21499\t0.18\tray::IDLE\n",
      "21504\t0.18\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:04:42,844 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.19\tray::IDLE\n",
      "21516\t0.19\tray::IDLE\n",
      "21507\t0.19\tray::IDLE\n",
      "21505\t0.19\tray::IDLE\n",
      "21504\t0.19\tray::IDLE\n",
      "21499\t0.19\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:04:42,845 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.21\tray::IDLE\n",
      "21512\t0.21\tray::IDLE\n",
      "21507\t0.21\tray::IDLE\n",
      "21505\t0.21\tray::IDLE\n",
      "21504\t0.21\tray::IDLE\n",
      "21515\t0.21\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:04:42,845 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.75\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21508\t0.31\tray::DefaultActor.run\n",
      "21505\t0.31\tray::DefaultActor.run\n",
      "21512\t0.31\tray::DefaultActor.run\n",
      "21506\t0.30\tray::DefaultActor.run\n",
      "21516\t0.30\tray::DefaultActor.run\n",
      "21504\t0.30\tray::DefaultActor.run\n",
      "21503\t0.29\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:04:42,846 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.99\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21508\t0.38\tray::DefaultActor.run\n",
      "21505\t0.38\tray::DefaultActor.run\n",
      "21506\t0.37\tray::DefaultActor.run\n",
      "21512\t0.37\tray::DefaultActor.run\n",
      "21516\t0.37\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21025\t0.08\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:04:42,849 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.89\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21506\t0.36\tray::DefaultActor.run\n",
      "21512\t0.35\tray::DefaultActor.run\n",
      "21508\t0.35\tray::DefaultActor\n",
      "21505\t0.35\tray::DefaultActor\n",
      "21516\t0.34\tray::DefaultActor\n",
      "21503\t0.34\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:04:42,849 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.52\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.26\tray::DefaultActor\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.22\tray::DefaultActor\n",
      "21505\t0.22\tray::DefaultActor\n",
      "21499\t0.22\tray::DefaultActor\n",
      "21508\t0.22\tray::DefaultActor\n",
      "21504\t0.22\tray::DefaultActor\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:04:42,854 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.78\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.29\tray::DefaultActor.run\n",
      "21508\t0.28\tray::DefaultActor.run\n",
      "21507\t0.27\tray::DefaultActor.run\n",
      "21506\t0.27\tray::DefaultActor.run\n",
      "21512\t0.27\tray::DefaultActor.run\n",
      "21505\t0.27\tray::DefaultActor.run\n",
      "21504\t0.26\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:42,856 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.72\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.27\tray::DefaultActor.run\n",
      "21512\t0.24\tray::DefaultActor.run\n",
      "21515\t0.24\tray::DefaultActor.run\n",
      "21508\t0.23\tray::DefaultActor.run\n",
      "21509\t0.23\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21507\t0.22\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:42,858 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.18\tray::IDLE\n",
      "21516\t0.18\tray::IDLE\n",
      "21505\t0.18\tray::IDLE\n",
      "21507\t0.18\tray::IDLE\n",
      "21499\t0.18\tray::IDLE\n",
      "21504\t0.18\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:42,860 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.19\tray::IDLE\n",
      "21516\t0.19\tray::IDLE\n",
      "21507\t0.19\tray::IDLE\n",
      "21505\t0.19\tray::IDLE\n",
      "21504\t0.19\tray::IDLE\n",
      "21499\t0.19\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:42,861 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.21\tray::IDLE\n",
      "21512\t0.21\tray::IDLE\n",
      "21507\t0.21\tray::IDLE\n",
      "21505\t0.21\tray::IDLE\n",
      "21504\t0.21\tray::IDLE\n",
      "21515\t0.21\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:04:42,866 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.99\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21508\t0.38\tray::DefaultActor.run\n",
      "21505\t0.38\tray::DefaultActor.run\n",
      "21506\t0.37\tray::DefaultActor.run\n",
      "21512\t0.37\tray::DefaultActor.run\n",
      "21516\t0.37\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21025\t0.08\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:42,867 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.89\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21506\t0.36\tray::DefaultActor.run\n",
      "21512\t0.35\tray::DefaultActor.run\n",
      "21508\t0.35\tray::DefaultActor\n",
      "21505\t0.35\tray::DefaultActor\n",
      "21516\t0.34\tray::DefaultActor\n",
      "21503\t0.34\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:42,868 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.52\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.26\tray::DefaultActor\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.22\tray::DefaultActor\n",
      "21505\t0.22\tray::DefaultActor\n",
      "21499\t0.22\tray::DefaultActor\n",
      "21508\t0.22\tray::DefaultActor\n",
      "21504\t0.22\tray::DefaultActor\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "DEBUG flwr 2024-03-05 15:04:50,017 | server.py:236 | fit_round 9 received 3 results and 16 failures\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(DefaultActor pid=21506)\u001b[0m 2\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21512)\u001b[0m starting client: 7 <class 'str'>\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21512)\u001b[0m starting client:  <class 'int'>\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21512)\u001b[0m Creating client model to client: 7\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21512)\u001b[0m Data X: 1331\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21512)\u001b[0m Data Y: 1331\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21512)\u001b[0m Creating client model to client: 7 round 9\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Saving round 9 aggregated_ndarrays...\n",
      "result_unbalanced_epoch_1_rounds_10_cycle_0/checkpoints/round-9-weights.npz\n",
      "\n",
      "Evaluating global model round 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO flwr 2024-03-05 15:04:55,524 | server.py:125 | fit progress: (9, 0.7143182158470154, {'accuracy': 0.6002097725868225}, 129.2739016759988)\n",
      "DEBUG flwr 2024-03-05 15:04:55,526 | server.py:173 | evaluate_round 9: strategy sampled 19 clients (out of 19)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(DefaultActor pid=21512)\u001b[0m /home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py:72: DeprecationWarning:  Ensure your client is of type `flwr.client.Client`. Please convert it using the `.to_client()` method before returning it in the `client_fn` you pass to `start_simulation`. We have applied this conversion on your behalf. Not returning a `Client` might trigger an error in future versions of Flower.\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21512)\u001b[0m   client = check_clientfn_returns_client(client_fn(cid))\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "ERROR flwr 2024-03-05 15:04:55,736 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.52\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.26\tray::DefaultActor\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.22\tray::DefaultActor\n",
      "21505\t0.22\tray::DefaultActor\n",
      "21499\t0.22\tray::DefaultActor\n",
      "21508\t0.22\tray::DefaultActor\n",
      "21504\t0.22\tray::DefaultActor\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:04:55,747 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.52\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.26\tray::DefaultActor\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.22\tray::DefaultActor\n",
      "21505\t0.22\tray::DefaultActor\n",
      "21499\t0.22\tray::DefaultActor\n",
      "21508\t0.22\tray::DefaultActor\n",
      "21504\t0.22\tray::DefaultActor\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:55,764 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.72\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.27\tray::DefaultActor.run\n",
      "21512\t0.24\tray::DefaultActor.run\n",
      "21515\t0.24\tray::DefaultActor.run\n",
      "21508\t0.23\tray::DefaultActor.run\n",
      "21509\t0.23\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21507\t0.22\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:04:55,766 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.72\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.27\tray::DefaultActor.run\n",
      "21512\t0.24\tray::DefaultActor.run\n",
      "21515\t0.24\tray::DefaultActor.run\n",
      "21508\t0.23\tray::DefaultActor.run\n",
      "21509\t0.23\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21507\t0.22\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:55,769 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.89\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21506\t0.36\tray::DefaultActor.run\n",
      "21512\t0.35\tray::DefaultActor.run\n",
      "21508\t0.35\tray::DefaultActor\n",
      "21505\t0.35\tray::DefaultActor\n",
      "21516\t0.34\tray::DefaultActor\n",
      "21503\t0.34\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:04:55,771 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.89\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21506\t0.36\tray::DefaultActor.run\n",
      "21512\t0.35\tray::DefaultActor.run\n",
      "21508\t0.35\tray::DefaultActor\n",
      "21505\t0.35\tray::DefaultActor\n",
      "21516\t0.34\tray::DefaultActor\n",
      "21503\t0.34\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:55,772 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.19\tray::IDLE\n",
      "21516\t0.19\tray::IDLE\n",
      "21507\t0.19\tray::IDLE\n",
      "21505\t0.19\tray::IDLE\n",
      "21504\t0.19\tray::IDLE\n",
      "21499\t0.19\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:04:55,775 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.75\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21508\t0.31\tray::DefaultActor.run\n",
      "21505\t0.31\tray::DefaultActor.run\n",
      "21512\t0.31\tray::DefaultActor.run\n",
      "21506\t0.30\tray::DefaultActor.run\n",
      "21516\t0.30\tray::DefaultActor.run\n",
      "21504\t0.30\tray::DefaultActor.run\n",
      "21503\t0.29\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:04:55,776 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.16\tray::IDLE\n",
      "21516\t0.16\tray::IDLE\n",
      "21511\t0.16\tray::IDLE\n",
      "21507\t0.16\tray::IDLE\n",
      "21504\t0.16\tray::IDLE\n",
      "21505\t0.16\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:04:55,776 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 0ec556f93983fd754e49558001000000, name=DefaultActor.__init__, pid=21516, memory used=0.43GB) was running was 7.07GB / 7.44GB (0.950049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01e894dafc14c58acc1d58301dbd318d734ed5b0ee61f337692679a3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-01e894dafc14c58acc1d58301dbd318d734ed5b0ee61f337692679a3*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.98\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21506\t0.45\tray::DefaultActor.run\n",
      "21505\t0.44\tray::DefaultActor.run\n",
      "21512\t0.44\tray::DefaultActor.run\n",
      "21516\t0.43\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21025\t0.08\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...\n",
      "21145\t0.06\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -u /home/guilherme/cpu-tens...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:04:55,777 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.19\tray::IDLE\n",
      "21516\t0.19\tray::IDLE\n",
      "21507\t0.19\tray::IDLE\n",
      "21505\t0.19\tray::IDLE\n",
      "21504\t0.19\tray::IDLE\n",
      "21499\t0.19\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:55,778 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.52\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.26\tray::DefaultActor\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.22\tray::DefaultActor\n",
      "21505\t0.22\tray::DefaultActor\n",
      "21499\t0.22\tray::DefaultActor\n",
      "21508\t0.22\tray::DefaultActor\n",
      "21504\t0.22\tray::DefaultActor\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:04:55,779 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.75\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21508\t0.31\tray::DefaultActor.run\n",
      "21505\t0.31\tray::DefaultActor.run\n",
      "21512\t0.31\tray::DefaultActor.run\n",
      "21506\t0.30\tray::DefaultActor.run\n",
      "21516\t0.30\tray::DefaultActor.run\n",
      "21504\t0.30\tray::DefaultActor.run\n",
      "21503\t0.29\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:55,780 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.21\tray::IDLE\n",
      "21512\t0.21\tray::IDLE\n",
      "21507\t0.21\tray::IDLE\n",
      "21505\t0.21\tray::IDLE\n",
      "21504\t0.21\tray::IDLE\n",
      "21515\t0.21\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:04:55,782 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.16\tray::IDLE\n",
      "21516\t0.16\tray::IDLE\n",
      "21511\t0.16\tray::IDLE\n",
      "21507\t0.16\tray::IDLE\n",
      "21504\t0.16\tray::IDLE\n",
      "21505\t0.16\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:55,784 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.78\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.29\tray::DefaultActor.run\n",
      "21508\t0.28\tray::DefaultActor.run\n",
      "21507\t0.27\tray::DefaultActor.run\n",
      "21506\t0.27\tray::DefaultActor.run\n",
      "21512\t0.27\tray::DefaultActor.run\n",
      "21505\t0.27\tray::DefaultActor.run\n",
      "21504\t0.26\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:04:55,784 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 0ec556f93983fd754e49558001000000, name=DefaultActor.__init__, pid=21516, memory used=0.43GB) was running was 7.07GB / 7.44GB (0.950049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01e894dafc14c58acc1d58301dbd318d734ed5b0ee61f337692679a3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-01e894dafc14c58acc1d58301dbd318d734ed5b0ee61f337692679a3*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.98\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21506\t0.45\tray::DefaultActor.run\n",
      "21505\t0.44\tray::DefaultActor.run\n",
      "21512\t0.44\tray::DefaultActor.run\n",
      "21516\t0.43\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21025\t0.08\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...\n",
      "21145\t0.06\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -u /home/guilherme/cpu-tens...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:55,784 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.99\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21508\t0.38\tray::DefaultActor.run\n",
      "21505\t0.38\tray::DefaultActor.run\n",
      "21506\t0.37\tray::DefaultActor.run\n",
      "21512\t0.37\tray::DefaultActor.run\n",
      "21516\t0.37\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21025\t0.08\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:04:55,784 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.17\tray::IDLE\n",
      "21512\t0.17\tray::IDLE\n",
      "21507\t0.17\tray::IDLE\n",
      "21505\t0.17\tray::IDLE\n",
      "21511\t0.17\tray::IDLE\n",
      "21504\t0.17\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:04:55,786 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.52\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.26\tray::DefaultActor\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.22\tray::DefaultActor\n",
      "21505\t0.22\tray::DefaultActor\n",
      "21499\t0.22\tray::DefaultActor\n",
      "21508\t0.22\tray::DefaultActor\n",
      "21504\t0.22\tray::DefaultActor\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:55,787 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.89\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21506\t0.36\tray::DefaultActor.run\n",
      "21512\t0.35\tray::DefaultActor.run\n",
      "21508\t0.35\tray::DefaultActor\n",
      "21505\t0.35\tray::DefaultActor\n",
      "21516\t0.34\tray::DefaultActor\n",
      "21503\t0.34\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:04:55,788 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.18\tray::IDLE\n",
      "21516\t0.18\tray::IDLE\n",
      "21505\t0.18\tray::IDLE\n",
      "21507\t0.18\tray::IDLE\n",
      "21499\t0.18\tray::IDLE\n",
      "21504\t0.18\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(DefaultActor pid=21512)\u001b[0m 2\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21506)\u001b[0m starting client: 17 <class 'str'>\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21506)\u001b[0m starting client:  <class 'int'>\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21506)\u001b[0m Creating client model to client: 17\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21506)\u001b[0m Data X: 1331\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21506)\u001b[0m Data Y: 1331\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21506)\u001b[0m Creating client model to client: 17 round 10\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21505)\u001b[0m starting client: 9 <class 'str'>\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21505)\u001b[0m starting client:  <class 'int'>\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21505)\u001b[0m Creating client model to client: 9\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21505)\u001b[0m Data X: 1331\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21505)\u001b[0m Data Y: 1331\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21505)\u001b[0m Creating client model to client: 9 round 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:04:55,790 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.78\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.28\tray::DefaultActor.run\n",
      "21515\t0.26\tray::DefaultActor.run\n",
      "21508\t0.26\tray::DefaultActor.run\n",
      "21512\t0.26\tray::DefaultActor.run\n",
      "21507\t0.24\tray::DefaultActor.run\n",
      "21504\t0.24\tray::DefaultActor.run\n",
      "21505\t0.24\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:04:55,792 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.21\tray::IDLE\n",
      "21512\t0.21\tray::IDLE\n",
      "21507\t0.21\tray::IDLE\n",
      "21505\t0.21\tray::IDLE\n",
      "21504\t0.21\tray::IDLE\n",
      "21515\t0.21\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:55,794 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.78\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.29\tray::DefaultActor.run\n",
      "21508\t0.28\tray::DefaultActor.run\n",
      "21507\t0.27\tray::DefaultActor.run\n",
      "21506\t0.27\tray::DefaultActor.run\n",
      "21512\t0.27\tray::DefaultActor.run\n",
      "21505\t0.27\tray::DefaultActor.run\n",
      "21504\t0.26\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:55,799 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.99\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21508\t0.38\tray::DefaultActor.run\n",
      "21505\t0.38\tray::DefaultActor.run\n",
      "21506\t0.37\tray::DefaultActor.run\n",
      "21512\t0.37\tray::DefaultActor.run\n",
      "21516\t0.37\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21025\t0.08\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:04:55,801 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.17\tray::IDLE\n",
      "21512\t0.17\tray::IDLE\n",
      "21507\t0.17\tray::IDLE\n",
      "21505\t0.17\tray::IDLE\n",
      "21511\t0.17\tray::IDLE\n",
      "21504\t0.17\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:55,803 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.89\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21506\t0.36\tray::DefaultActor.run\n",
      "21512\t0.35\tray::DefaultActor.run\n",
      "21508\t0.35\tray::DefaultActor\n",
      "21505\t0.35\tray::DefaultActor\n",
      "21516\t0.34\tray::DefaultActor\n",
      "21503\t0.34\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:55,804 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.18\tray::IDLE\n",
      "21516\t0.18\tray::IDLE\n",
      "21505\t0.18\tray::IDLE\n",
      "21507\t0.18\tray::IDLE\n",
      "21499\t0.18\tray::IDLE\n",
      "21504\t0.18\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:55,806 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.78\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.28\tray::DefaultActor.run\n",
      "21515\t0.26\tray::DefaultActor.run\n",
      "21508\t0.26\tray::DefaultActor.run\n",
      "21512\t0.26\tray::DefaultActor.run\n",
      "21507\t0.24\tray::DefaultActor.run\n",
      "21504\t0.24\tray::DefaultActor.run\n",
      "21505\t0.24\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "DEBUG flwr 2024-03-05 15:04:55,814 | server.py:187 | evaluate_round 9 received 0 results and 19 failures\n",
      "DEBUG flwr 2024-03-05 15:04:55,815 | server.py:222 | fit_round 10: strategy sampled 19 clients (out of 19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:04:56,010 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.78\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.28\tray::DefaultActor.run\n",
      "21515\t0.26\tray::DefaultActor.run\n",
      "21508\t0.26\tray::DefaultActor.run\n",
      "21512\t0.26\tray::DefaultActor.run\n",
      "21507\t0.24\tray::DefaultActor.run\n",
      "21504\t0.24\tray::DefaultActor.run\n",
      "21505\t0.24\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:04:56,022 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.78\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.28\tray::DefaultActor.run\n",
      "21515\t0.26\tray::DefaultActor.run\n",
      "21508\t0.26\tray::DefaultActor.run\n",
      "21512\t0.26\tray::DefaultActor.run\n",
      "21507\t0.24\tray::DefaultActor.run\n",
      "21504\t0.24\tray::DefaultActor.run\n",
      "21505\t0.24\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:56,026 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.18\tray::IDLE\n",
      "21516\t0.18\tray::IDLE\n",
      "21505\t0.18\tray::IDLE\n",
      "21507\t0.18\tray::IDLE\n",
      "21499\t0.18\tray::IDLE\n",
      "21504\t0.18\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:04:56,040 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.18\tray::IDLE\n",
      "21516\t0.18\tray::IDLE\n",
      "21505\t0.18\tray::IDLE\n",
      "21507\t0.18\tray::IDLE\n",
      "21499\t0.18\tray::IDLE\n",
      "21504\t0.18\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:56,047 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.72\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.27\tray::DefaultActor.run\n",
      "21512\t0.24\tray::DefaultActor.run\n",
      "21515\t0.24\tray::DefaultActor.run\n",
      "21508\t0.23\tray::DefaultActor.run\n",
      "21509\t0.23\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21507\t0.22\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:04:56,050 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.17\tray::IDLE\n",
      "21512\t0.17\tray::IDLE\n",
      "21507\t0.17\tray::IDLE\n",
      "21505\t0.17\tray::IDLE\n",
      "21511\t0.17\tray::IDLE\n",
      "21504\t0.17\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:04:56,051 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.72\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.27\tray::DefaultActor.run\n",
      "21512\t0.24\tray::DefaultActor.run\n",
      "21515\t0.24\tray::DefaultActor.run\n",
      "21508\t0.23\tray::DefaultActor.run\n",
      "21509\t0.23\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21507\t0.22\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:56,052 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.78\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.29\tray::DefaultActor.run\n",
      "21508\t0.28\tray::DefaultActor.run\n",
      "21507\t0.27\tray::DefaultActor.run\n",
      "21506\t0.27\tray::DefaultActor.run\n",
      "21512\t0.27\tray::DefaultActor.run\n",
      "21505\t0.27\tray::DefaultActor.run\n",
      "21504\t0.26\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:04:56,053 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.21\tray::IDLE\n",
      "21512\t0.21\tray::IDLE\n",
      "21507\t0.21\tray::IDLE\n",
      "21505\t0.21\tray::IDLE\n",
      "21504\t0.21\tray::IDLE\n",
      "21515\t0.21\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:04:56,055 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.89\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21506\t0.36\tray::DefaultActor.run\n",
      "21512\t0.35\tray::DefaultActor.run\n",
      "21508\t0.35\tray::DefaultActor\n",
      "21505\t0.35\tray::DefaultActor\n",
      "21516\t0.34\tray::DefaultActor\n",
      "21503\t0.34\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:04:56,055 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.16\tray::IDLE\n",
      "21516\t0.16\tray::IDLE\n",
      "21511\t0.16\tray::IDLE\n",
      "21507\t0.16\tray::IDLE\n",
      "21504\t0.16\tray::IDLE\n",
      "21505\t0.16\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:04:56,056 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.17\tray::IDLE\n",
      "21512\t0.17\tray::IDLE\n",
      "21507\t0.17\tray::IDLE\n",
      "21505\t0.17\tray::IDLE\n",
      "21511\t0.17\tray::IDLE\n",
      "21504\t0.17\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:04:56,057 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.52\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.26\tray::DefaultActor\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.22\tray::DefaultActor\n",
      "21505\t0.22\tray::DefaultActor\n",
      "21499\t0.22\tray::DefaultActor\n",
      "21508\t0.22\tray::DefaultActor\n",
      "21504\t0.22\tray::DefaultActor\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:04:56,059 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.19\tray::IDLE\n",
      "21516\t0.19\tray::IDLE\n",
      "21507\t0.19\tray::IDLE\n",
      "21505\t0.19\tray::IDLE\n",
      "21504\t0.19\tray::IDLE\n",
      "21499\t0.19\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:04:56,059 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 0ec556f93983fd754e49558001000000, name=DefaultActor.__init__, pid=21516, memory used=0.43GB) was running was 7.07GB / 7.44GB (0.950049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01e894dafc14c58acc1d58301dbd318d734ed5b0ee61f337692679a3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-01e894dafc14c58acc1d58301dbd318d734ed5b0ee61f337692679a3*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.98\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21506\t0.45\tray::DefaultActor.run\n",
      "21505\t0.44\tray::DefaultActor.run\n",
      "21512\t0.44\tray::DefaultActor.run\n",
      "21516\t0.43\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21025\t0.08\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...\n",
      "21145\t0.06\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -u /home/guilherme/cpu-tens...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:04:56,061 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.78\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.29\tray::DefaultActor.run\n",
      "21508\t0.28\tray::DefaultActor.run\n",
      "21507\t0.27\tray::DefaultActor.run\n",
      "21506\t0.27\tray::DefaultActor.run\n",
      "21512\t0.27\tray::DefaultActor.run\n",
      "21505\t0.27\tray::DefaultActor.run\n",
      "21504\t0.26\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:56,063 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.78\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.28\tray::DefaultActor.run\n",
      "21515\t0.26\tray::DefaultActor.run\n",
      "21508\t0.26\tray::DefaultActor.run\n",
      "21512\t0.26\tray::DefaultActor.run\n",
      "21507\t0.24\tray::DefaultActor.run\n",
      "21504\t0.24\tray::DefaultActor.run\n",
      "21505\t0.24\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:04:56,068 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.75\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21508\t0.31\tray::DefaultActor.run\n",
      "21505\t0.31\tray::DefaultActor.run\n",
      "21512\t0.31\tray::DefaultActor.run\n",
      "21506\t0.30\tray::DefaultActor.run\n",
      "21516\t0.30\tray::DefaultActor.run\n",
      "21504\t0.30\tray::DefaultActor.run\n",
      "21503\t0.29\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:04:56,068 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.21\tray::IDLE\n",
      "21512\t0.21\tray::IDLE\n",
      "21507\t0.21\tray::IDLE\n",
      "21505\t0.21\tray::IDLE\n",
      "21504\t0.21\tray::IDLE\n",
      "21515\t0.21\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:56,069 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.89\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21506\t0.36\tray::DefaultActor.run\n",
      "21512\t0.35\tray::DefaultActor.run\n",
      "21508\t0.35\tray::DefaultActor\n",
      "21505\t0.35\tray::DefaultActor\n",
      "21516\t0.34\tray::DefaultActor\n",
      "21503\t0.34\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:56,070 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.16\tray::IDLE\n",
      "21516\t0.16\tray::IDLE\n",
      "21511\t0.16\tray::IDLE\n",
      "21507\t0.16\tray::IDLE\n",
      "21504\t0.16\tray::IDLE\n",
      "21505\t0.16\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:56,071 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.99\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21508\t0.38\tray::DefaultActor.run\n",
      "21505\t0.38\tray::DefaultActor.run\n",
      "21506\t0.37\tray::DefaultActor.run\n",
      "21512\t0.37\tray::DefaultActor.run\n",
      "21516\t0.37\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21025\t0.08\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:04:56,073 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.52\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.26\tray::DefaultActor\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.22\tray::DefaultActor\n",
      "21505\t0.22\tray::DefaultActor\n",
      "21499\t0.22\tray::DefaultActor\n",
      "21508\t0.22\tray::DefaultActor\n",
      "21504\t0.22\tray::DefaultActor\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:56,073 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.89\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21506\t0.36\tray::DefaultActor.run\n",
      "21512\t0.35\tray::DefaultActor.run\n",
      "21508\t0.35\tray::DefaultActor\n",
      "21505\t0.35\tray::DefaultActor\n",
      "21516\t0.34\tray::DefaultActor\n",
      "21503\t0.34\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:04:56,074 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.19\tray::IDLE\n",
      "21516\t0.19\tray::IDLE\n",
      "21507\t0.19\tray::IDLE\n",
      "21505\t0.19\tray::IDLE\n",
      "21504\t0.19\tray::IDLE\n",
      "21499\t0.19\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:56,074 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.18\tray::IDLE\n",
      "21516\t0.18\tray::IDLE\n",
      "21505\t0.18\tray::IDLE\n",
      "21507\t0.18\tray::IDLE\n",
      "21499\t0.18\tray::IDLE\n",
      "21504\t0.18\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:04:56,076 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 0ec556f93983fd754e49558001000000, name=DefaultActor.__init__, pid=21516, memory used=0.43GB) was running was 7.07GB / 7.44GB (0.950049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01e894dafc14c58acc1d58301dbd318d734ed5b0ee61f337692679a3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-01e894dafc14c58acc1d58301dbd318d734ed5b0ee61f337692679a3*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.98\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21506\t0.45\tray::DefaultActor.run\n",
      "21505\t0.44\tray::DefaultActor.run\n",
      "21512\t0.44\tray::DefaultActor.run\n",
      "21516\t0.43\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21025\t0.08\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...\n",
      "21145\t0.06\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -u /home/guilherme/cpu-tens...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:56,082 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.78\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.28\tray::DefaultActor.run\n",
      "21515\t0.26\tray::DefaultActor.run\n",
      "21508\t0.26\tray::DefaultActor.run\n",
      "21512\t0.26\tray::DefaultActor.run\n",
      "21507\t0.24\tray::DefaultActor.run\n",
      "21504\t0.24\tray::DefaultActor.run\n",
      "21505\t0.24\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:56,084 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.75\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21508\t0.31\tray::DefaultActor.run\n",
      "21505\t0.31\tray::DefaultActor.run\n",
      "21512\t0.31\tray::DefaultActor.run\n",
      "21506\t0.30\tray::DefaultActor.run\n",
      "21516\t0.30\tray::DefaultActor.run\n",
      "21504\t0.30\tray::DefaultActor.run\n",
      "21503\t0.29\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:56,089 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.99\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21508\t0.38\tray::DefaultActor.run\n",
      "21505\t0.38\tray::DefaultActor.run\n",
      "21506\t0.37\tray::DefaultActor.run\n",
      "21512\t0.37\tray::DefaultActor.run\n",
      "21516\t0.37\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21025\t0.08\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:04:56,091 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.89\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21506\t0.36\tray::DefaultActor.run\n",
      "21512\t0.35\tray::DefaultActor.run\n",
      "21508\t0.35\tray::DefaultActor\n",
      "21505\t0.35\tray::DefaultActor\n",
      "21516\t0.34\tray::DefaultActor\n",
      "21503\t0.34\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:04:56,093 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.18\tray::IDLE\n",
      "21516\t0.18\tray::IDLE\n",
      "21505\t0.18\tray::IDLE\n",
      "21507\t0.18\tray::IDLE\n",
      "21499\t0.18\tray::IDLE\n",
      "21504\t0.18\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "DEBUG flwr 2024-03-05 15:05:03,066 | server.py:236 | fit_round 10 received 3 results and 16 failures\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(DefaultActor pid=21506)\u001b[0m 2\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21505)\u001b[0m starting client: 7 <class 'str'>\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21505)\u001b[0m starting client:  <class 'int'>\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21505)\u001b[0m Creating client model to client: 7\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21505)\u001b[0m Data X: 1331\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21505)\u001b[0m Data Y: 1331\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21505)\u001b[0m Creating client model to client: 7 round 10\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Saving round 10 aggregated_ndarrays...\n",
      "result_unbalanced_epoch_1_rounds_10_cycle_0/checkpoints/round-10-weights.npz\n",
      "\n",
      "Evaluating global model round 10\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO flwr 2024-03-05 15:05:08,591 | server.py:125 | fit progress: (10, 0.7127873301506042, {'accuracy': 0.6153992414474487}, 142.3405186529999)\n",
      "DEBUG flwr 2024-03-05 15:05:08,593 | server.py:173 | evaluate_round 10: strategy sampled 19 clients (out of 19)\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21506)\u001b[0m /home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py:72: DeprecationWarning:  Ensure your client is of type `flwr.client.Client`. Please convert it using the `.to_client()` method before returning it in the `client_fn` you pass to `start_simulation`. We have applied this conversion on your behalf. Not returning a `Client` might trigger an error in future versions of Flower.\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21506)\u001b[0m   client = check_clientfn_returns_client(client_fn(cid))\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "ERROR flwr 2024-03-05 15:05:08,792 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.18\tray::IDLE\n",
      "21516\t0.18\tray::IDLE\n",
      "21505\t0.18\tray::IDLE\n",
      "21507\t0.18\tray::IDLE\n",
      "21499\t0.18\tray::IDLE\n",
      "21504\t0.18\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:05:08,803 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.18\tray::IDLE\n",
      "21516\t0.18\tray::IDLE\n",
      "21505\t0.18\tray::IDLE\n",
      "21507\t0.18\tray::IDLE\n",
      "21499\t0.18\tray::IDLE\n",
      "21504\t0.18\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:05:08,821 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.52\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.26\tray::DefaultActor\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.22\tray::DefaultActor\n",
      "21505\t0.22\tray::DefaultActor\n",
      "21499\t0.22\tray::DefaultActor\n",
      "21508\t0.22\tray::DefaultActor\n",
      "21504\t0.22\tray::DefaultActor\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:05:08,822 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 0ec556f93983fd754e49558001000000, name=DefaultActor.__init__, pid=21516, memory used=0.43GB) was running was 7.07GB / 7.44GB (0.950049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01e894dafc14c58acc1d58301dbd318d734ed5b0ee61f337692679a3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-01e894dafc14c58acc1d58301dbd318d734ed5b0ee61f337692679a3*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.98\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21506\t0.45\tray::DefaultActor.run\n",
      "21505\t0.44\tray::DefaultActor.run\n",
      "21512\t0.44\tray::DefaultActor.run\n",
      "21516\t0.43\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21025\t0.08\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...\n",
      "21145\t0.06\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -u /home/guilherme/cpu-tens...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:05:08,824 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.52\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.26\tray::DefaultActor\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.22\tray::DefaultActor\n",
      "21505\t0.22\tray::DefaultActor\n",
      "21499\t0.22\tray::DefaultActor\n",
      "21508\t0.22\tray::DefaultActor\n",
      "21504\t0.22\tray::DefaultActor\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:05:08,825 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 0ec556f93983fd754e49558001000000, name=DefaultActor.__init__, pid=21516, memory used=0.43GB) was running was 7.07GB / 7.44GB (0.950049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01e894dafc14c58acc1d58301dbd318d734ed5b0ee61f337692679a3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-01e894dafc14c58acc1d58301dbd318d734ed5b0ee61f337692679a3*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.98\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21506\t0.45\tray::DefaultActor.run\n",
      "21505\t0.44\tray::DefaultActor.run\n",
      "21512\t0.44\tray::DefaultActor.run\n",
      "21516\t0.43\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21025\t0.08\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...\n",
      "21145\t0.06\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -u /home/guilherme/cpu-tens...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:05:08,829 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.89\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21506\t0.36\tray::DefaultActor.run\n",
      "21512\t0.35\tray::DefaultActor.run\n",
      "21508\t0.35\tray::DefaultActor\n",
      "21505\t0.35\tray::DefaultActor\n",
      "21516\t0.34\tray::DefaultActor\n",
      "21503\t0.34\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:05:08,829 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.75\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21508\t0.31\tray::DefaultActor.run\n",
      "21505\t0.31\tray::DefaultActor.run\n",
      "21512\t0.31\tray::DefaultActor.run\n",
      "21506\t0.30\tray::DefaultActor.run\n",
      "21516\t0.30\tray::DefaultActor.run\n",
      "21504\t0.30\tray::DefaultActor.run\n",
      "21503\t0.29\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:05:08,831 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.72\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.27\tray::DefaultActor.run\n",
      "21512\t0.24\tray::DefaultActor.run\n",
      "21515\t0.24\tray::DefaultActor.run\n",
      "21508\t0.23\tray::DefaultActor.run\n",
      "21509\t0.23\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21507\t0.22\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:05:08,831 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.78\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.28\tray::DefaultActor.run\n",
      "21515\t0.26\tray::DefaultActor.run\n",
      "21508\t0.26\tray::DefaultActor.run\n",
      "21512\t0.26\tray::DefaultActor.run\n",
      "21507\t0.24\tray::DefaultActor.run\n",
      "21504\t0.24\tray::DefaultActor.run\n",
      "21505\t0.24\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:05:08,834 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.89\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21506\t0.36\tray::DefaultActor.run\n",
      "21512\t0.35\tray::DefaultActor.run\n",
      "21508\t0.35\tray::DefaultActor\n",
      "21505\t0.35\tray::DefaultActor\n",
      "21516\t0.34\tray::DefaultActor\n",
      "21503\t0.34\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:05:08,835 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.21\tray::IDLE\n",
      "21512\t0.21\tray::IDLE\n",
      "21507\t0.21\tray::IDLE\n",
      "21505\t0.21\tray::IDLE\n",
      "21504\t0.21\tray::IDLE\n",
      "21515\t0.21\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:05:08,836 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.18\tray::IDLE\n",
      "21516\t0.18\tray::IDLE\n",
      "21505\t0.18\tray::IDLE\n",
      "21507\t0.18\tray::IDLE\n",
      "21499\t0.18\tray::IDLE\n",
      "21504\t0.18\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:05:08,837 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.75\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21508\t0.31\tray::DefaultActor.run\n",
      "21505\t0.31\tray::DefaultActor.run\n",
      "21512\t0.31\tray::DefaultActor.run\n",
      "21506\t0.30\tray::DefaultActor.run\n",
      "21516\t0.30\tray::DefaultActor.run\n",
      "21504\t0.30\tray::DefaultActor.run\n",
      "21503\t0.29\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:05:08,838 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.78\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.29\tray::DefaultActor.run\n",
      "21508\t0.28\tray::DefaultActor.run\n",
      "21507\t0.27\tray::DefaultActor.run\n",
      "21506\t0.27\tray::DefaultActor.run\n",
      "21512\t0.27\tray::DefaultActor.run\n",
      "21505\t0.27\tray::DefaultActor.run\n",
      "21504\t0.26\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:05:08,838 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.72\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.27\tray::DefaultActor.run\n",
      "21512\t0.24\tray::DefaultActor.run\n",
      "21515\t0.24\tray::DefaultActor.run\n",
      "21508\t0.23\tray::DefaultActor.run\n",
      "21509\t0.23\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21507\t0.22\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:05:08,841 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.78\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.28\tray::DefaultActor.run\n",
      "21515\t0.26\tray::DefaultActor.run\n",
      "21508\t0.26\tray::DefaultActor.run\n",
      "21512\t0.26\tray::DefaultActor.run\n",
      "21507\t0.24\tray::DefaultActor.run\n",
      "21504\t0.24\tray::DefaultActor.run\n",
      "21505\t0.24\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:05:08,842 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.99\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21508\t0.38\tray::DefaultActor.run\n",
      "21505\t0.38\tray::DefaultActor.run\n",
      "21506\t0.37\tray::DefaultActor.run\n",
      "21512\t0.37\tray::DefaultActor.run\n",
      "21516\t0.37\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21025\t0.08\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:05:08,844 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.19\tray::IDLE\n",
      "21516\t0.19\tray::IDLE\n",
      "21507\t0.19\tray::IDLE\n",
      "21505\t0.19\tray::IDLE\n",
      "21504\t0.19\tray::IDLE\n",
      "21499\t0.19\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:05:08,846 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.16\tray::IDLE\n",
      "21516\t0.16\tray::IDLE\n",
      "21511\t0.16\tray::IDLE\n",
      "21507\t0.16\tray::IDLE\n",
      "21504\t0.16\tray::IDLE\n",
      "21505\t0.16\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:05:08,847 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.52\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.26\tray::DefaultActor\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.22\tray::DefaultActor\n",
      "21505\t0.22\tray::DefaultActor\n",
      "21499\t0.22\tray::DefaultActor\n",
      "21508\t0.22\tray::DefaultActor\n",
      "21504\t0.22\tray::DefaultActor\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:05:08,847 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.17\tray::IDLE\n",
      "21512\t0.17\tray::IDLE\n",
      "21507\t0.17\tray::IDLE\n",
      "21505\t0.17\tray::IDLE\n",
      "21511\t0.17\tray::IDLE\n",
      "21504\t0.17\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-03-05 15:05:08,848 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.21\tray::IDLE\n",
      "21512\t0.21\tray::IDLE\n",
      "21507\t0.21\tray::IDLE\n",
      "21505\t0.21\tray::IDLE\n",
      "21504\t0.21\tray::IDLE\n",
      "21515\t0.21\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:05:08,850 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.18\tray::IDLE\n",
      "21516\t0.18\tray::IDLE\n",
      "21505\t0.18\tray::IDLE\n",
      "21507\t0.18\tray::IDLE\n",
      "21499\t0.18\tray::IDLE\n",
      "21504\t0.18\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:05:08,854 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.78\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.29\tray::DefaultActor.run\n",
      "21508\t0.28\tray::DefaultActor.run\n",
      "21507\t0.27\tray::DefaultActor.run\n",
      "21506\t0.27\tray::DefaultActor.run\n",
      "21512\t0.27\tray::DefaultActor.run\n",
      "21505\t0.27\tray::DefaultActor.run\n",
      "21504\t0.26\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:05:08,856 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.99\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21508\t0.38\tray::DefaultActor.run\n",
      "21505\t0.38\tray::DefaultActor.run\n",
      "21506\t0.37\tray::DefaultActor.run\n",
      "21512\t0.37\tray::DefaultActor.run\n",
      "21516\t0.37\tray::DefaultActor.run\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21025\t0.08\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:05:08,859 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.19\tray::IDLE\n",
      "21516\t0.19\tray::IDLE\n",
      "21507\t0.19\tray::IDLE\n",
      "21505\t0.19\tray::IDLE\n",
      "21504\t0.19\tray::IDLE\n",
      "21499\t0.19\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:05:08,860 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.16\tray::IDLE\n",
      "21516\t0.16\tray::IDLE\n",
      "21511\t0.16\tray::IDLE\n",
      "21507\t0.16\tray::IDLE\n",
      "21504\t0.16\tray::IDLE\n",
      "21505\t0.16\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-03-05 15:05:08,860 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.52\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.26\tray::DefaultActor\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21512\t0.22\tray::DefaultActor\n",
      "21505\t0.22\tray::DefaultActor\n",
      "21499\t0.22\tray::DefaultActor\n",
      "21508\t0.22\tray::DefaultActor\n",
      "21504\t0.22\tray::DefaultActor\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-03-05 15:05:08,862 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "10303\t1.61\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "533\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20942\t0.49\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "20894\t0.23\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "21516\t0.17\tray::IDLE\n",
      "21512\t0.17\tray::IDLE\n",
      "21507\t0.17\tray::IDLE\n",
      "21505\t0.17\tray::IDLE\n",
      "21511\t0.17\tray::IDLE\n",
      "21504\t0.17\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "DEBUG flwr 2024-03-05 15:05:08,871 | server.py:187 | evaluate_round 10 received 0 results and 19 failures\n",
      "INFO flwr 2024-03-05 15:05:08,872 | server.py:153 | FL finished in 142.62188674400022\n",
      "INFO flwr 2024-03-05 15:05:08,874 | app.py:226 | app_fit: losses_distributed []\n",
      "INFO flwr 2024-03-05 15:05:08,875 | app.py:227 | app_fit: metrics_distributed_fit {}\n",
      "INFO flwr 2024-03-05 15:05:08,877 | app.py:228 | app_fit: metrics_distributed {}\n",
      "INFO flwr 2024-03-05 15:05:08,879 | app.py:229 | app_fit: losses_centralized [(0, 0.693403422832489), (1, 0.7716444134712219), (2, 0.7965250015258789), (3, 0.7837085127830505), (4, 0.785237729549408), (5, 0.7544503808021545), (6, 0.7448371648788452), (7, 0.7352955341339111), (8, 0.7206368446350098), (9, 0.7143182158470154), (10, 0.7127873301506042)]\n",
      "INFO flwr 2024-03-05 15:05:08,880 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.5219749808311462), (1, 0.3431371748447418), (2, 0.4252054691314697), (3, 0.46832364797592163), (4, 0.48932668566703796), (5, 0.5235030055046082), (6, 0.5410522222518921), (7, 0.559216320514679), (8, 0.5837092995643616), (9, 0.6002097725868225), (10, 0.6153992414474487)]}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(DefaultActor pid=21512)\u001b[0m 2\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21506)\u001b[0m starting client: 15 <class 'str'>\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21506)\u001b[0m starting client:  <class 'int'>\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21506)\u001b[0m Creating client model to client: 15\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21506)\u001b[0m Data X: 1331\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21506)\u001b[0m Data Y: 1331\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21506)\u001b[0m Creating client model to client: 15 round 11\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21505)\u001b[0m starting client: 9 <class 'str'>\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21505)\u001b[0m starting client:  <class 'int'>\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21505)\u001b[0m Creating client model to client: 9\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21505)\u001b[0m Data X: 1331\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21505)\u001b[0m Data Y: 1331\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=21505)\u001b[0m Creating client model to client: 9 round 11\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "History (loss, centralized):\n",
       "\tround 0: 0.693403422832489\n",
       "\tround 1: 0.7716444134712219\n",
       "\tround 2: 0.7965250015258789\n",
       "\tround 3: 0.7837085127830505\n",
       "\tround 4: 0.785237729549408\n",
       "\tround 5: 0.7544503808021545\n",
       "\tround 6: 0.7448371648788452\n",
       "\tround 7: 0.7352955341339111\n",
       "\tround 8: 0.7206368446350098\n",
       "\tround 9: 0.7143182158470154\n",
       "\tround 10: 0.7127873301506042\n",
       "History (metrics, centralized):\n",
       "{'accuracy': [(0, 0.5219749808311462), (1, 0.3431371748447418), (2, 0.4252054691314697), (3, 0.46832364797592163), (4, 0.48932668566703796), (5, 0.5235030055046082), (6, 0.5410522222518921), (7, 0.559216320514679), (8, 0.5837092995643616), (9, 0.6002097725868225), (10, 0.6153992414474487)]}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ray\n",
    "import math\n",
    "# Create an instance of the model and get the parameters\n",
    "\n",
    "# Specify client resources if you need GPU (defaults to 1 CPU and 0 GPU)\n",
    "client_resources = None\n",
    "#if DEVICE.type == \"cuda\":\n",
    "\n",
    "client_resources = {\"num_cpus\": 1}\n",
    "\n",
    "#keras_model = create_keras_model()\n",
    "keras_model.compile(\"adam\", \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "\n",
    "def client_fn(cid) -> FlowerISABELASleepClient:\n",
    "    print(\"starting client: \"+str(cid),type(cid))\n",
    "    #convert client ID to int\n",
    "    clientId = int(cid)\n",
    "    print(\"starting client: \", type(clientId))\n",
    "\n",
    "    data   = clientList[clientId][inputFeatures]\n",
    "    labels = clientList[clientId][outputClasses]\n",
    "    \n",
    "    print(\"Creating client model to client: \"+str(cid))\n",
    "    print(\"Data X: \"+str(len(data)))\n",
    "    print(\"Data Y: \"+str(len(labels)))\n",
    "    \n",
    "    file_global_model = outputFolder+\"/global_model_metrics_after_agregation.csv\"\n",
    "    index_round = 0 \n",
    "    \n",
    "    # get last\n",
    "    if(os.path.isfile(file_global_model)):\n",
    "        dataset = pd.read_csv(file_global_model)\n",
    "        index_round = dataset[\"round\"].max() + 1\n",
    "        del dataset\n",
    "    \n",
    "    # update the index round in the previous checkpoint\n",
    "    if(last_round_checkpoint > -1 and (index_round == last_round_checkpoint)):\n",
    "        index_round = last_round_checkpoint\n",
    "    \n",
    "    print(\"Creating client model to client: \"+str(cid),\"round\",index_round)\n",
    "    # Load and compile a Keras model for CIFAR-10\n",
    "    model = create_keras_model()\n",
    "    model.compile(\"adam\", \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    \n",
    "    proxyClient = FlowerISABELASleepClient(clientId,model,data,labels,index_round)\n",
    "    \n",
    "    return proxyClient\n",
    "\n",
    "strategy = SaveModelStrategy(\n",
    "    min_available_clients=NUMBER_OF_CLIENTS,\n",
    "    evaluate_fn=get_evaluate_fn(keras_model)\n",
    ") # (same arguments as FedAvg here)\n",
    "\n",
    "# load checkpoint\n",
    "if(filename_h5 is not None):\n",
    "    \n",
    "    #npzFile = np.load(filename_np)\n",
    "    keras_model.load_weights(filename_h5)\n",
    "    \n",
    "    initial_parameters = keras_model.get_weights() \n",
    "    # Convert the weights (np.ndarray) to parameters (bytes)\n",
    "    init_param = fl.common.ndarrays_to_parameters(initial_parameters)\n",
    "\n",
    "    strategy = SaveModelStrategy(\n",
    "        min_available_clients=NUMBER_OF_CLIENTS,\n",
    "        evaluate_fn=get_evaluate_fn(keras_model),\n",
    "        initial_parameters = init_param\n",
    "    ) # (same arguments as FedAvg here)\n",
    "\n",
    "# Start simulation\n",
    "fl.simulation.start_simulation(\n",
    "    client_fn=client_fn,\n",
    "    num_clients=NUMBER_OF_CLIENTS,\n",
    "    config=fl.server.ServerConfig(num_rounds=NUMBER_OF_ITERATIONS),  # Just three rounds\n",
    "    client_resources=client_resources,\n",
    "    strategy = strategy\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8f400a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7de2c363",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activity</th>\n",
       "      <th>location</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>light</th>\n",
       "      <th>phone_lock</th>\n",
       "      <th>proximity</th>\n",
       "      <th>sound</th>\n",
       "      <th>time_to_next_alarm</th>\n",
       "      <th>minutes_day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.022970</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.974501</td>\n",
       "      <td>0.681723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.009981</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.604264</td>\n",
       "      <td>0.974402</td>\n",
       "      <td>0.682418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.016817</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.597445</td>\n",
       "      <td>0.974303</td>\n",
       "      <td>0.683113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.010254</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.597445</td>\n",
       "      <td>0.974303</td>\n",
       "      <td>0.683113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.014629</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.536104</td>\n",
       "      <td>0.974204</td>\n",
       "      <td>0.683808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1326</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.011758</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.581536</td>\n",
       "      <td>0.988789</td>\n",
       "      <td>0.581654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1327</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.039923</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.557805</td>\n",
       "      <td>0.988689</td>\n",
       "      <td>0.582349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1328</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.006426</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.574526</td>\n",
       "      <td>0.988491</td>\n",
       "      <td>0.583739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1329</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.020782</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.558190</td>\n",
       "      <td>0.988293</td>\n",
       "      <td>0.585129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1330</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.568211</td>\n",
       "      <td>0.988193</td>\n",
       "      <td>0.585823</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1331 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      activity  location  day_of_week     light  phone_lock  proximity  \\\n",
       "0         0.00       0.0         0.00  0.022970         0.0        1.0   \n",
       "1         0.00       0.5         0.00  0.009981         0.0        1.0   \n",
       "2         0.25       0.5         0.00  0.016817         0.0        1.0   \n",
       "3         0.00       0.5         0.00  0.010254         0.0        1.0   \n",
       "4         0.25       0.5         0.00  0.014629         0.0        1.0   \n",
       "...        ...       ...          ...       ...         ...        ...   \n",
       "1326      1.00       0.0         0.25  0.011758         1.0        0.0   \n",
       "1327      1.00       0.0         0.25  0.039923         0.0        1.0   \n",
       "1328      0.75       0.0         0.25  0.006426         1.0        0.0   \n",
       "1329      1.00       0.0         0.25  0.020782         0.0        1.0   \n",
       "1330      1.00       0.0         0.25  0.000000         1.0        0.0   \n",
       "\n",
       "         sound  time_to_next_alarm  minutes_day  \n",
       "0     0.000000            0.974501     0.681723  \n",
       "1     0.604264            0.974402     0.682418  \n",
       "2     0.597445            0.974303     0.683113  \n",
       "3     0.597445            0.974303     0.683113  \n",
       "4     0.536104            0.974204     0.683808  \n",
       "...        ...                 ...          ...  \n",
       "1326  0.581536            0.988789     0.581654  \n",
       "1327  0.557805            0.988689     0.582349  \n",
       "1328  0.574526            0.988491     0.583739  \n",
       "1329  0.558190            0.988293     0.585129  \n",
       "1330  0.568211            0.988193     0.585823  \n",
       "\n",
       "[1331 rows x 9 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clientList[0][inputFeatures]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4bc76a55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>awake</th>\n",
       "      <th>asleep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1326</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1327</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1328</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1329</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1330</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1331 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      awake  asleep\n",
       "0      True   False\n",
       "1      True   False\n",
       "2      True   False\n",
       "3      True   False\n",
       "4      True   False\n",
       "...     ...     ...\n",
       "1326   True   False\n",
       "1327   True   False\n",
       "1328   True   False\n",
       "1329   True   False\n",
       "1330   True   False\n",
       "\n",
       "[1331 rows x 2 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2024-03-05 15:05:28,479 E 21098 21098] (raylet) node_manager.cc:3084: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa, IP: 172.30.126.159) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.30.126.159`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    }
   ],
   "source": [
    "clientList[0][outputClasses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e25c20d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e2d7f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
