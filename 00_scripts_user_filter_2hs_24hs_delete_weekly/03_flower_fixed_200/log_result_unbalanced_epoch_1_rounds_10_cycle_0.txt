myFlowerExperiment | INFO flwr 2024-03-05 15:02:25,532 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
myFlowerExperiment | INFO flwr 2024-03-05 15:02:25,532 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)
myFlowerExperiment | INFO flwr 2024-03-05 15:02:29,950 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:172.30.126.159': 1.0, 'memory': 2047935284.0, 'object_store_memory': 1023967641.0, 'node:__internal_head__': 1.0, 'CPU': 16.0}
myFlowerExperiment | INFO flwr 2024-03-05 15:02:29,950 | app.py:213 | Flower VCE: Ray initialized with resources: {'node:172.30.126.159': 1.0, 'memory': 2047935284.0, 'object_store_memory': 1023967641.0, 'node:__internal_head__': 1.0, 'CPU': 16.0}
myFlowerExperiment | INFO flwr 2024-03-05 15:02:29,992 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
myFlowerExperiment | INFO flwr 2024-03-05 15:02:29,992 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html
myFlowerExperiment | INFO flwr 2024-03-05 15:02:29,994 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 1}
myFlowerExperiment | INFO flwr 2024-03-05 15:02:29,994 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 1}
myFlowerExperiment | INFO flwr 2024-03-05 15:02:30,011 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 16 actors
myFlowerExperiment | INFO flwr 2024-03-05 15:02:30,011 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 16 actors
myFlowerExperiment | INFO flwr 2024-03-05 15:02:30,015 | server.py:89 | Initializing global parameters
myFlowerExperiment | INFO flwr 2024-03-05 15:02:30,015 | server.py:89 | Initializing global parameters
myFlowerExperiment | INFO flwr 2024-03-05 15:02:30,018 | server.py:276 | Requesting initial parameters from one random client
myFlowerExperiment | INFO flwr 2024-03-05 15:02:30,018 | server.py:276 | Requesting initial parameters from one random client
myFlowerExperiment | INFO flwr 2024-03-05 15:02:38,939 | server.py:280 | Received initial parameters from one random client
myFlowerExperiment | INFO flwr 2024-03-05 15:02:38,939 | server.py:280 | Received initial parameters from one random client
myFlowerExperiment | INFO flwr 2024-03-05 15:02:38,942 | server.py:91 | Evaluating initial parameters
myFlowerExperiment | INFO flwr 2024-03-05 15:02:38,942 | server.py:91 | Evaluating initial parameters
myFlowerExperiment | INFO flwr 2024-03-05 15:02:46,247 | server.py:94 | initial parameters (loss, other metrics): 0.693403422832489, {'accuracy': 0.5219749808311462}
myFlowerExperiment | INFO flwr 2024-03-05 15:02:46,247 | server.py:94 | initial parameters (loss, other metrics): 0.693403422832489, {'accuracy': 0.5219749808311462}
myFlowerExperiment | INFO flwr 2024-03-05 15:02:46,249 | server.py:104 | FL starting
myFlowerExperiment | INFO flwr 2024-03-05 15:02:46,249 | server.py:104 | FL starting
myFlowerExperiment | DEBUG flwr 2024-03-05 15:02:46,250 | server.py:222 | fit_round 1: strategy sampled 19 clients (out of 19)
myFlowerExperiment | DEBUG flwr 2024-03-05 15:02:46,250 | server.py:222 | fit_round 1: strategy sampled 19 clients (out of 19)
myFlowerExperiment | ERROR flwr 2024-03-05 15:02:46,879 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.18	ray::IDLE
21516	0.18	ray::IDLE
21505	0.18	ray::IDLE
21507	0.18	ray::IDLE
21499	0.18	ray::IDLE
21504	0.18	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:02:46,879 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.18	ray::IDLE
21516	0.18	ray::IDLE
21505	0.18	ray::IDLE
21507	0.18	ray::IDLE
21499	0.18	ray::IDLE
21504	0.18	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:02:46,880 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.16	ray::IDLE
21516	0.16	ray::IDLE
21511	0.16	ray::IDLE
21507	0.16	ray::IDLE
21504	0.16	ray::IDLE
21505	0.16	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:02:46,880 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.16	ray::IDLE
21516	0.16	ray::IDLE
21511	0.16	ray::IDLE
21507	0.16	ray::IDLE
21504	0.16	ray::IDLE
21505	0.16	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:02:46,884 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.18	ray::IDLE
21516	0.18	ray::IDLE
21505	0.18	ray::IDLE
21507	0.18	ray::IDLE
21499	0.18	ray::IDLE
21504	0.18	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:02:46,884 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.18	ray::IDLE
21516	0.18	ray::IDLE
21505	0.18	ray::IDLE
21507	0.18	ray::IDLE
21499	0.18	ray::IDLE
21504	0.18	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:02:46,886 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.16	ray::IDLE
21516	0.16	ray::IDLE
21511	0.16	ray::IDLE
21507	0.16	ray::IDLE
21504	0.16	ray::IDLE
21505	0.16	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:02:46,886 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.16	ray::IDLE
21516	0.16	ray::IDLE
21511	0.16	ray::IDLE
21507	0.16	ray::IDLE
21504	0.16	ray::IDLE
21505	0.16	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:02:46,899 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.17	ray::IDLE
21512	0.17	ray::IDLE
21507	0.17	ray::IDLE
21505	0.17	ray::IDLE
21511	0.17	ray::IDLE
21504	0.17	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:02:46,899 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.17	ray::IDLE
21512	0.17	ray::IDLE
21507	0.17	ray::IDLE
21505	0.17	ray::IDLE
21511	0.17	ray::IDLE
21504	0.17	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:02:46,902 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.18	ray::IDLE
21516	0.18	ray::IDLE
21505	0.18	ray::IDLE
21507	0.18	ray::IDLE
21499	0.18	ray::IDLE
21504	0.18	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:02:46,902 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.18	ray::IDLE
21516	0.18	ray::IDLE
21505	0.18	ray::IDLE
21507	0.18	ray::IDLE
21499	0.18	ray::IDLE
21504	0.18	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:02:46,907 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.21	ray::IDLE
21512	0.21	ray::IDLE
21507	0.21	ray::IDLE
21505	0.21	ray::IDLE
21504	0.21	ray::IDLE
21515	0.21	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:02:46,907 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.21	ray::IDLE
21512	0.21	ray::IDLE
21507	0.21	ray::IDLE
21505	0.21	ray::IDLE
21504	0.21	ray::IDLE
21515	0.21	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:02:46,909 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.17	ray::IDLE
21512	0.17	ray::IDLE
21507	0.17	ray::IDLE
21505	0.17	ray::IDLE
21511	0.17	ray::IDLE
21504	0.17	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:02:46,909 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.17	ray::IDLE
21512	0.17	ray::IDLE
21507	0.17	ray::IDLE
21505	0.17	ray::IDLE
21511	0.17	ray::IDLE
21504	0.17	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:02:46,914 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.18	ray::IDLE
21516	0.18	ray::IDLE
21505	0.18	ray::IDLE
21507	0.18	ray::IDLE
21499	0.18	ray::IDLE
21504	0.18	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:02:46,914 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.18	ray::IDLE
21516	0.18	ray::IDLE
21505	0.18	ray::IDLE
21507	0.18	ray::IDLE
21499	0.18	ray::IDLE
21504	0.18	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:02:46,915 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.21	ray::IDLE
21512	0.21	ray::IDLE
21507	0.21	ray::IDLE
21505	0.21	ray::IDLE
21504	0.21	ray::IDLE
21515	0.21	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:02:46,915 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.21	ray::IDLE
21512	0.21	ray::IDLE
21507	0.21	ray::IDLE
21505	0.21	ray::IDLE
21504	0.21	ray::IDLE
21515	0.21	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:02:46,915 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.17	ray::IDLE
21512	0.17	ray::IDLE
21507	0.17	ray::IDLE
21505	0.17	ray::IDLE
21511	0.17	ray::IDLE
21504	0.17	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:02:46,915 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.17	ray::IDLE
21512	0.17	ray::IDLE
21507	0.17	ray::IDLE
21505	0.17	ray::IDLE
21511	0.17	ray::IDLE
21504	0.17	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:02:46,918 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.52	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.26	ray::DefaultActor
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.22	ray::DefaultActor
21505	0.22	ray::DefaultActor
21499	0.22	ray::DefaultActor
21508	0.22	ray::DefaultActor
21504	0.22	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:02:46,918 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.52	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.26	ray::DefaultActor
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.22	ray::DefaultActor
21505	0.22	ray::DefaultActor
21499	0.22	ray::DefaultActor
21508	0.22	ray::DefaultActor
21504	0.22	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:02:46,919 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.19	ray::IDLE
21516	0.19	ray::IDLE
21507	0.19	ray::IDLE
21505	0.19	ray::IDLE
21504	0.19	ray::IDLE
21499	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:02:46,919 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.19	ray::IDLE
21516	0.19	ray::IDLE
21507	0.19	ray::IDLE
21505	0.19	ray::IDLE
21504	0.19	ray::IDLE
21499	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:02:46,931 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.72	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.27	ray::DefaultActor.run
21512	0.24	ray::DefaultActor.run
21515	0.24	ray::DefaultActor.run
21508	0.23	ray::DefaultActor.run
21509	0.23	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21507	0.22	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:02:46,931 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.72	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.27	ray::DefaultActor.run
21512	0.24	ray::DefaultActor.run
21515	0.24	ray::DefaultActor.run
21508	0.23	ray::DefaultActor.run
21509	0.23	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21507	0.22	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:02:46,931 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.17	ray::IDLE
21512	0.17	ray::IDLE
21507	0.17	ray::IDLE
21505	0.17	ray::IDLE
21511	0.17	ray::IDLE
21504	0.17	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:02:46,931 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.17	ray::IDLE
21512	0.17	ray::IDLE
21507	0.17	ray::IDLE
21505	0.17	ray::IDLE
21511	0.17	ray::IDLE
21504	0.17	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:02:46,934 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.16	ray::IDLE
21516	0.16	ray::IDLE
21511	0.16	ray::IDLE
21507	0.16	ray::IDLE
21504	0.16	ray::IDLE
21505	0.16	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:02:46,934 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.16	ray::IDLE
21516	0.16	ray::IDLE
21511	0.16	ray::IDLE
21507	0.16	ray::IDLE
21504	0.16	ray::IDLE
21505	0.16	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:02:46,936 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.52	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.26	ray::DefaultActor
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.22	ray::DefaultActor
21505	0.22	ray::DefaultActor
21499	0.22	ray::DefaultActor
21508	0.22	ray::DefaultActor
21504	0.22	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:02:46,936 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.52	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.26	ray::DefaultActor
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.22	ray::DefaultActor
21505	0.22	ray::DefaultActor
21499	0.22	ray::DefaultActor
21508	0.22	ray::DefaultActor
21504	0.22	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:02:46,936 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.19	ray::IDLE
21516	0.19	ray::IDLE
21507	0.19	ray::IDLE
21505	0.19	ray::IDLE
21504	0.19	ray::IDLE
21499	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:02:46,936 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.19	ray::IDLE
21516	0.19	ray::IDLE
21507	0.19	ray::IDLE
21505	0.19	ray::IDLE
21504	0.19	ray::IDLE
21499	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:02:46,940 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.72	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.27	ray::DefaultActor.run
21512	0.24	ray::DefaultActor.run
21515	0.24	ray::DefaultActor.run
21508	0.23	ray::DefaultActor.run
21509	0.23	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21507	0.22	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:02:46,940 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.72	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.27	ray::DefaultActor.run
21512	0.24	ray::DefaultActor.run
21515	0.24	ray::DefaultActor.run
21508	0.23	ray::DefaultActor.run
21509	0.23	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21507	0.22	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:02:46,942 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.16	ray::IDLE
21516	0.16	ray::IDLE
21511	0.16	ray::IDLE
21507	0.16	ray::IDLE
21504	0.16	ray::IDLE
21505	0.16	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:02:46,942 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.16	ray::IDLE
21516	0.16	ray::IDLE
21511	0.16	ray::IDLE
21507	0.16	ray::IDLE
21504	0.16	ray::IDLE
21505	0.16	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:02:47,240 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.28	ray::DefaultActor.run
21515	0.26	ray::DefaultActor.run
21508	0.26	ray::DefaultActor.run
21512	0.26	ray::DefaultActor.run
21507	0.24	ray::DefaultActor.run
21504	0.24	ray::DefaultActor.run
21505	0.24	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:02:47,240 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.28	ray::DefaultActor.run
21515	0.26	ray::DefaultActor.run
21508	0.26	ray::DefaultActor.run
21512	0.26	ray::DefaultActor.run
21507	0.24	ray::DefaultActor.run
21504	0.24	ray::DefaultActor.run
21505	0.24	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:02:47,243 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.28	ray::DefaultActor.run
21515	0.26	ray::DefaultActor.run
21508	0.26	ray::DefaultActor.run
21512	0.26	ray::DefaultActor.run
21507	0.24	ray::DefaultActor.run
21504	0.24	ray::DefaultActor.run
21505	0.24	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:02:47,243 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.28	ray::DefaultActor.run
21515	0.26	ray::DefaultActor.run
21508	0.26	ray::DefaultActor.run
21512	0.26	ray::DefaultActor.run
21507	0.24	ray::DefaultActor.run
21504	0.24	ray::DefaultActor.run
21505	0.24	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:02:48,077 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.29	ray::DefaultActor.run
21508	0.28	ray::DefaultActor.run
21507	0.27	ray::DefaultActor.run
21506	0.27	ray::DefaultActor.run
21512	0.27	ray::DefaultActor.run
21505	0.27	ray::DefaultActor.run
21504	0.26	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:02:48,077 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.29	ray::DefaultActor.run
21508	0.28	ray::DefaultActor.run
21507	0.27	ray::DefaultActor.run
21506	0.27	ray::DefaultActor.run
21512	0.27	ray::DefaultActor.run
21505	0.27	ray::DefaultActor.run
21504	0.26	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:02:48,079 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.29	ray::DefaultActor.run
21508	0.28	ray::DefaultActor.run
21507	0.27	ray::DefaultActor.run
21506	0.27	ray::DefaultActor.run
21512	0.27	ray::DefaultActor.run
21505	0.27	ray::DefaultActor.run
21504	0.26	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:02:48,079 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.29	ray::DefaultActor.run
21508	0.28	ray::DefaultActor.run
21507	0.27	ray::DefaultActor.run
21506	0.27	ray::DefaultActor.run
21512	0.27	ray::DefaultActor.run
21505	0.27	ray::DefaultActor.run
21504	0.26	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:02:55,421 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.75	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.31	ray::DefaultActor.run
21505	0.31	ray::DefaultActor.run
21512	0.31	ray::DefaultActor.run
21506	0.30	ray::DefaultActor.run
21516	0.30	ray::DefaultActor.run
21504	0.30	ray::DefaultActor.run
21503	0.29	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:02:55,421 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.75	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.31	ray::DefaultActor.run
21505	0.31	ray::DefaultActor.run
21512	0.31	ray::DefaultActor.run
21506	0.30	ray::DefaultActor.run
21516	0.30	ray::DefaultActor.run
21504	0.30	ray::DefaultActor.run
21503	0.29	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:02:55,426 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.75	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.31	ray::DefaultActor.run
21505	0.31	ray::DefaultActor.run
21512	0.31	ray::DefaultActor.run
21506	0.30	ray::DefaultActor.run
21516	0.30	ray::DefaultActor.run
21504	0.30	ray::DefaultActor.run
21503	0.29	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:02:55,426 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.75	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.31	ray::DefaultActor.run
21505	0.31	ray::DefaultActor.run
21512	0.31	ray::DefaultActor.run
21506	0.30	ray::DefaultActor.run
21516	0.30	ray::DefaultActor.run
21504	0.30	ray::DefaultActor.run
21503	0.29	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | DEBUG flwr 2024-03-05 15:02:59,293 | server.py:236 | fit_round 1 received 6 results and 13 failures
myFlowerExperiment | DEBUG flwr 2024-03-05 15:02:59,293 | server.py:236 | fit_round 1 received 6 results and 13 failures
myFlowerExperiment | WARNING flwr 2024-03-05 15:02:59,297 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
myFlowerExperiment | WARNING flwr 2024-03-05 15:02:59,297 | fedavg.py:250 | No fit_metrics_aggregation_fn provided
myFlowerExperiment | INFO flwr 2024-03-05 15:03:04,984 | server.py:125 | fit progress: (1, 0.7716444134712219, {'accuracy': 0.3431371748447418}, 18.733179419999942)
myFlowerExperiment | INFO flwr 2024-03-05 15:03:04,984 | server.py:125 | fit progress: (1, 0.7716444134712219, {'accuracy': 0.3431371748447418}, 18.733179419999942)
myFlowerExperiment | DEBUG flwr 2024-03-05 15:03:04,986 | server.py:173 | evaluate_round 1: strategy sampled 19 clients (out of 19)
myFlowerExperiment | DEBUG flwr 2024-03-05 15:03:04,986 | server.py:173 | evaluate_round 1: strategy sampled 19 clients (out of 19)
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:05,235 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.75	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.31	ray::DefaultActor.run
21505	0.31	ray::DefaultActor.run
21512	0.31	ray::DefaultActor.run
21506	0.30	ray::DefaultActor.run
21516	0.30	ray::DefaultActor.run
21504	0.30	ray::DefaultActor.run
21503	0.29	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:05,235 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.75	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.31	ray::DefaultActor.run
21505	0.31	ray::DefaultActor.run
21512	0.31	ray::DefaultActor.run
21506	0.30	ray::DefaultActor.run
21516	0.30	ray::DefaultActor.run
21504	0.30	ray::DefaultActor.run
21503	0.29	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:05,246 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.75	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.31	ray::DefaultActor.run
21505	0.31	ray::DefaultActor.run
21512	0.31	ray::DefaultActor.run
21506	0.30	ray::DefaultActor.run
21516	0.30	ray::DefaultActor.run
21504	0.30	ray::DefaultActor.run
21503	0.29	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:05,246 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.75	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.31	ray::DefaultActor.run
21505	0.31	ray::DefaultActor.run
21512	0.31	ray::DefaultActor.run
21506	0.30	ray::DefaultActor.run
21516	0.30	ray::DefaultActor.run
21504	0.30	ray::DefaultActor.run
21503	0.29	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:05,255 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.28	ray::DefaultActor.run
21515	0.26	ray::DefaultActor.run
21508	0.26	ray::DefaultActor.run
21512	0.26	ray::DefaultActor.run
21507	0.24	ray::DefaultActor.run
21504	0.24	ray::DefaultActor.run
21505	0.24	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:05,255 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.28	ray::DefaultActor.run
21515	0.26	ray::DefaultActor.run
21508	0.26	ray::DefaultActor.run
21512	0.26	ray::DefaultActor.run
21507	0.24	ray::DefaultActor.run
21504	0.24	ray::DefaultActor.run
21505	0.24	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:05,257 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.16	ray::IDLE
21516	0.16	ray::IDLE
21511	0.16	ray::IDLE
21507	0.16	ray::IDLE
21504	0.16	ray::IDLE
21505	0.16	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:05,257 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.16	ray::IDLE
21516	0.16	ray::IDLE
21511	0.16	ray::IDLE
21507	0.16	ray::IDLE
21504	0.16	ray::IDLE
21505	0.16	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:05,258 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.17	ray::IDLE
21512	0.17	ray::IDLE
21507	0.17	ray::IDLE
21505	0.17	ray::IDLE
21511	0.17	ray::IDLE
21504	0.17	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:05,258 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.17	ray::IDLE
21512	0.17	ray::IDLE
21507	0.17	ray::IDLE
21505	0.17	ray::IDLE
21511	0.17	ray::IDLE
21504	0.17	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:05,260 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.52	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.26	ray::DefaultActor
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.22	ray::DefaultActor
21505	0.22	ray::DefaultActor
21499	0.22	ray::DefaultActor
21508	0.22	ray::DefaultActor
21504	0.22	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:05,260 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.52	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.26	ray::DefaultActor
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.22	ray::DefaultActor
21505	0.22	ray::DefaultActor
21499	0.22	ray::DefaultActor
21508	0.22	ray::DefaultActor
21504	0.22	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:05,260 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.28	ray::DefaultActor.run
21515	0.26	ray::DefaultActor.run
21508	0.26	ray::DefaultActor.run
21512	0.26	ray::DefaultActor.run
21507	0.24	ray::DefaultActor.run
21504	0.24	ray::DefaultActor.run
21505	0.24	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:05,260 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.28	ray::DefaultActor.run
21515	0.26	ray::DefaultActor.run
21508	0.26	ray::DefaultActor.run
21512	0.26	ray::DefaultActor.run
21507	0.24	ray::DefaultActor.run
21504	0.24	ray::DefaultActor.run
21505	0.24	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:05,261 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.72	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.27	ray::DefaultActor.run
21512	0.24	ray::DefaultActor.run
21515	0.24	ray::DefaultActor.run
21508	0.23	ray::DefaultActor.run
21509	0.23	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21507	0.22	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:05,261 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.72	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.27	ray::DefaultActor.run
21512	0.24	ray::DefaultActor.run
21515	0.24	ray::DefaultActor.run
21508	0.23	ray::DefaultActor.run
21509	0.23	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21507	0.22	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:05,262 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.16	ray::IDLE
21516	0.16	ray::IDLE
21511	0.16	ray::IDLE
21507	0.16	ray::IDLE
21504	0.16	ray::IDLE
21505	0.16	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:05,262 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.16	ray::IDLE
21516	0.16	ray::IDLE
21511	0.16	ray::IDLE
21507	0.16	ray::IDLE
21504	0.16	ray::IDLE
21505	0.16	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:05,263 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.17	ray::IDLE
21512	0.17	ray::IDLE
21507	0.17	ray::IDLE
21505	0.17	ray::IDLE
21511	0.17	ray::IDLE
21504	0.17	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:05,263 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.17	ray::IDLE
21512	0.17	ray::IDLE
21507	0.17	ray::IDLE
21505	0.17	ray::IDLE
21511	0.17	ray::IDLE
21504	0.17	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:05,265 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.52	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.26	ray::DefaultActor
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.22	ray::DefaultActor
21505	0.22	ray::DefaultActor
21499	0.22	ray::DefaultActor
21508	0.22	ray::DefaultActor
21504	0.22	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:05,265 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.52	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.26	ray::DefaultActor
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.22	ray::DefaultActor
21505	0.22	ray::DefaultActor
21499	0.22	ray::DefaultActor
21508	0.22	ray::DefaultActor
21504	0.22	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:05,267 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.72	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.27	ray::DefaultActor.run
21512	0.24	ray::DefaultActor.run
21515	0.24	ray::DefaultActor.run
21508	0.23	ray::DefaultActor.run
21509	0.23	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21507	0.22	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:05,267 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.72	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.27	ray::DefaultActor.run
21512	0.24	ray::DefaultActor.run
21515	0.24	ray::DefaultActor.run
21508	0.23	ray::DefaultActor.run
21509	0.23	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21507	0.22	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:05,268 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.29	ray::DefaultActor.run
21508	0.28	ray::DefaultActor.run
21507	0.27	ray::DefaultActor.run
21506	0.27	ray::DefaultActor.run
21512	0.27	ray::DefaultActor.run
21505	0.27	ray::DefaultActor.run
21504	0.26	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:05,270 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.19	ray::IDLE
21516	0.19	ray::IDLE
21507	0.19	ray::IDLE
21505	0.19	ray::IDLE
21504	0.19	ray::IDLE
21499	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:05,268 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.29	ray::DefaultActor.run
21508	0.28	ray::DefaultActor.run
21507	0.27	ray::DefaultActor.run
21506	0.27	ray::DefaultActor.run
21512	0.27	ray::DefaultActor.run
21505	0.27	ray::DefaultActor.run
21504	0.26	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:05,270 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.19	ray::IDLE
21516	0.19	ray::IDLE
21507	0.19	ray::IDLE
21505	0.19	ray::IDLE
21504	0.19	ray::IDLE
21499	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:05,274 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.18	ray::IDLE
21516	0.18	ray::IDLE
21505	0.18	ray::IDLE
21507	0.18	ray::IDLE
21499	0.18	ray::IDLE
21504	0.18	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:05,274 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.18	ray::IDLE
21516	0.18	ray::IDLE
21505	0.18	ray::IDLE
21507	0.18	ray::IDLE
21499	0.18	ray::IDLE
21504	0.18	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:05,277 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.29	ray::DefaultActor.run
21508	0.28	ray::DefaultActor.run
21507	0.27	ray::DefaultActor.run
21506	0.27	ray::DefaultActor.run
21512	0.27	ray::DefaultActor.run
21505	0.27	ray::DefaultActor.run
21504	0.26	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:05,277 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.29	ray::DefaultActor.run
21508	0.28	ray::DefaultActor.run
21507	0.27	ray::DefaultActor.run
21506	0.27	ray::DefaultActor.run
21512	0.27	ray::DefaultActor.run
21505	0.27	ray::DefaultActor.run
21504	0.26	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:05,278 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.19	ray::IDLE
21516	0.19	ray::IDLE
21507	0.19	ray::IDLE
21505	0.19	ray::IDLE
21504	0.19	ray::IDLE
21499	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:05,278 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.19	ray::IDLE
21516	0.19	ray::IDLE
21507	0.19	ray::IDLE
21505	0.19	ray::IDLE
21504	0.19	ray::IDLE
21499	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:05,278 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.18	ray::IDLE
21516	0.18	ray::IDLE
21505	0.18	ray::IDLE
21507	0.18	ray::IDLE
21499	0.18	ray::IDLE
21504	0.18	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:05,278 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.18	ray::IDLE
21516	0.18	ray::IDLE
21505	0.18	ray::IDLE
21507	0.18	ray::IDLE
21499	0.18	ray::IDLE
21504	0.18	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:05,278 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.75	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.31	ray::DefaultActor.run
21505	0.31	ray::DefaultActor.run
21512	0.31	ray::DefaultActor.run
21506	0.30	ray::DefaultActor.run
21516	0.30	ray::DefaultActor.run
21504	0.30	ray::DefaultActor.run
21503	0.29	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:05,278 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.75	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.31	ray::DefaultActor.run
21505	0.31	ray::DefaultActor.run
21512	0.31	ray::DefaultActor.run
21506	0.30	ray::DefaultActor.run
21516	0.30	ray::DefaultActor.run
21504	0.30	ray::DefaultActor.run
21503	0.29	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:05,279 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.21	ray::IDLE
21512	0.21	ray::IDLE
21507	0.21	ray::IDLE
21505	0.21	ray::IDLE
21504	0.21	ray::IDLE
21515	0.21	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:05,279 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.21	ray::IDLE
21512	0.21	ray::IDLE
21507	0.21	ray::IDLE
21505	0.21	ray::IDLE
21504	0.21	ray::IDLE
21515	0.21	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:05,284 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.75	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.31	ray::DefaultActor.run
21505	0.31	ray::DefaultActor.run
21512	0.31	ray::DefaultActor.run
21506	0.30	ray::DefaultActor.run
21516	0.30	ray::DefaultActor.run
21504	0.30	ray::DefaultActor.run
21503	0.29	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:05,284 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.75	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.31	ray::DefaultActor.run
21505	0.31	ray::DefaultActor.run
21512	0.31	ray::DefaultActor.run
21506	0.30	ray::DefaultActor.run
21516	0.30	ray::DefaultActor.run
21504	0.30	ray::DefaultActor.run
21503	0.29	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:05,285 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.21	ray::IDLE
21512	0.21	ray::IDLE
21507	0.21	ray::IDLE
21505	0.21	ray::IDLE
21504	0.21	ray::IDLE
21515	0.21	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:05,285 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.21	ray::IDLE
21512	0.21	ray::IDLE
21507	0.21	ray::IDLE
21505	0.21	ray::IDLE
21504	0.21	ray::IDLE
21515	0.21	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | DEBUG flwr 2024-03-05 15:03:05,293 | server.py:187 | evaluate_round 1 received 0 results and 19 failures
myFlowerExperiment | DEBUG flwr 2024-03-05 15:03:05,293 | server.py:187 | evaluate_round 1 received 0 results and 19 failures
myFlowerExperiment | DEBUG flwr 2024-03-05 15:03:05,295 | server.py:222 | fit_round 2: strategy sampled 19 clients (out of 19)
myFlowerExperiment | DEBUG flwr 2024-03-05 15:03:05,295 | server.py:222 | fit_round 2: strategy sampled 19 clients (out of 19)
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:05,633 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.21	ray::IDLE
21512	0.21	ray::IDLE
21507	0.21	ray::IDLE
21505	0.21	ray::IDLE
21504	0.21	ray::IDLE
21515	0.21	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:05,633 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.21	ray::IDLE
21512	0.21	ray::IDLE
21507	0.21	ray::IDLE
21505	0.21	ray::IDLE
21504	0.21	ray::IDLE
21515	0.21	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:05,647 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.21	ray::IDLE
21512	0.21	ray::IDLE
21507	0.21	ray::IDLE
21505	0.21	ray::IDLE
21504	0.21	ray::IDLE
21515	0.21	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:05,647 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.21	ray::IDLE
21512	0.21	ray::IDLE
21507	0.21	ray::IDLE
21505	0.21	ray::IDLE
21504	0.21	ray::IDLE
21515	0.21	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:05,666 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.16	ray::IDLE
21516	0.16	ray::IDLE
21511	0.16	ray::IDLE
21507	0.16	ray::IDLE
21504	0.16	ray::IDLE
21505	0.16	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:05,666 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.16	ray::IDLE
21516	0.16	ray::IDLE
21511	0.16	ray::IDLE
21507	0.16	ray::IDLE
21504	0.16	ray::IDLE
21505	0.16	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:05,669 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.75	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.31	ray::DefaultActor.run
21505	0.31	ray::DefaultActor.run
21512	0.31	ray::DefaultActor.run
21506	0.30	ray::DefaultActor.run
21516	0.30	ray::DefaultActor.run
21504	0.30	ray::DefaultActor.run
21503	0.29	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:05,669 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.75	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.31	ray::DefaultActor.run
21505	0.31	ray::DefaultActor.run
21512	0.31	ray::DefaultActor.run
21506	0.30	ray::DefaultActor.run
21516	0.30	ray::DefaultActor.run
21504	0.30	ray::DefaultActor.run
21503	0.29	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:05,671 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.16	ray::IDLE
21516	0.16	ray::IDLE
21511	0.16	ray::IDLE
21507	0.16	ray::IDLE
21504	0.16	ray::IDLE
21505	0.16	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:05,671 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.16	ray::IDLE
21516	0.16	ray::IDLE
21511	0.16	ray::IDLE
21507	0.16	ray::IDLE
21504	0.16	ray::IDLE
21505	0.16	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:05,673 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.75	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.31	ray::DefaultActor.run
21505	0.31	ray::DefaultActor.run
21512	0.31	ray::DefaultActor.run
21506	0.30	ray::DefaultActor.run
21516	0.30	ray::DefaultActor.run
21504	0.30	ray::DefaultActor.run
21503	0.29	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:05,673 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.75	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.31	ray::DefaultActor.run
21505	0.31	ray::DefaultActor.run
21512	0.31	ray::DefaultActor.run
21506	0.30	ray::DefaultActor.run
21516	0.30	ray::DefaultActor.run
21504	0.30	ray::DefaultActor.run
21503	0.29	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:05,674 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.75	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.31	ray::DefaultActor.run
21505	0.31	ray::DefaultActor.run
21512	0.31	ray::DefaultActor.run
21506	0.30	ray::DefaultActor.run
21516	0.30	ray::DefaultActor.run
21504	0.30	ray::DefaultActor.run
21503	0.29	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:05,674 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.75	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.31	ray::DefaultActor.run
21505	0.31	ray::DefaultActor.run
21512	0.31	ray::DefaultActor.run
21506	0.30	ray::DefaultActor.run
21516	0.30	ray::DefaultActor.run
21504	0.30	ray::DefaultActor.run
21503	0.29	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:05,674 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.18	ray::IDLE
21516	0.18	ray::IDLE
21505	0.18	ray::IDLE
21507	0.18	ray::IDLE
21499	0.18	ray::IDLE
21504	0.18	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:05,674 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.18	ray::IDLE
21516	0.18	ray::IDLE
21505	0.18	ray::IDLE
21507	0.18	ray::IDLE
21499	0.18	ray::IDLE
21504	0.18	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:05,675 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.29	ray::DefaultActor.run
21508	0.28	ray::DefaultActor.run
21507	0.27	ray::DefaultActor.run
21506	0.27	ray::DefaultActor.run
21512	0.27	ray::DefaultActor.run
21505	0.27	ray::DefaultActor.run
21504	0.26	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:05,675 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.29	ray::DefaultActor.run
21508	0.28	ray::DefaultActor.run
21507	0.27	ray::DefaultActor.run
21506	0.27	ray::DefaultActor.run
21512	0.27	ray::DefaultActor.run
21505	0.27	ray::DefaultActor.run
21504	0.26	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:05,677 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.89	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.36	ray::DefaultActor.run
21512	0.35	ray::DefaultActor.run
21508	0.35	ray::DefaultActor
21505	0.35	ray::DefaultActor
21516	0.34	ray::DefaultActor
21503	0.34	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:05,677 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.89	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.36	ray::DefaultActor.run
21512	0.35	ray::DefaultActor.run
21508	0.35	ray::DefaultActor
21505	0.35	ray::DefaultActor
21516	0.34	ray::DefaultActor
21503	0.34	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:05,763 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.17	ray::IDLE
21512	0.17	ray::IDLE
21507	0.17	ray::IDLE
21505	0.17	ray::IDLE
21511	0.17	ray::IDLE
21504	0.17	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:05,763 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.17	ray::IDLE
21512	0.17	ray::IDLE
21507	0.17	ray::IDLE
21505	0.17	ray::IDLE
21511	0.17	ray::IDLE
21504	0.17	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:05,764 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.75	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.31	ray::DefaultActor.run
21505	0.31	ray::DefaultActor.run
21512	0.31	ray::DefaultActor.run
21506	0.30	ray::DefaultActor.run
21516	0.30	ray::DefaultActor.run
21504	0.30	ray::DefaultActor.run
21503	0.29	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:05,764 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.75	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.31	ray::DefaultActor.run
21505	0.31	ray::DefaultActor.run
21512	0.31	ray::DefaultActor.run
21506	0.30	ray::DefaultActor.run
21516	0.30	ray::DefaultActor.run
21504	0.30	ray::DefaultActor.run
21503	0.29	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:05,765 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.28	ray::DefaultActor.run
21515	0.26	ray::DefaultActor.run
21508	0.26	ray::DefaultActor.run
21512	0.26	ray::DefaultActor.run
21507	0.24	ray::DefaultActor.run
21504	0.24	ray::DefaultActor.run
21505	0.24	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:05,765 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.28	ray::DefaultActor.run
21515	0.26	ray::DefaultActor.run
21508	0.26	ray::DefaultActor.run
21512	0.26	ray::DefaultActor.run
21507	0.24	ray::DefaultActor.run
21504	0.24	ray::DefaultActor.run
21505	0.24	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:05,766 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.18	ray::IDLE
21516	0.18	ray::IDLE
21505	0.18	ray::IDLE
21507	0.18	ray::IDLE
21499	0.18	ray::IDLE
21504	0.18	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:05,766 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.18	ray::IDLE
21516	0.18	ray::IDLE
21505	0.18	ray::IDLE
21507	0.18	ray::IDLE
21499	0.18	ray::IDLE
21504	0.18	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:05,767 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.18	ray::IDLE
21516	0.18	ray::IDLE
21505	0.18	ray::IDLE
21507	0.18	ray::IDLE
21499	0.18	ray::IDLE
21504	0.18	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:05,767 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.18	ray::IDLE
21516	0.18	ray::IDLE
21505	0.18	ray::IDLE
21507	0.18	ray::IDLE
21499	0.18	ray::IDLE
21504	0.18	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:05,767 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.21	ray::IDLE
21512	0.21	ray::IDLE
21507	0.21	ray::IDLE
21505	0.21	ray::IDLE
21504	0.21	ray::IDLE
21515	0.21	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:05,767 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.21	ray::IDLE
21512	0.21	ray::IDLE
21507	0.21	ray::IDLE
21505	0.21	ray::IDLE
21504	0.21	ray::IDLE
21515	0.21	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:05,770 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.72	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.27	ray::DefaultActor.run
21512	0.24	ray::DefaultActor.run
21515	0.24	ray::DefaultActor.run
21508	0.23	ray::DefaultActor.run
21509	0.23	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21507	0.22	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:05,770 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.72	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.27	ray::DefaultActor.run
21512	0.24	ray::DefaultActor.run
21515	0.24	ray::DefaultActor.run
21508	0.23	ray::DefaultActor.run
21509	0.23	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21507	0.22	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:05,770 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.52	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.26	ray::DefaultActor
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.22	ray::DefaultActor
21505	0.22	ray::DefaultActor
21499	0.22	ray::DefaultActor
21508	0.22	ray::DefaultActor
21504	0.22	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:05,770 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.52	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.26	ray::DefaultActor
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.22	ray::DefaultActor
21505	0.22	ray::DefaultActor
21499	0.22	ray::DefaultActor
21508	0.22	ray::DefaultActor
21504	0.22	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:05,771 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.29	ray::DefaultActor.run
21508	0.28	ray::DefaultActor.run
21507	0.27	ray::DefaultActor.run
21506	0.27	ray::DefaultActor.run
21512	0.27	ray::DefaultActor.run
21505	0.27	ray::DefaultActor.run
21504	0.26	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:05,771 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.29	ray::DefaultActor.run
21508	0.28	ray::DefaultActor.run
21507	0.27	ray::DefaultActor.run
21506	0.27	ray::DefaultActor.run
21512	0.27	ray::DefaultActor.run
21505	0.27	ray::DefaultActor.run
21504	0.26	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:05,771 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.19	ray::IDLE
21516	0.19	ray::IDLE
21507	0.19	ray::IDLE
21505	0.19	ray::IDLE
21504	0.19	ray::IDLE
21499	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:05,771 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.19	ray::IDLE
21516	0.19	ray::IDLE
21507	0.19	ray::IDLE
21505	0.19	ray::IDLE
21504	0.19	ray::IDLE
21499	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:05,775 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.89	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.36	ray::DefaultActor.run
21512	0.35	ray::DefaultActor.run
21508	0.35	ray::DefaultActor
21505	0.35	ray::DefaultActor
21516	0.34	ray::DefaultActor
21503	0.34	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:05,775 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.89	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.36	ray::DefaultActor.run
21512	0.35	ray::DefaultActor.run
21508	0.35	ray::DefaultActor
21505	0.35	ray::DefaultActor
21516	0.34	ray::DefaultActor
21503	0.34	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:05,776 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.17	ray::IDLE
21512	0.17	ray::IDLE
21507	0.17	ray::IDLE
21505	0.17	ray::IDLE
21511	0.17	ray::IDLE
21504	0.17	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:05,776 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.17	ray::IDLE
21512	0.17	ray::IDLE
21507	0.17	ray::IDLE
21505	0.17	ray::IDLE
21511	0.17	ray::IDLE
21504	0.17	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:05,780 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.28	ray::DefaultActor.run
21515	0.26	ray::DefaultActor.run
21508	0.26	ray::DefaultActor.run
21512	0.26	ray::DefaultActor.run
21507	0.24	ray::DefaultActor.run
21504	0.24	ray::DefaultActor.run
21505	0.24	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:05,780 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.28	ray::DefaultActor.run
21515	0.26	ray::DefaultActor.run
21508	0.26	ray::DefaultActor.run
21512	0.26	ray::DefaultActor.run
21507	0.24	ray::DefaultActor.run
21504	0.24	ray::DefaultActor.run
21505	0.24	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:05,782 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.18	ray::IDLE
21516	0.18	ray::IDLE
21505	0.18	ray::IDLE
21507	0.18	ray::IDLE
21499	0.18	ray::IDLE
21504	0.18	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:05,782 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.18	ray::IDLE
21516	0.18	ray::IDLE
21505	0.18	ray::IDLE
21507	0.18	ray::IDLE
21499	0.18	ray::IDLE
21504	0.18	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:05,786 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.21	ray::IDLE
21512	0.21	ray::IDLE
21507	0.21	ray::IDLE
21505	0.21	ray::IDLE
21504	0.21	ray::IDLE
21515	0.21	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:05,786 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.21	ray::IDLE
21512	0.21	ray::IDLE
21507	0.21	ray::IDLE
21505	0.21	ray::IDLE
21504	0.21	ray::IDLE
21515	0.21	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:05,787 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.72	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.27	ray::DefaultActor.run
21512	0.24	ray::DefaultActor.run
21515	0.24	ray::DefaultActor.run
21508	0.23	ray::DefaultActor.run
21509	0.23	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21507	0.22	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:05,787 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.72	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.27	ray::DefaultActor.run
21512	0.24	ray::DefaultActor.run
21515	0.24	ray::DefaultActor.run
21508	0.23	ray::DefaultActor.run
21509	0.23	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21507	0.22	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:05,787 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.52	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.26	ray::DefaultActor
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.22	ray::DefaultActor
21505	0.22	ray::DefaultActor
21499	0.22	ray::DefaultActor
21508	0.22	ray::DefaultActor
21504	0.22	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:05,787 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.52	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.26	ray::DefaultActor
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.22	ray::DefaultActor
21505	0.22	ray::DefaultActor
21499	0.22	ray::DefaultActor
21508	0.22	ray::DefaultActor
21504	0.22	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:05,791 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.19	ray::IDLE
21516	0.19	ray::IDLE
21507	0.19	ray::IDLE
21505	0.19	ray::IDLE
21504	0.19	ray::IDLE
21499	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:05,791 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.19	ray::IDLE
21516	0.19	ray::IDLE
21507	0.19	ray::IDLE
21505	0.19	ray::IDLE
21504	0.19	ray::IDLE
21499	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | DEBUG flwr 2024-03-05 15:03:14,254 | server.py:236 | fit_round 2 received 5 results and 14 failures
myFlowerExperiment | DEBUG flwr 2024-03-05 15:03:14,254 | server.py:236 | fit_round 2 received 5 results and 14 failures
myFlowerExperiment | INFO flwr 2024-03-05 15:03:19,754 | server.py:125 | fit progress: (2, 0.7965250015258789, {'accuracy': 0.4252054691314697}, 33.50355370499892)
myFlowerExperiment | INFO flwr 2024-03-05 15:03:19,754 | server.py:125 | fit progress: (2, 0.7965250015258789, {'accuracy': 0.4252054691314697}, 33.50355370499892)
myFlowerExperiment | DEBUG flwr 2024-03-05 15:03:19,756 | server.py:173 | evaluate_round 2: strategy sampled 19 clients (out of 19)
myFlowerExperiment | DEBUG flwr 2024-03-05 15:03:19,756 | server.py:173 | evaluate_round 2: strategy sampled 19 clients (out of 19)
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:19,993 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.19	ray::IDLE
21516	0.19	ray::IDLE
21507	0.19	ray::IDLE
21505	0.19	ray::IDLE
21504	0.19	ray::IDLE
21499	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:19,993 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.19	ray::IDLE
21516	0.19	ray::IDLE
21507	0.19	ray::IDLE
21505	0.19	ray::IDLE
21504	0.19	ray::IDLE
21499	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:19,996 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.18	ray::IDLE
21516	0.18	ray::IDLE
21505	0.18	ray::IDLE
21507	0.18	ray::IDLE
21499	0.18	ray::IDLE
21504	0.18	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:19,996 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.18	ray::IDLE
21516	0.18	ray::IDLE
21505	0.18	ray::IDLE
21507	0.18	ray::IDLE
21499	0.18	ray::IDLE
21504	0.18	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:19,996 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.72	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.27	ray::DefaultActor.run
21512	0.24	ray::DefaultActor.run
21515	0.24	ray::DefaultActor.run
21508	0.23	ray::DefaultActor.run
21509	0.23	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21507	0.22	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:19,996 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.72	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.27	ray::DefaultActor.run
21512	0.24	ray::DefaultActor.run
21515	0.24	ray::DefaultActor.run
21508	0.23	ray::DefaultActor.run
21509	0.23	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21507	0.22	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:19,996 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.21	ray::IDLE
21512	0.21	ray::IDLE
21507	0.21	ray::IDLE
21505	0.21	ray::IDLE
21504	0.21	ray::IDLE
21515	0.21	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:19,996 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.21	ray::IDLE
21512	0.21	ray::IDLE
21507	0.21	ray::IDLE
21505	0.21	ray::IDLE
21504	0.21	ray::IDLE
21515	0.21	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:20,000 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.19	ray::IDLE
21516	0.19	ray::IDLE
21507	0.19	ray::IDLE
21505	0.19	ray::IDLE
21504	0.19	ray::IDLE
21499	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:20,000 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.19	ray::IDLE
21516	0.19	ray::IDLE
21507	0.19	ray::IDLE
21505	0.19	ray::IDLE
21504	0.19	ray::IDLE
21499	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:20,000 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.89	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.36	ray::DefaultActor.run
21512	0.35	ray::DefaultActor.run
21508	0.35	ray::DefaultActor
21505	0.35	ray::DefaultActor
21516	0.34	ray::DefaultActor
21503	0.34	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:20,000 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.89	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.36	ray::DefaultActor.run
21512	0.35	ray::DefaultActor.run
21508	0.35	ray::DefaultActor
21505	0.35	ray::DefaultActor
21516	0.34	ray::DefaultActor
21503	0.34	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:20,001 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.18	ray::IDLE
21516	0.18	ray::IDLE
21505	0.18	ray::IDLE
21507	0.18	ray::IDLE
21499	0.18	ray::IDLE
21504	0.18	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:20,001 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.18	ray::IDLE
21516	0.18	ray::IDLE
21505	0.18	ray::IDLE
21507	0.18	ray::IDLE
21499	0.18	ray::IDLE
21504	0.18	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:20,003 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.72	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.27	ray::DefaultActor.run
21512	0.24	ray::DefaultActor.run
21515	0.24	ray::DefaultActor.run
21508	0.23	ray::DefaultActor.run
21509	0.23	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21507	0.22	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:20,003 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.72	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.27	ray::DefaultActor.run
21512	0.24	ray::DefaultActor.run
21515	0.24	ray::DefaultActor.run
21508	0.23	ray::DefaultActor.run
21509	0.23	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21507	0.22	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:20,003 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.29	ray::DefaultActor.run
21508	0.28	ray::DefaultActor.run
21507	0.27	ray::DefaultActor.run
21506	0.27	ray::DefaultActor.run
21512	0.27	ray::DefaultActor.run
21505	0.27	ray::DefaultActor.run
21504	0.26	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:20,003 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.29	ray::DefaultActor.run
21508	0.28	ray::DefaultActor.run
21507	0.27	ray::DefaultActor.run
21506	0.27	ray::DefaultActor.run
21512	0.27	ray::DefaultActor.run
21505	0.27	ray::DefaultActor.run
21504	0.26	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:20,004 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.21	ray::IDLE
21512	0.21	ray::IDLE
21507	0.21	ray::IDLE
21505	0.21	ray::IDLE
21504	0.21	ray::IDLE
21515	0.21	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:20,004 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.21	ray::IDLE
21512	0.21	ray::IDLE
21507	0.21	ray::IDLE
21505	0.21	ray::IDLE
21504	0.21	ray::IDLE
21515	0.21	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:20,004 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.16	ray::IDLE
21516	0.16	ray::IDLE
21511	0.16	ray::IDLE
21507	0.16	ray::IDLE
21504	0.16	ray::IDLE
21505	0.16	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:20,004 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.16	ray::IDLE
21516	0.16	ray::IDLE
21511	0.16	ray::IDLE
21507	0.16	ray::IDLE
21504	0.16	ray::IDLE
21505	0.16	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:20,009 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.17	ray::IDLE
21512	0.17	ray::IDLE
21507	0.17	ray::IDLE
21505	0.17	ray::IDLE
21511	0.17	ray::IDLE
21504	0.17	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:20,009 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.17	ray::IDLE
21512	0.17	ray::IDLE
21507	0.17	ray::IDLE
21505	0.17	ray::IDLE
21511	0.17	ray::IDLE
21504	0.17	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:20,007 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.89	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.36	ray::DefaultActor.run
21512	0.35	ray::DefaultActor.run
21508	0.35	ray::DefaultActor
21505	0.35	ray::DefaultActor
21516	0.34	ray::DefaultActor
21503	0.34	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:20,007 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.89	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.36	ray::DefaultActor.run
21512	0.35	ray::DefaultActor.run
21508	0.35	ray::DefaultActor
21505	0.35	ray::DefaultActor
21516	0.34	ray::DefaultActor
21503	0.34	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:20,010 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.75	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.31	ray::DefaultActor.run
21505	0.31	ray::DefaultActor.run
21512	0.31	ray::DefaultActor.run
21506	0.30	ray::DefaultActor.run
21516	0.30	ray::DefaultActor.run
21504	0.30	ray::DefaultActor.run
21503	0.29	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:20,010 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.75	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.31	ray::DefaultActor.run
21505	0.31	ray::DefaultActor.run
21512	0.31	ray::DefaultActor.run
21506	0.30	ray::DefaultActor.run
21516	0.30	ray::DefaultActor.run
21504	0.30	ray::DefaultActor.run
21503	0.29	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:20,012 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.52	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.26	ray::DefaultActor
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.22	ray::DefaultActor
21505	0.22	ray::DefaultActor
21499	0.22	ray::DefaultActor
21508	0.22	ray::DefaultActor
21504	0.22	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:20,012 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.52	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.26	ray::DefaultActor
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.22	ray::DefaultActor
21505	0.22	ray::DefaultActor
21499	0.22	ray::DefaultActor
21508	0.22	ray::DefaultActor
21504	0.22	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:20,015 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.29	ray::DefaultActor.run
21508	0.28	ray::DefaultActor.run
21507	0.27	ray::DefaultActor.run
21506	0.27	ray::DefaultActor.run
21512	0.27	ray::DefaultActor.run
21505	0.27	ray::DefaultActor.run
21504	0.26	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:20,015 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.29	ray::DefaultActor.run
21508	0.28	ray::DefaultActor.run
21507	0.27	ray::DefaultActor.run
21506	0.27	ray::DefaultActor.run
21512	0.27	ray::DefaultActor.run
21505	0.27	ray::DefaultActor.run
21504	0.26	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:20,015 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.28	ray::DefaultActor.run
21515	0.26	ray::DefaultActor.run
21508	0.26	ray::DefaultActor.run
21512	0.26	ray::DefaultActor.run
21507	0.24	ray::DefaultActor.run
21504	0.24	ray::DefaultActor.run
21505	0.24	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:20,015 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.28	ray::DefaultActor.run
21515	0.26	ray::DefaultActor.run
21508	0.26	ray::DefaultActor.run
21512	0.26	ray::DefaultActor.run
21507	0.24	ray::DefaultActor.run
21504	0.24	ray::DefaultActor.run
21505	0.24	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:20,018 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.16	ray::IDLE
21516	0.16	ray::IDLE
21511	0.16	ray::IDLE
21507	0.16	ray::IDLE
21504	0.16	ray::IDLE
21505	0.16	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:20,018 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.16	ray::IDLE
21516	0.16	ray::IDLE
21511	0.16	ray::IDLE
21507	0.16	ray::IDLE
21504	0.16	ray::IDLE
21505	0.16	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:20,020 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.17	ray::IDLE
21512	0.17	ray::IDLE
21507	0.17	ray::IDLE
21505	0.17	ray::IDLE
21511	0.17	ray::IDLE
21504	0.17	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:20,020 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.17	ray::IDLE
21512	0.17	ray::IDLE
21507	0.17	ray::IDLE
21505	0.17	ray::IDLE
21511	0.17	ray::IDLE
21504	0.17	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:20,022 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.75	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.31	ray::DefaultActor.run
21505	0.31	ray::DefaultActor.run
21512	0.31	ray::DefaultActor.run
21506	0.30	ray::DefaultActor.run
21516	0.30	ray::DefaultActor.run
21504	0.30	ray::DefaultActor.run
21503	0.29	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:20,022 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.75	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.31	ray::DefaultActor.run
21505	0.31	ray::DefaultActor.run
21512	0.31	ray::DefaultActor.run
21506	0.30	ray::DefaultActor.run
21516	0.30	ray::DefaultActor.run
21504	0.30	ray::DefaultActor.run
21503	0.29	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:20,024 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.52	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.26	ray::DefaultActor
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.22	ray::DefaultActor
21505	0.22	ray::DefaultActor
21499	0.22	ray::DefaultActor
21508	0.22	ray::DefaultActor
21504	0.22	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:20,024 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.52	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.26	ray::DefaultActor
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.22	ray::DefaultActor
21505	0.22	ray::DefaultActor
21499	0.22	ray::DefaultActor
21508	0.22	ray::DefaultActor
21504	0.22	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:20,027 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.28	ray::DefaultActor.run
21515	0.26	ray::DefaultActor.run
21508	0.26	ray::DefaultActor.run
21512	0.26	ray::DefaultActor.run
21507	0.24	ray::DefaultActor.run
21504	0.24	ray::DefaultActor.run
21505	0.24	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:20,027 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.28	ray::DefaultActor.run
21515	0.26	ray::DefaultActor.run
21508	0.26	ray::DefaultActor.run
21512	0.26	ray::DefaultActor.run
21507	0.24	ray::DefaultActor.run
21504	0.24	ray::DefaultActor.run
21505	0.24	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | DEBUG flwr 2024-03-05 15:03:20,033 | server.py:187 | evaluate_round 2 received 0 results and 19 failures
myFlowerExperiment | DEBUG flwr 2024-03-05 15:03:20,033 | server.py:187 | evaluate_round 2 received 0 results and 19 failures
myFlowerExperiment | DEBUG flwr 2024-03-05 15:03:20,034 | server.py:222 | fit_round 3: strategy sampled 19 clients (out of 19)
myFlowerExperiment | DEBUG flwr 2024-03-05 15:03:20,034 | server.py:222 | fit_round 3: strategy sampled 19 clients (out of 19)
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:20,283 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.28	ray::DefaultActor.run
21515	0.26	ray::DefaultActor.run
21508	0.26	ray::DefaultActor.run
21512	0.26	ray::DefaultActor.run
21507	0.24	ray::DefaultActor.run
21504	0.24	ray::DefaultActor.run
21505	0.24	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:20,283 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.28	ray::DefaultActor.run
21515	0.26	ray::DefaultActor.run
21508	0.26	ray::DefaultActor.run
21512	0.26	ray::DefaultActor.run
21507	0.24	ray::DefaultActor.run
21504	0.24	ray::DefaultActor.run
21505	0.24	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:20,301 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.28	ray::DefaultActor.run
21515	0.26	ray::DefaultActor.run
21508	0.26	ray::DefaultActor.run
21512	0.26	ray::DefaultActor.run
21507	0.24	ray::DefaultActor.run
21504	0.24	ray::DefaultActor.run
21505	0.24	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:20,301 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.28	ray::DefaultActor.run
21515	0.26	ray::DefaultActor.run
21508	0.26	ray::DefaultActor.run
21512	0.26	ray::DefaultActor.run
21507	0.24	ray::DefaultActor.run
21504	0.24	ray::DefaultActor.run
21505	0.24	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:20,308 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.75	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.31	ray::DefaultActor.run
21505	0.31	ray::DefaultActor.run
21512	0.31	ray::DefaultActor.run
21506	0.30	ray::DefaultActor.run
21516	0.30	ray::DefaultActor.run
21504	0.30	ray::DefaultActor.run
21503	0.29	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:20,308 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.75	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.31	ray::DefaultActor.run
21505	0.31	ray::DefaultActor.run
21512	0.31	ray::DefaultActor.run
21506	0.30	ray::DefaultActor.run
21516	0.30	ray::DefaultActor.run
21504	0.30	ray::DefaultActor.run
21503	0.29	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:20,325 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.75	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.31	ray::DefaultActor.run
21505	0.31	ray::DefaultActor.run
21512	0.31	ray::DefaultActor.run
21506	0.30	ray::DefaultActor.run
21516	0.30	ray::DefaultActor.run
21504	0.30	ray::DefaultActor.run
21503	0.29	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:20,325 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.75	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.31	ray::DefaultActor.run
21505	0.31	ray::DefaultActor.run
21512	0.31	ray::DefaultActor.run
21506	0.30	ray::DefaultActor.run
21516	0.30	ray::DefaultActor.run
21504	0.30	ray::DefaultActor.run
21503	0.29	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:20,334 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.18	ray::IDLE
21516	0.18	ray::IDLE
21505	0.18	ray::IDLE
21507	0.18	ray::IDLE
21499	0.18	ray::IDLE
21504	0.18	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:20,334 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.18	ray::IDLE
21516	0.18	ray::IDLE
21505	0.18	ray::IDLE
21507	0.18	ray::IDLE
21499	0.18	ray::IDLE
21504	0.18	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:20,336 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.19	ray::IDLE
21516	0.19	ray::IDLE
21507	0.19	ray::IDLE
21505	0.19	ray::IDLE
21504	0.19	ray::IDLE
21499	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:20,336 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.19	ray::IDLE
21516	0.19	ray::IDLE
21507	0.19	ray::IDLE
21505	0.19	ray::IDLE
21504	0.19	ray::IDLE
21499	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:20,340 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.18	ray::IDLE
21516	0.18	ray::IDLE
21505	0.18	ray::IDLE
21507	0.18	ray::IDLE
21499	0.18	ray::IDLE
21504	0.18	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:20,340 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.18	ray::IDLE
21516	0.18	ray::IDLE
21505	0.18	ray::IDLE
21507	0.18	ray::IDLE
21499	0.18	ray::IDLE
21504	0.18	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:20,343 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.52	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.26	ray::DefaultActor
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.22	ray::DefaultActor
21505	0.22	ray::DefaultActor
21499	0.22	ray::DefaultActor
21508	0.22	ray::DefaultActor
21504	0.22	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:20,343 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.52	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.26	ray::DefaultActor
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.22	ray::DefaultActor
21505	0.22	ray::DefaultActor
21499	0.22	ray::DefaultActor
21508	0.22	ray::DefaultActor
21504	0.22	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:20,343 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.19	ray::IDLE
21516	0.19	ray::IDLE
21507	0.19	ray::IDLE
21505	0.19	ray::IDLE
21504	0.19	ray::IDLE
21499	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:20,343 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.19	ray::IDLE
21516	0.19	ray::IDLE
21507	0.19	ray::IDLE
21505	0.19	ray::IDLE
21504	0.19	ray::IDLE
21499	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:20,345 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.17	ray::IDLE
21512	0.17	ray::IDLE
21507	0.17	ray::IDLE
21505	0.17	ray::IDLE
21511	0.17	ray::IDLE
21504	0.17	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:20,345 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.17	ray::IDLE
21512	0.17	ray::IDLE
21507	0.17	ray::IDLE
21505	0.17	ray::IDLE
21511	0.17	ray::IDLE
21504	0.17	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:20,349 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.72	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.27	ray::DefaultActor.run
21512	0.24	ray::DefaultActor.run
21515	0.24	ray::DefaultActor.run
21508	0.23	ray::DefaultActor.run
21509	0.23	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21507	0.22	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:20,349 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.72	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.27	ray::DefaultActor.run
21512	0.24	ray::DefaultActor.run
21515	0.24	ray::DefaultActor.run
21508	0.23	ray::DefaultActor.run
21509	0.23	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21507	0.22	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:20,349 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.29	ray::DefaultActor.run
21508	0.28	ray::DefaultActor.run
21507	0.27	ray::DefaultActor.run
21506	0.27	ray::DefaultActor.run
21512	0.27	ray::DefaultActor.run
21505	0.27	ray::DefaultActor.run
21504	0.26	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:20,349 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.29	ray::DefaultActor.run
21508	0.28	ray::DefaultActor.run
21507	0.27	ray::DefaultActor.run
21506	0.27	ray::DefaultActor.run
21512	0.27	ray::DefaultActor.run
21505	0.27	ray::DefaultActor.run
21504	0.26	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:20,349 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.52	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.26	ray::DefaultActor
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.22	ray::DefaultActor
21505	0.22	ray::DefaultActor
21499	0.22	ray::DefaultActor
21508	0.22	ray::DefaultActor
21504	0.22	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:20,349 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.52	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.26	ray::DefaultActor
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.22	ray::DefaultActor
21505	0.22	ray::DefaultActor
21499	0.22	ray::DefaultActor
21508	0.22	ray::DefaultActor
21504	0.22	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:20,355 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.17	ray::IDLE
21512	0.17	ray::IDLE
21507	0.17	ray::IDLE
21505	0.17	ray::IDLE
21511	0.17	ray::IDLE
21504	0.17	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:20,355 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.17	ray::IDLE
21512	0.17	ray::IDLE
21507	0.17	ray::IDLE
21505	0.17	ray::IDLE
21511	0.17	ray::IDLE
21504	0.17	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:20,356 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.75	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.31	ray::DefaultActor.run
21505	0.31	ray::DefaultActor.run
21512	0.31	ray::DefaultActor.run
21506	0.30	ray::DefaultActor.run
21516	0.30	ray::DefaultActor.run
21504	0.30	ray::DefaultActor.run
21503	0.29	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:20,356 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.75	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.31	ray::DefaultActor.run
21505	0.31	ray::DefaultActor.run
21512	0.31	ray::DefaultActor.run
21506	0.30	ray::DefaultActor.run
21516	0.30	ray::DefaultActor.run
21504	0.30	ray::DefaultActor.run
21503	0.29	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:20,358 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.17	ray::IDLE
21512	0.17	ray::IDLE
21507	0.17	ray::IDLE
21505	0.17	ray::IDLE
21511	0.17	ray::IDLE
21504	0.17	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:20,358 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.17	ray::IDLE
21512	0.17	ray::IDLE
21507	0.17	ray::IDLE
21505	0.17	ray::IDLE
21511	0.17	ray::IDLE
21504	0.17	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:20,359 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.72	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.27	ray::DefaultActor.run
21512	0.24	ray::DefaultActor.run
21515	0.24	ray::DefaultActor.run
21508	0.23	ray::DefaultActor.run
21509	0.23	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21507	0.22	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:20,359 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.72	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.27	ray::DefaultActor.run
21512	0.24	ray::DefaultActor.run
21515	0.24	ray::DefaultActor.run
21508	0.23	ray::DefaultActor.run
21509	0.23	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21507	0.22	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:20,359 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.89	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.36	ray::DefaultActor.run
21512	0.35	ray::DefaultActor.run
21508	0.35	ray::DefaultActor
21505	0.35	ray::DefaultActor
21516	0.34	ray::DefaultActor
21503	0.34	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:20,359 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.89	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.36	ray::DefaultActor.run
21512	0.35	ray::DefaultActor.run
21508	0.35	ray::DefaultActor
21505	0.35	ray::DefaultActor
21516	0.34	ray::DefaultActor
21503	0.34	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:20,359 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.28	ray::DefaultActor.run
21515	0.26	ray::DefaultActor.run
21508	0.26	ray::DefaultActor.run
21512	0.26	ray::DefaultActor.run
21507	0.24	ray::DefaultActor.run
21504	0.24	ray::DefaultActor.run
21505	0.24	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:20,359 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.28	ray::DefaultActor.run
21515	0.26	ray::DefaultActor.run
21508	0.26	ray::DefaultActor.run
21512	0.26	ray::DefaultActor.run
21507	0.24	ray::DefaultActor.run
21504	0.24	ray::DefaultActor.run
21505	0.24	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:20,359 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.16	ray::IDLE
21516	0.16	ray::IDLE
21511	0.16	ray::IDLE
21507	0.16	ray::IDLE
21504	0.16	ray::IDLE
21505	0.16	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:20,361 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.29	ray::DefaultActor.run
21508	0.28	ray::DefaultActor.run
21507	0.27	ray::DefaultActor.run
21506	0.27	ray::DefaultActor.run
21512	0.27	ray::DefaultActor.run
21505	0.27	ray::DefaultActor.run
21504	0.26	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:20,359 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.16	ray::IDLE
21516	0.16	ray::IDLE
21511	0.16	ray::IDLE
21507	0.16	ray::IDLE
21504	0.16	ray::IDLE
21505	0.16	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:20,361 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.29	ray::DefaultActor.run
21508	0.28	ray::DefaultActor.run
21507	0.27	ray::DefaultActor.run
21506	0.27	ray::DefaultActor.run
21512	0.27	ray::DefaultActor.run
21505	0.27	ray::DefaultActor.run
21504	0.26	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:20,363 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.21	ray::IDLE
21512	0.21	ray::IDLE
21507	0.21	ray::IDLE
21505	0.21	ray::IDLE
21504	0.21	ray::IDLE
21515	0.21	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:20,363 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.21	ray::IDLE
21512	0.21	ray::IDLE
21507	0.21	ray::IDLE
21505	0.21	ray::IDLE
21504	0.21	ray::IDLE
21515	0.21	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:20,366 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.17	ray::IDLE
21512	0.17	ray::IDLE
21507	0.17	ray::IDLE
21505	0.17	ray::IDLE
21511	0.17	ray::IDLE
21504	0.17	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:20,366 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.17	ray::IDLE
21512	0.17	ray::IDLE
21507	0.17	ray::IDLE
21505	0.17	ray::IDLE
21511	0.17	ray::IDLE
21504	0.17	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:20,369 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.75	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.31	ray::DefaultActor.run
21505	0.31	ray::DefaultActor.run
21512	0.31	ray::DefaultActor.run
21506	0.30	ray::DefaultActor.run
21516	0.30	ray::DefaultActor.run
21504	0.30	ray::DefaultActor.run
21503	0.29	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:20,369 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.75	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.31	ray::DefaultActor.run
21505	0.31	ray::DefaultActor.run
21512	0.31	ray::DefaultActor.run
21506	0.30	ray::DefaultActor.run
21516	0.30	ray::DefaultActor.run
21504	0.30	ray::DefaultActor.run
21503	0.29	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:20,374 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.89	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.36	ray::DefaultActor.run
21512	0.35	ray::DefaultActor.run
21508	0.35	ray::DefaultActor
21505	0.35	ray::DefaultActor
21516	0.34	ray::DefaultActor
21503	0.34	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:20,374 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.89	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.36	ray::DefaultActor.run
21512	0.35	ray::DefaultActor.run
21508	0.35	ray::DefaultActor
21505	0.35	ray::DefaultActor
21516	0.34	ray::DefaultActor
21503	0.34	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:20,377 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.28	ray::DefaultActor.run
21515	0.26	ray::DefaultActor.run
21508	0.26	ray::DefaultActor.run
21512	0.26	ray::DefaultActor.run
21507	0.24	ray::DefaultActor.run
21504	0.24	ray::DefaultActor.run
21505	0.24	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:20,377 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.28	ray::DefaultActor.run
21515	0.26	ray::DefaultActor.run
21508	0.26	ray::DefaultActor.run
21512	0.26	ray::DefaultActor.run
21507	0.24	ray::DefaultActor.run
21504	0.24	ray::DefaultActor.run
21505	0.24	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:20,378 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.16	ray::IDLE
21516	0.16	ray::IDLE
21511	0.16	ray::IDLE
21507	0.16	ray::IDLE
21504	0.16	ray::IDLE
21505	0.16	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:20,378 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.16	ray::IDLE
21516	0.16	ray::IDLE
21511	0.16	ray::IDLE
21507	0.16	ray::IDLE
21504	0.16	ray::IDLE
21505	0.16	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:20,382 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.21	ray::IDLE
21512	0.21	ray::IDLE
21507	0.21	ray::IDLE
21505	0.21	ray::IDLE
21504	0.21	ray::IDLE
21515	0.21	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:20,382 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.21	ray::IDLE
21512	0.21	ray::IDLE
21507	0.21	ray::IDLE
21505	0.21	ray::IDLE
21504	0.21	ray::IDLE
21515	0.21	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:20,539 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.99	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.38	ray::DefaultActor.run
21505	0.38	ray::DefaultActor.run
21506	0.37	ray::DefaultActor.run
21512	0.37	ray::DefaultActor.run
21516	0.37	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21025	0.08	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:20,539 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.99	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.38	ray::DefaultActor.run
21505	0.38	ray::DefaultActor.run
21506	0.37	ray::DefaultActor.run
21512	0.37	ray::DefaultActor.run
21516	0.37	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21025	0.08	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:20,541 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.99	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.38	ray::DefaultActor.run
21505	0.38	ray::DefaultActor.run
21506	0.37	ray::DefaultActor.run
21512	0.37	ray::DefaultActor.run
21516	0.37	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21025	0.08	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:20,541 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.99	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.38	ray::DefaultActor.run
21505	0.38	ray::DefaultActor.run
21506	0.37	ray::DefaultActor.run
21512	0.37	ray::DefaultActor.run
21516	0.37	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21025	0.08	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | DEBUG flwr 2024-03-05 15:03:28,302 | server.py:236 | fit_round 3 received 4 results and 15 failures
myFlowerExperiment | DEBUG flwr 2024-03-05 15:03:28,302 | server.py:236 | fit_round 3 received 4 results and 15 failures
myFlowerExperiment | INFO flwr 2024-03-05 15:03:33,766 | server.py:125 | fit progress: (3, 0.7837085127830505, {'accuracy': 0.46832364797592163}, 47.5152722319981)
myFlowerExperiment | INFO flwr 2024-03-05 15:03:33,766 | server.py:125 | fit progress: (3, 0.7837085127830505, {'accuracy': 0.46832364797592163}, 47.5152722319981)
myFlowerExperiment | DEBUG flwr 2024-03-05 15:03:33,768 | server.py:173 | evaluate_round 3: strategy sampled 19 clients (out of 19)
myFlowerExperiment | DEBUG flwr 2024-03-05 15:03:33,768 | server.py:173 | evaluate_round 3: strategy sampled 19 clients (out of 19)
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:33,977 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.99	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.38	ray::DefaultActor.run
21505	0.38	ray::DefaultActor.run
21506	0.37	ray::DefaultActor.run
21512	0.37	ray::DefaultActor.run
21516	0.37	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21025	0.08	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:33,977 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.99	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.38	ray::DefaultActor.run
21505	0.38	ray::DefaultActor.run
21506	0.37	ray::DefaultActor.run
21512	0.37	ray::DefaultActor.run
21516	0.37	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21025	0.08	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:33,987 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.99	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.38	ray::DefaultActor.run
21505	0.38	ray::DefaultActor.run
21506	0.37	ray::DefaultActor.run
21512	0.37	ray::DefaultActor.run
21516	0.37	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21025	0.08	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:33,987 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.99	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.38	ray::DefaultActor.run
21505	0.38	ray::DefaultActor.run
21506	0.37	ray::DefaultActor.run
21512	0.37	ray::DefaultActor.run
21516	0.37	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21025	0.08	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:33,995 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.21	ray::IDLE
21512	0.21	ray::IDLE
21507	0.21	ray::IDLE
21505	0.21	ray::IDLE
21504	0.21	ray::IDLE
21515	0.21	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:33,995 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.21	ray::IDLE
21512	0.21	ray::IDLE
21507	0.21	ray::IDLE
21505	0.21	ray::IDLE
21504	0.21	ray::IDLE
21515	0.21	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:33,997 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.89	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.36	ray::DefaultActor.run
21512	0.35	ray::DefaultActor.run
21508	0.35	ray::DefaultActor
21505	0.35	ray::DefaultActor
21516	0.34	ray::DefaultActor
21503	0.34	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:33,997 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.89	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.36	ray::DefaultActor.run
21512	0.35	ray::DefaultActor.run
21508	0.35	ray::DefaultActor
21505	0.35	ray::DefaultActor
21516	0.34	ray::DefaultActor
21503	0.34	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:33,999 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.21	ray::IDLE
21512	0.21	ray::IDLE
21507	0.21	ray::IDLE
21505	0.21	ray::IDLE
21504	0.21	ray::IDLE
21515	0.21	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:33,999 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.21	ray::IDLE
21512	0.21	ray::IDLE
21507	0.21	ray::IDLE
21505	0.21	ray::IDLE
21504	0.21	ray::IDLE
21515	0.21	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:34,000 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.75	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.31	ray::DefaultActor.run
21505	0.31	ray::DefaultActor.run
21512	0.31	ray::DefaultActor.run
21506	0.30	ray::DefaultActor.run
21516	0.30	ray::DefaultActor.run
21504	0.30	ray::DefaultActor.run
21503	0.29	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:34,000 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.75	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.31	ray::DefaultActor.run
21505	0.31	ray::DefaultActor.run
21512	0.31	ray::DefaultActor.run
21506	0.30	ray::DefaultActor.run
21516	0.30	ray::DefaultActor.run
21504	0.30	ray::DefaultActor.run
21503	0.29	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:34,001 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.16	ray::IDLE
21516	0.16	ray::IDLE
21511	0.16	ray::IDLE
21507	0.16	ray::IDLE
21504	0.16	ray::IDLE
21505	0.16	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:34,001 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.16	ray::IDLE
21516	0.16	ray::IDLE
21511	0.16	ray::IDLE
21507	0.16	ray::IDLE
21504	0.16	ray::IDLE
21505	0.16	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:34,001 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.89	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.36	ray::DefaultActor.run
21512	0.35	ray::DefaultActor.run
21508	0.35	ray::DefaultActor
21505	0.35	ray::DefaultActor
21516	0.34	ray::DefaultActor
21503	0.34	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:34,001 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.89	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.36	ray::DefaultActor.run
21512	0.35	ray::DefaultActor.run
21508	0.35	ray::DefaultActor
21505	0.35	ray::DefaultActor
21516	0.34	ray::DefaultActor
21503	0.34	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:34,001 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.52	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.26	ray::DefaultActor
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.22	ray::DefaultActor
21505	0.22	ray::DefaultActor
21499	0.22	ray::DefaultActor
21508	0.22	ray::DefaultActor
21504	0.22	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:34,001 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.52	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.26	ray::DefaultActor
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.22	ray::DefaultActor
21505	0.22	ray::DefaultActor
21499	0.22	ray::DefaultActor
21508	0.22	ray::DefaultActor
21504	0.22	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:34,003 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.19	ray::IDLE
21516	0.19	ray::IDLE
21507	0.19	ray::IDLE
21505	0.19	ray::IDLE
21504	0.19	ray::IDLE
21499	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:34,003 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.19	ray::IDLE
21516	0.19	ray::IDLE
21507	0.19	ray::IDLE
21505	0.19	ray::IDLE
21504	0.19	ray::IDLE
21499	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:34,004 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.72	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.27	ray::DefaultActor.run
21512	0.24	ray::DefaultActor.run
21515	0.24	ray::DefaultActor.run
21508	0.23	ray::DefaultActor.run
21509	0.23	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21507	0.22	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:34,004 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.72	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.27	ray::DefaultActor.run
21512	0.24	ray::DefaultActor.run
21515	0.24	ray::DefaultActor.run
21508	0.23	ray::DefaultActor.run
21509	0.23	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21507	0.22	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:34,004 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.18	ray::IDLE
21516	0.18	ray::IDLE
21505	0.18	ray::IDLE
21507	0.18	ray::IDLE
21499	0.18	ray::IDLE
21504	0.18	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:34,004 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.18	ray::IDLE
21516	0.18	ray::IDLE
21505	0.18	ray::IDLE
21507	0.18	ray::IDLE
21499	0.18	ray::IDLE
21504	0.18	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:34,005 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.75	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.31	ray::DefaultActor.run
21505	0.31	ray::DefaultActor.run
21512	0.31	ray::DefaultActor.run
21506	0.30	ray::DefaultActor.run
21516	0.30	ray::DefaultActor.run
21504	0.30	ray::DefaultActor.run
21503	0.29	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:34,005 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.75	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.31	ray::DefaultActor.run
21505	0.31	ray::DefaultActor.run
21512	0.31	ray::DefaultActor.run
21506	0.30	ray::DefaultActor.run
21516	0.30	ray::DefaultActor.run
21504	0.30	ray::DefaultActor.run
21503	0.29	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:34,006 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.29	ray::DefaultActor.run
21508	0.28	ray::DefaultActor.run
21507	0.27	ray::DefaultActor.run
21506	0.27	ray::DefaultActor.run
21512	0.27	ray::DefaultActor.run
21505	0.27	ray::DefaultActor.run
21504	0.26	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:34,006 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.29	ray::DefaultActor.run
21508	0.28	ray::DefaultActor.run
21507	0.27	ray::DefaultActor.run
21506	0.27	ray::DefaultActor.run
21512	0.27	ray::DefaultActor.run
21505	0.27	ray::DefaultActor.run
21504	0.26	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:34,007 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.99	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.38	ray::DefaultActor.run
21505	0.38	ray::DefaultActor.run
21506	0.37	ray::DefaultActor.run
21512	0.37	ray::DefaultActor.run
21516	0.37	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21025	0.08	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:34,007 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.99	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.38	ray::DefaultActor.run
21505	0.38	ray::DefaultActor.run
21506	0.37	ray::DefaultActor.run
21512	0.37	ray::DefaultActor.run
21516	0.37	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21025	0.08	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:34,007 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.28	ray::DefaultActor.run
21515	0.26	ray::DefaultActor.run
21508	0.26	ray::DefaultActor.run
21512	0.26	ray::DefaultActor.run
21507	0.24	ray::DefaultActor.run
21504	0.24	ray::DefaultActor.run
21505	0.24	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:34,007 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.28	ray::DefaultActor.run
21515	0.26	ray::DefaultActor.run
21508	0.26	ray::DefaultActor.run
21512	0.26	ray::DefaultActor.run
21507	0.24	ray::DefaultActor.run
21504	0.24	ray::DefaultActor.run
21505	0.24	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:34,008 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.16	ray::IDLE
21516	0.16	ray::IDLE
21511	0.16	ray::IDLE
21507	0.16	ray::IDLE
21504	0.16	ray::IDLE
21505	0.16	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:34,008 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.16	ray::IDLE
21516	0.16	ray::IDLE
21511	0.16	ray::IDLE
21507	0.16	ray::IDLE
21504	0.16	ray::IDLE
21505	0.16	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:34,011 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.52	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.26	ray::DefaultActor
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.22	ray::DefaultActor
21505	0.22	ray::DefaultActor
21499	0.22	ray::DefaultActor
21508	0.22	ray::DefaultActor
21504	0.22	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:34,013 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.17	ray::IDLE
21512	0.17	ray::IDLE
21507	0.17	ray::IDLE
21505	0.17	ray::IDLE
21511	0.17	ray::IDLE
21504	0.17	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:34,011 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.52	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.26	ray::DefaultActor
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.22	ray::DefaultActor
21505	0.22	ray::DefaultActor
21499	0.22	ray::DefaultActor
21508	0.22	ray::DefaultActor
21504	0.22	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:34,016 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.19	ray::IDLE
21516	0.19	ray::IDLE
21507	0.19	ray::IDLE
21505	0.19	ray::IDLE
21504	0.19	ray::IDLE
21499	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:34,013 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.17	ray::IDLE
21512	0.17	ray::IDLE
21507	0.17	ray::IDLE
21505	0.17	ray::IDLE
21511	0.17	ray::IDLE
21504	0.17	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:34,018 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.72	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.27	ray::DefaultActor.run
21512	0.24	ray::DefaultActor.run
21515	0.24	ray::DefaultActor.run
21508	0.23	ray::DefaultActor.run
21509	0.23	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21507	0.22	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:34,016 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.19	ray::IDLE
21516	0.19	ray::IDLE
21507	0.19	ray::IDLE
21505	0.19	ray::IDLE
21504	0.19	ray::IDLE
21499	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:34,018 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.18	ray::IDLE
21516	0.18	ray::IDLE
21505	0.18	ray::IDLE
21507	0.18	ray::IDLE
21499	0.18	ray::IDLE
21504	0.18	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:34,018 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.72	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.27	ray::DefaultActor.run
21512	0.24	ray::DefaultActor.run
21515	0.24	ray::DefaultActor.run
21508	0.23	ray::DefaultActor.run
21509	0.23	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21507	0.22	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:34,018 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.18	ray::IDLE
21516	0.18	ray::IDLE
21505	0.18	ray::IDLE
21507	0.18	ray::IDLE
21499	0.18	ray::IDLE
21504	0.18	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:34,023 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.29	ray::DefaultActor.run
21508	0.28	ray::DefaultActor.run
21507	0.27	ray::DefaultActor.run
21506	0.27	ray::DefaultActor.run
21512	0.27	ray::DefaultActor.run
21505	0.27	ray::DefaultActor.run
21504	0.26	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:34,023 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.29	ray::DefaultActor.run
21508	0.28	ray::DefaultActor.run
21507	0.27	ray::DefaultActor.run
21506	0.27	ray::DefaultActor.run
21512	0.27	ray::DefaultActor.run
21505	0.27	ray::DefaultActor.run
21504	0.26	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:34,024 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.99	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.38	ray::DefaultActor.run
21505	0.38	ray::DefaultActor.run
21506	0.37	ray::DefaultActor.run
21512	0.37	ray::DefaultActor.run
21516	0.37	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21025	0.08	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:34,024 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.99	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.38	ray::DefaultActor.run
21505	0.38	ray::DefaultActor.run
21506	0.37	ray::DefaultActor.run
21512	0.37	ray::DefaultActor.run
21516	0.37	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21025	0.08	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:34,024 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.28	ray::DefaultActor.run
21515	0.26	ray::DefaultActor.run
21508	0.26	ray::DefaultActor.run
21512	0.26	ray::DefaultActor.run
21507	0.24	ray::DefaultActor.run
21504	0.24	ray::DefaultActor.run
21505	0.24	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:34,024 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.28	ray::DefaultActor.run
21515	0.26	ray::DefaultActor.run
21508	0.26	ray::DefaultActor.run
21512	0.26	ray::DefaultActor.run
21507	0.24	ray::DefaultActor.run
21504	0.24	ray::DefaultActor.run
21505	0.24	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:34,028 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.17	ray::IDLE
21512	0.17	ray::IDLE
21507	0.17	ray::IDLE
21505	0.17	ray::IDLE
21511	0.17	ray::IDLE
21504	0.17	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:34,028 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.17	ray::IDLE
21512	0.17	ray::IDLE
21507	0.17	ray::IDLE
21505	0.17	ray::IDLE
21511	0.17	ray::IDLE
21504	0.17	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | DEBUG flwr 2024-03-05 15:03:34,035 | server.py:187 | evaluate_round 3 received 0 results and 19 failures
myFlowerExperiment | DEBUG flwr 2024-03-05 15:03:34,035 | server.py:187 | evaluate_round 3 received 0 results and 19 failures
myFlowerExperiment | DEBUG flwr 2024-03-05 15:03:34,036 | server.py:222 | fit_round 4: strategy sampled 19 clients (out of 19)
myFlowerExperiment | DEBUG flwr 2024-03-05 15:03:34,036 | server.py:222 | fit_round 4: strategy sampled 19 clients (out of 19)
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:34,244 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.17	ray::IDLE
21512	0.17	ray::IDLE
21507	0.17	ray::IDLE
21505	0.17	ray::IDLE
21511	0.17	ray::IDLE
21504	0.17	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:34,244 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.17	ray::IDLE
21512	0.17	ray::IDLE
21507	0.17	ray::IDLE
21505	0.17	ray::IDLE
21511	0.17	ray::IDLE
21504	0.17	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:34,259 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.17	ray::IDLE
21512	0.17	ray::IDLE
21507	0.17	ray::IDLE
21505	0.17	ray::IDLE
21511	0.17	ray::IDLE
21504	0.17	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:34,259 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.17	ray::IDLE
21512	0.17	ray::IDLE
21507	0.17	ray::IDLE
21505	0.17	ray::IDLE
21511	0.17	ray::IDLE
21504	0.17	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:34,262 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.28	ray::DefaultActor.run
21515	0.26	ray::DefaultActor.run
21508	0.26	ray::DefaultActor.run
21512	0.26	ray::DefaultActor.run
21507	0.24	ray::DefaultActor.run
21504	0.24	ray::DefaultActor.run
21505	0.24	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:34,262 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.28	ray::DefaultActor.run
21515	0.26	ray::DefaultActor.run
21508	0.26	ray::DefaultActor.run
21512	0.26	ray::DefaultActor.run
21507	0.24	ray::DefaultActor.run
21504	0.24	ray::DefaultActor.run
21505	0.24	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:34,276 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.28	ray::DefaultActor.run
21515	0.26	ray::DefaultActor.run
21508	0.26	ray::DefaultActor.run
21512	0.26	ray::DefaultActor.run
21507	0.24	ray::DefaultActor.run
21504	0.24	ray::DefaultActor.run
21505	0.24	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:34,276 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.28	ray::DefaultActor.run
21515	0.26	ray::DefaultActor.run
21508	0.26	ray::DefaultActor.run
21512	0.26	ray::DefaultActor.run
21507	0.24	ray::DefaultActor.run
21504	0.24	ray::DefaultActor.run
21505	0.24	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:34,280 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.72	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.27	ray::DefaultActor.run
21512	0.24	ray::DefaultActor.run
21515	0.24	ray::DefaultActor.run
21508	0.23	ray::DefaultActor.run
21509	0.23	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21507	0.22	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:34,280 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.72	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.27	ray::DefaultActor.run
21512	0.24	ray::DefaultActor.run
21515	0.24	ray::DefaultActor.run
21508	0.23	ray::DefaultActor.run
21509	0.23	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21507	0.22	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:34,283 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.72	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.27	ray::DefaultActor.run
21512	0.24	ray::DefaultActor.run
21515	0.24	ray::DefaultActor.run
21508	0.23	ray::DefaultActor.run
21509	0.23	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21507	0.22	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:34,283 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.72	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.27	ray::DefaultActor.run
21512	0.24	ray::DefaultActor.run
21515	0.24	ray::DefaultActor.run
21508	0.23	ray::DefaultActor.run
21509	0.23	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21507	0.22	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:34,285 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.21	ray::IDLE
21512	0.21	ray::IDLE
21507	0.21	ray::IDLE
21505	0.21	ray::IDLE
21504	0.21	ray::IDLE
21515	0.21	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:34,285 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.21	ray::IDLE
21512	0.21	ray::IDLE
21507	0.21	ray::IDLE
21505	0.21	ray::IDLE
21504	0.21	ray::IDLE
21515	0.21	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:34,288 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.21	ray::IDLE
21512	0.21	ray::IDLE
21507	0.21	ray::IDLE
21505	0.21	ray::IDLE
21504	0.21	ray::IDLE
21515	0.21	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:34,288 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.21	ray::IDLE
21512	0.21	ray::IDLE
21507	0.21	ray::IDLE
21505	0.21	ray::IDLE
21504	0.21	ray::IDLE
21515	0.21	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:34,290 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.19	ray::IDLE
21516	0.19	ray::IDLE
21507	0.19	ray::IDLE
21505	0.19	ray::IDLE
21504	0.19	ray::IDLE
21499	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:34,290 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.19	ray::IDLE
21516	0.19	ray::IDLE
21507	0.19	ray::IDLE
21505	0.19	ray::IDLE
21504	0.19	ray::IDLE
21499	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:34,292 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.52	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.26	ray::DefaultActor
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.22	ray::DefaultActor
21505	0.22	ray::DefaultActor
21499	0.22	ray::DefaultActor
21508	0.22	ray::DefaultActor
21504	0.22	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:34,292 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.52	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.26	ray::DefaultActor
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.22	ray::DefaultActor
21505	0.22	ray::DefaultActor
21499	0.22	ray::DefaultActor
21508	0.22	ray::DefaultActor
21504	0.22	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:34,296 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.19	ray::IDLE
21516	0.19	ray::IDLE
21507	0.19	ray::IDLE
21505	0.19	ray::IDLE
21504	0.19	ray::IDLE
21499	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:34,296 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.19	ray::IDLE
21516	0.19	ray::IDLE
21507	0.19	ray::IDLE
21505	0.19	ray::IDLE
21504	0.19	ray::IDLE
21499	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:34,296 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.52	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.26	ray::DefaultActor
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.22	ray::DefaultActor
21505	0.22	ray::DefaultActor
21499	0.22	ray::DefaultActor
21508	0.22	ray::DefaultActor
21504	0.22	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:34,296 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.52	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.26	ray::DefaultActor
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.22	ray::DefaultActor
21505	0.22	ray::DefaultActor
21499	0.22	ray::DefaultActor
21508	0.22	ray::DefaultActor
21504	0.22	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:34,298 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.89	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.36	ray::DefaultActor.run
21512	0.35	ray::DefaultActor.run
21508	0.35	ray::DefaultActor
21505	0.35	ray::DefaultActor
21516	0.34	ray::DefaultActor
21503	0.34	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:34,298 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.89	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.36	ray::DefaultActor.run
21512	0.35	ray::DefaultActor.run
21508	0.35	ray::DefaultActor
21505	0.35	ray::DefaultActor
21516	0.34	ray::DefaultActor
21503	0.34	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:34,302 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.17	ray::IDLE
21512	0.17	ray::IDLE
21507	0.17	ray::IDLE
21505	0.17	ray::IDLE
21511	0.17	ray::IDLE
21504	0.17	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:34,302 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.17	ray::IDLE
21512	0.17	ray::IDLE
21507	0.17	ray::IDLE
21505	0.17	ray::IDLE
21511	0.17	ray::IDLE
21504	0.17	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:34,302 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.16	ray::IDLE
21516	0.16	ray::IDLE
21511	0.16	ray::IDLE
21507	0.16	ray::IDLE
21504	0.16	ray::IDLE
21505	0.16	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:34,302 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.16	ray::IDLE
21516	0.16	ray::IDLE
21511	0.16	ray::IDLE
21507	0.16	ray::IDLE
21504	0.16	ray::IDLE
21505	0.16	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:34,305 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.18	ray::IDLE
21516	0.18	ray::IDLE
21505	0.18	ray::IDLE
21507	0.18	ray::IDLE
21499	0.18	ray::IDLE
21504	0.18	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:34,305 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.18	ray::IDLE
21516	0.18	ray::IDLE
21505	0.18	ray::IDLE
21507	0.18	ray::IDLE
21499	0.18	ray::IDLE
21504	0.18	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:34,305 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.19	ray::IDLE
21516	0.19	ray::IDLE
21507	0.19	ray::IDLE
21505	0.19	ray::IDLE
21504	0.19	ray::IDLE
21499	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:34,305 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.19	ray::IDLE
21516	0.19	ray::IDLE
21507	0.19	ray::IDLE
21505	0.19	ray::IDLE
21504	0.19	ray::IDLE
21499	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:34,307 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.89	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.36	ray::DefaultActor.run
21512	0.35	ray::DefaultActor.run
21508	0.35	ray::DefaultActor
21505	0.35	ray::DefaultActor
21516	0.34	ray::DefaultActor
21503	0.34	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:34,307 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.89	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.36	ray::DefaultActor.run
21512	0.35	ray::DefaultActor.run
21508	0.35	ray::DefaultActor
21505	0.35	ray::DefaultActor
21516	0.34	ray::DefaultActor
21503	0.34	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:34,309 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.99	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.38	ray::DefaultActor.run
21505	0.38	ray::DefaultActor.run
21506	0.37	ray::DefaultActor.run
21512	0.37	ray::DefaultActor.run
21516	0.37	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21025	0.08	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:34,309 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.99	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.38	ray::DefaultActor.run
21505	0.38	ray::DefaultActor.run
21506	0.37	ray::DefaultActor.run
21512	0.37	ray::DefaultActor.run
21516	0.37	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21025	0.08	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:34,310 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.75	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.31	ray::DefaultActor.run
21505	0.31	ray::DefaultActor.run
21512	0.31	ray::DefaultActor.run
21506	0.30	ray::DefaultActor.run
21516	0.30	ray::DefaultActor.run
21504	0.30	ray::DefaultActor.run
21503	0.29	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:34,310 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.75	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.31	ray::DefaultActor.run
21505	0.31	ray::DefaultActor.run
21512	0.31	ray::DefaultActor.run
21506	0.30	ray::DefaultActor.run
21516	0.30	ray::DefaultActor.run
21504	0.30	ray::DefaultActor.run
21503	0.29	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:34,312 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.17	ray::IDLE
21512	0.17	ray::IDLE
21507	0.17	ray::IDLE
21505	0.17	ray::IDLE
21511	0.17	ray::IDLE
21504	0.17	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:34,312 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.17	ray::IDLE
21512	0.17	ray::IDLE
21507	0.17	ray::IDLE
21505	0.17	ray::IDLE
21511	0.17	ray::IDLE
21504	0.17	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:34,312 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.29	ray::DefaultActor.run
21508	0.28	ray::DefaultActor.run
21507	0.27	ray::DefaultActor.run
21506	0.27	ray::DefaultActor.run
21512	0.27	ray::DefaultActor.run
21505	0.27	ray::DefaultActor.run
21504	0.26	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:34,312 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.29	ray::DefaultActor.run
21508	0.28	ray::DefaultActor.run
21507	0.27	ray::DefaultActor.run
21506	0.27	ray::DefaultActor.run
21512	0.27	ray::DefaultActor.run
21505	0.27	ray::DefaultActor.run
21504	0.26	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:34,313 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.16	ray::IDLE
21516	0.16	ray::IDLE
21511	0.16	ray::IDLE
21507	0.16	ray::IDLE
21504	0.16	ray::IDLE
21505	0.16	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:34,313 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.16	ray::IDLE
21516	0.16	ray::IDLE
21511	0.16	ray::IDLE
21507	0.16	ray::IDLE
21504	0.16	ray::IDLE
21505	0.16	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:34,314 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.18	ray::IDLE
21516	0.18	ray::IDLE
21505	0.18	ray::IDLE
21507	0.18	ray::IDLE
21499	0.18	ray::IDLE
21504	0.18	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:34,314 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.18	ray::IDLE
21516	0.18	ray::IDLE
21505	0.18	ray::IDLE
21507	0.18	ray::IDLE
21499	0.18	ray::IDLE
21504	0.18	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:34,315 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.28	ray::DefaultActor.run
21515	0.26	ray::DefaultActor.run
21508	0.26	ray::DefaultActor.run
21512	0.26	ray::DefaultActor.run
21507	0.24	ray::DefaultActor.run
21504	0.24	ray::DefaultActor.run
21505	0.24	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:34,315 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.28	ray::DefaultActor.run
21515	0.26	ray::DefaultActor.run
21508	0.26	ray::DefaultActor.run
21512	0.26	ray::DefaultActor.run
21507	0.24	ray::DefaultActor.run
21504	0.24	ray::DefaultActor.run
21505	0.24	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:34,317 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.19	ray::IDLE
21516	0.19	ray::IDLE
21507	0.19	ray::IDLE
21505	0.19	ray::IDLE
21504	0.19	ray::IDLE
21499	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:34,317 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.19	ray::IDLE
21516	0.19	ray::IDLE
21507	0.19	ray::IDLE
21505	0.19	ray::IDLE
21504	0.19	ray::IDLE
21499	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:34,321 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.99	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.38	ray::DefaultActor.run
21505	0.38	ray::DefaultActor.run
21506	0.37	ray::DefaultActor.run
21512	0.37	ray::DefaultActor.run
21516	0.37	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21025	0.08	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:34,321 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.99	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.38	ray::DefaultActor.run
21505	0.38	ray::DefaultActor.run
21506	0.37	ray::DefaultActor.run
21512	0.37	ray::DefaultActor.run
21516	0.37	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21025	0.08	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:34,322 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.75	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.31	ray::DefaultActor.run
21505	0.31	ray::DefaultActor.run
21512	0.31	ray::DefaultActor.run
21506	0.30	ray::DefaultActor.run
21516	0.30	ray::DefaultActor.run
21504	0.30	ray::DefaultActor.run
21503	0.29	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:34,322 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.75	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.31	ray::DefaultActor.run
21505	0.31	ray::DefaultActor.run
21512	0.31	ray::DefaultActor.run
21506	0.30	ray::DefaultActor.run
21516	0.30	ray::DefaultActor.run
21504	0.30	ray::DefaultActor.run
21503	0.29	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:34,325 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.29	ray::DefaultActor.run
21508	0.28	ray::DefaultActor.run
21507	0.27	ray::DefaultActor.run
21506	0.27	ray::DefaultActor.run
21512	0.27	ray::DefaultActor.run
21505	0.27	ray::DefaultActor.run
21504	0.26	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:34,325 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.29	ray::DefaultActor.run
21508	0.28	ray::DefaultActor.run
21507	0.27	ray::DefaultActor.run
21506	0.27	ray::DefaultActor.run
21512	0.27	ray::DefaultActor.run
21505	0.27	ray::DefaultActor.run
21504	0.26	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:34,334 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.28	ray::DefaultActor.run
21515	0.26	ray::DefaultActor.run
21508	0.26	ray::DefaultActor.run
21512	0.26	ray::DefaultActor.run
21507	0.24	ray::DefaultActor.run
21504	0.24	ray::DefaultActor.run
21505	0.24	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:34,334 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.28	ray::DefaultActor.run
21515	0.26	ray::DefaultActor.run
21508	0.26	ray::DefaultActor.run
21512	0.26	ray::DefaultActor.run
21507	0.24	ray::DefaultActor.run
21504	0.24	ray::DefaultActor.run
21505	0.24	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | DEBUG flwr 2024-03-05 15:03:42,258 | server.py:236 | fit_round 4 received 4 results and 15 failures
myFlowerExperiment | DEBUG flwr 2024-03-05 15:03:42,258 | server.py:236 | fit_round 4 received 4 results and 15 failures
myFlowerExperiment | INFO flwr 2024-03-05 15:03:47,778 | server.py:125 | fit progress: (4, 0.785237729549408, {'accuracy': 0.48932668566703796}, 61.527633637997496)
myFlowerExperiment | INFO flwr 2024-03-05 15:03:47,778 | server.py:125 | fit progress: (4, 0.785237729549408, {'accuracy': 0.48932668566703796}, 61.527633637997496)
myFlowerExperiment | DEBUG flwr 2024-03-05 15:03:47,780 | server.py:173 | evaluate_round 4: strategy sampled 19 clients (out of 19)
myFlowerExperiment | DEBUG flwr 2024-03-05 15:03:47,780 | server.py:173 | evaluate_round 4: strategy sampled 19 clients (out of 19)
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:47,989 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.28	ray::DefaultActor.run
21515	0.26	ray::DefaultActor.run
21508	0.26	ray::DefaultActor.run
21512	0.26	ray::DefaultActor.run
21507	0.24	ray::DefaultActor.run
21504	0.24	ray::DefaultActor.run
21505	0.24	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:47,989 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.28	ray::DefaultActor.run
21515	0.26	ray::DefaultActor.run
21508	0.26	ray::DefaultActor.run
21512	0.26	ray::DefaultActor.run
21507	0.24	ray::DefaultActor.run
21504	0.24	ray::DefaultActor.run
21505	0.24	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:47,999 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.28	ray::DefaultActor.run
21515	0.26	ray::DefaultActor.run
21508	0.26	ray::DefaultActor.run
21512	0.26	ray::DefaultActor.run
21507	0.24	ray::DefaultActor.run
21504	0.24	ray::DefaultActor.run
21505	0.24	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:47,999 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.28	ray::DefaultActor.run
21515	0.26	ray::DefaultActor.run
21508	0.26	ray::DefaultActor.run
21512	0.26	ray::DefaultActor.run
21507	0.24	ray::DefaultActor.run
21504	0.24	ray::DefaultActor.run
21505	0.24	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,005 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.29	ray::DefaultActor.run
21508	0.28	ray::DefaultActor.run
21507	0.27	ray::DefaultActor.run
21506	0.27	ray::DefaultActor.run
21512	0.27	ray::DefaultActor.run
21505	0.27	ray::DefaultActor.run
21504	0.26	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,005 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.29	ray::DefaultActor.run
21508	0.28	ray::DefaultActor.run
21507	0.27	ray::DefaultActor.run
21506	0.27	ray::DefaultActor.run
21512	0.27	ray::DefaultActor.run
21505	0.27	ray::DefaultActor.run
21504	0.26	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,007 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.75	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.31	ray::DefaultActor.run
21505	0.31	ray::DefaultActor.run
21512	0.31	ray::DefaultActor.run
21506	0.30	ray::DefaultActor.run
21516	0.30	ray::DefaultActor.run
21504	0.30	ray::DefaultActor.run
21503	0.29	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,007 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.75	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.31	ray::DefaultActor.run
21505	0.31	ray::DefaultActor.run
21512	0.31	ray::DefaultActor.run
21506	0.30	ray::DefaultActor.run
21516	0.30	ray::DefaultActor.run
21504	0.30	ray::DefaultActor.run
21503	0.29	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,008 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.99	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.38	ray::DefaultActor.run
21505	0.38	ray::DefaultActor.run
21506	0.37	ray::DefaultActor.run
21512	0.37	ray::DefaultActor.run
21516	0.37	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21025	0.08	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,008 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.99	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.38	ray::DefaultActor.run
21505	0.38	ray::DefaultActor.run
21506	0.37	ray::DefaultActor.run
21512	0.37	ray::DefaultActor.run
21516	0.37	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21025	0.08	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,009 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.29	ray::DefaultActor.run
21508	0.28	ray::DefaultActor.run
21507	0.27	ray::DefaultActor.run
21506	0.27	ray::DefaultActor.run
21512	0.27	ray::DefaultActor.run
21505	0.27	ray::DefaultActor.run
21504	0.26	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,009 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.29	ray::DefaultActor.run
21508	0.28	ray::DefaultActor.run
21507	0.27	ray::DefaultActor.run
21506	0.27	ray::DefaultActor.run
21512	0.27	ray::DefaultActor.run
21505	0.27	ray::DefaultActor.run
21504	0.26	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,011 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.75	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.31	ray::DefaultActor.run
21505	0.31	ray::DefaultActor.run
21512	0.31	ray::DefaultActor.run
21506	0.30	ray::DefaultActor.run
21516	0.30	ray::DefaultActor.run
21504	0.30	ray::DefaultActor.run
21503	0.29	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,011 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.75	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.31	ray::DefaultActor.run
21505	0.31	ray::DefaultActor.run
21512	0.31	ray::DefaultActor.run
21506	0.30	ray::DefaultActor.run
21516	0.30	ray::DefaultActor.run
21504	0.30	ray::DefaultActor.run
21503	0.29	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,012 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.99	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.38	ray::DefaultActor.run
21505	0.38	ray::DefaultActor.run
21506	0.37	ray::DefaultActor.run
21512	0.37	ray::DefaultActor.run
21516	0.37	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21025	0.08	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,012 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.99	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.38	ray::DefaultActor.run
21505	0.38	ray::DefaultActor.run
21506	0.37	ray::DefaultActor.run
21512	0.37	ray::DefaultActor.run
21516	0.37	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21025	0.08	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,015 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.21	ray::IDLE
21512	0.21	ray::IDLE
21507	0.21	ray::IDLE
21505	0.21	ray::IDLE
21504	0.21	ray::IDLE
21515	0.21	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,015 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.21	ray::IDLE
21512	0.21	ray::IDLE
21507	0.21	ray::IDLE
21505	0.21	ray::IDLE
21504	0.21	ray::IDLE
21515	0.21	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,015 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.52	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.26	ray::DefaultActor
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.22	ray::DefaultActor
21505	0.22	ray::DefaultActor
21499	0.22	ray::DefaultActor
21508	0.22	ray::DefaultActor
21504	0.22	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,015 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.52	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.26	ray::DefaultActor
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.22	ray::DefaultActor
21505	0.22	ray::DefaultActor
21499	0.22	ray::DefaultActor
21508	0.22	ray::DefaultActor
21504	0.22	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,018 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.21	ray::IDLE
21512	0.21	ray::IDLE
21507	0.21	ray::IDLE
21505	0.21	ray::IDLE
21504	0.21	ray::IDLE
21515	0.21	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,018 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.21	ray::IDLE
21512	0.21	ray::IDLE
21507	0.21	ray::IDLE
21505	0.21	ray::IDLE
21504	0.21	ray::IDLE
21515	0.21	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,019 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.52	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.26	ray::DefaultActor
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.22	ray::DefaultActor
21505	0.22	ray::DefaultActor
21499	0.22	ray::DefaultActor
21508	0.22	ray::DefaultActor
21504	0.22	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,019 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.52	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.26	ray::DefaultActor
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.22	ray::DefaultActor
21505	0.22	ray::DefaultActor
21499	0.22	ray::DefaultActor
21508	0.22	ray::DefaultActor
21504	0.22	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,021 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.19	ray::IDLE
21516	0.19	ray::IDLE
21507	0.19	ray::IDLE
21505	0.19	ray::IDLE
21504	0.19	ray::IDLE
21499	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,021 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.19	ray::IDLE
21516	0.19	ray::IDLE
21507	0.19	ray::IDLE
21505	0.19	ray::IDLE
21504	0.19	ray::IDLE
21499	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,023 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.89	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.36	ray::DefaultActor.run
21512	0.35	ray::DefaultActor.run
21508	0.35	ray::DefaultActor
21505	0.35	ray::DefaultActor
21516	0.34	ray::DefaultActor
21503	0.34	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,023 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.89	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.36	ray::DefaultActor.run
21512	0.35	ray::DefaultActor.run
21508	0.35	ray::DefaultActor
21505	0.35	ray::DefaultActor
21516	0.34	ray::DefaultActor
21503	0.34	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,024 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.16	ray::IDLE
21516	0.16	ray::IDLE
21511	0.16	ray::IDLE
21507	0.16	ray::IDLE
21504	0.16	ray::IDLE
21505	0.16	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,024 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.16	ray::IDLE
21516	0.16	ray::IDLE
21511	0.16	ray::IDLE
21507	0.16	ray::IDLE
21504	0.16	ray::IDLE
21505	0.16	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,025 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.72	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.27	ray::DefaultActor.run
21512	0.24	ray::DefaultActor.run
21515	0.24	ray::DefaultActor.run
21508	0.23	ray::DefaultActor.run
21509	0.23	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21507	0.22	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,025 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.72	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.27	ray::DefaultActor.run
21512	0.24	ray::DefaultActor.run
21515	0.24	ray::DefaultActor.run
21508	0.23	ray::DefaultActor.run
21509	0.23	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21507	0.22	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,026 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.19	ray::IDLE
21516	0.19	ray::IDLE
21507	0.19	ray::IDLE
21505	0.19	ray::IDLE
21504	0.19	ray::IDLE
21499	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,026 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.19	ray::IDLE
21516	0.19	ray::IDLE
21507	0.19	ray::IDLE
21505	0.19	ray::IDLE
21504	0.19	ray::IDLE
21499	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,029 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.89	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.36	ray::DefaultActor.run
21512	0.35	ray::DefaultActor.run
21508	0.35	ray::DefaultActor
21505	0.35	ray::DefaultActor
21516	0.34	ray::DefaultActor
21503	0.34	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,029 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.89	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.36	ray::DefaultActor.run
21512	0.35	ray::DefaultActor.run
21508	0.35	ray::DefaultActor
21505	0.35	ray::DefaultActor
21516	0.34	ray::DefaultActor
21503	0.34	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,029 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.18	ray::IDLE
21516	0.18	ray::IDLE
21505	0.18	ray::IDLE
21507	0.18	ray::IDLE
21499	0.18	ray::IDLE
21504	0.18	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,029 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.18	ray::IDLE
21516	0.18	ray::IDLE
21505	0.18	ray::IDLE
21507	0.18	ray::IDLE
21499	0.18	ray::IDLE
21504	0.18	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,030 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.16	ray::IDLE
21516	0.16	ray::IDLE
21511	0.16	ray::IDLE
21507	0.16	ray::IDLE
21504	0.16	ray::IDLE
21505	0.16	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,030 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.16	ray::IDLE
21516	0.16	ray::IDLE
21511	0.16	ray::IDLE
21507	0.16	ray::IDLE
21504	0.16	ray::IDLE
21505	0.16	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,030 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.28	ray::DefaultActor.run
21515	0.26	ray::DefaultActor.run
21508	0.26	ray::DefaultActor.run
21512	0.26	ray::DefaultActor.run
21507	0.24	ray::DefaultActor.run
21504	0.24	ray::DefaultActor.run
21505	0.24	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,030 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.28	ray::DefaultActor.run
21515	0.26	ray::DefaultActor.run
21508	0.26	ray::DefaultActor.run
21512	0.26	ray::DefaultActor.run
21507	0.24	ray::DefaultActor.run
21504	0.24	ray::DefaultActor.run
21505	0.24	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,031 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.17	ray::IDLE
21512	0.17	ray::IDLE
21507	0.17	ray::IDLE
21505	0.17	ray::IDLE
21511	0.17	ray::IDLE
21504	0.17	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,031 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.17	ray::IDLE
21512	0.17	ray::IDLE
21507	0.17	ray::IDLE
21505	0.17	ray::IDLE
21511	0.17	ray::IDLE
21504	0.17	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,031 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.72	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.27	ray::DefaultActor.run
21512	0.24	ray::DefaultActor.run
21515	0.24	ray::DefaultActor.run
21508	0.23	ray::DefaultActor.run
21509	0.23	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21507	0.22	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,031 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.72	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.27	ray::DefaultActor.run
21512	0.24	ray::DefaultActor.run
21515	0.24	ray::DefaultActor.run
21508	0.23	ray::DefaultActor.run
21509	0.23	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21507	0.22	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,032 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.29	ray::DefaultActor.run
21508	0.28	ray::DefaultActor.run
21507	0.27	ray::DefaultActor.run
21506	0.27	ray::DefaultActor.run
21512	0.27	ray::DefaultActor.run
21505	0.27	ray::DefaultActor.run
21504	0.26	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,035 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.18	ray::IDLE
21516	0.18	ray::IDLE
21505	0.18	ray::IDLE
21507	0.18	ray::IDLE
21499	0.18	ray::IDLE
21504	0.18	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,032 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.29	ray::DefaultActor.run
21508	0.28	ray::DefaultActor.run
21507	0.27	ray::DefaultActor.run
21506	0.27	ray::DefaultActor.run
21512	0.27	ray::DefaultActor.run
21505	0.27	ray::DefaultActor.run
21504	0.26	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,035 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.18	ray::IDLE
21516	0.18	ray::IDLE
21505	0.18	ray::IDLE
21507	0.18	ray::IDLE
21499	0.18	ray::IDLE
21504	0.18	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,037 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.28	ray::DefaultActor.run
21515	0.26	ray::DefaultActor.run
21508	0.26	ray::DefaultActor.run
21512	0.26	ray::DefaultActor.run
21507	0.24	ray::DefaultActor.run
21504	0.24	ray::DefaultActor.run
21505	0.24	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,037 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.28	ray::DefaultActor.run
21515	0.26	ray::DefaultActor.run
21508	0.26	ray::DefaultActor.run
21512	0.26	ray::DefaultActor.run
21507	0.24	ray::DefaultActor.run
21504	0.24	ray::DefaultActor.run
21505	0.24	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,039 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.17	ray::IDLE
21512	0.17	ray::IDLE
21507	0.17	ray::IDLE
21505	0.17	ray::IDLE
21511	0.17	ray::IDLE
21504	0.17	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,039 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.17	ray::IDLE
21512	0.17	ray::IDLE
21507	0.17	ray::IDLE
21505	0.17	ray::IDLE
21511	0.17	ray::IDLE
21504	0.17	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,042 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.29	ray::DefaultActor.run
21508	0.28	ray::DefaultActor.run
21507	0.27	ray::DefaultActor.run
21506	0.27	ray::DefaultActor.run
21512	0.27	ray::DefaultActor.run
21505	0.27	ray::DefaultActor.run
21504	0.26	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,042 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.29	ray::DefaultActor.run
21508	0.28	ray::DefaultActor.run
21507	0.27	ray::DefaultActor.run
21506	0.27	ray::DefaultActor.run
21512	0.27	ray::DefaultActor.run
21505	0.27	ray::DefaultActor.run
21504	0.26	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | DEBUG flwr 2024-03-05 15:03:48,046 | server.py:187 | evaluate_round 4 received 0 results and 19 failures
myFlowerExperiment | DEBUG flwr 2024-03-05 15:03:48,046 | server.py:187 | evaluate_round 4 received 0 results and 19 failures
myFlowerExperiment | DEBUG flwr 2024-03-05 15:03:48,047 | server.py:222 | fit_round 5: strategy sampled 19 clients (out of 19)
myFlowerExperiment | DEBUG flwr 2024-03-05 15:03:48,047 | server.py:222 | fit_round 5: strategy sampled 19 clients (out of 19)
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,262 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.29	ray::DefaultActor.run
21508	0.28	ray::DefaultActor.run
21507	0.27	ray::DefaultActor.run
21506	0.27	ray::DefaultActor.run
21512	0.27	ray::DefaultActor.run
21505	0.27	ray::DefaultActor.run
21504	0.26	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,262 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.29	ray::DefaultActor.run
21508	0.28	ray::DefaultActor.run
21507	0.27	ray::DefaultActor.run
21506	0.27	ray::DefaultActor.run
21512	0.27	ray::DefaultActor.run
21505	0.27	ray::DefaultActor.run
21504	0.26	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,264 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.29	ray::DefaultActor.run
21508	0.28	ray::DefaultActor.run
21507	0.27	ray::DefaultActor.run
21506	0.27	ray::DefaultActor.run
21512	0.27	ray::DefaultActor.run
21505	0.27	ray::DefaultActor.run
21504	0.26	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,264 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.29	ray::DefaultActor.run
21508	0.28	ray::DefaultActor.run
21507	0.27	ray::DefaultActor.run
21506	0.27	ray::DefaultActor.run
21512	0.27	ray::DefaultActor.run
21505	0.27	ray::DefaultActor.run
21504	0.26	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,296 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.28	ray::DefaultActor.run
21515	0.26	ray::DefaultActor.run
21508	0.26	ray::DefaultActor.run
21512	0.26	ray::DefaultActor.run
21507	0.24	ray::DefaultActor.run
21504	0.24	ray::DefaultActor.run
21505	0.24	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,296 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.28	ray::DefaultActor.run
21515	0.26	ray::DefaultActor.run
21508	0.26	ray::DefaultActor.run
21512	0.26	ray::DefaultActor.run
21507	0.24	ray::DefaultActor.run
21504	0.24	ray::DefaultActor.run
21505	0.24	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,298 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.17	ray::IDLE
21512	0.17	ray::IDLE
21507	0.17	ray::IDLE
21505	0.17	ray::IDLE
21511	0.17	ray::IDLE
21504	0.17	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,298 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.17	ray::IDLE
21512	0.17	ray::IDLE
21507	0.17	ray::IDLE
21505	0.17	ray::IDLE
21511	0.17	ray::IDLE
21504	0.17	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,300 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.18	ray::IDLE
21516	0.18	ray::IDLE
21505	0.18	ray::IDLE
21507	0.18	ray::IDLE
21499	0.18	ray::IDLE
21504	0.18	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,300 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.18	ray::IDLE
21516	0.18	ray::IDLE
21505	0.18	ray::IDLE
21507	0.18	ray::IDLE
21499	0.18	ray::IDLE
21504	0.18	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,302 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.28	ray::DefaultActor.run
21515	0.26	ray::DefaultActor.run
21508	0.26	ray::DefaultActor.run
21512	0.26	ray::DefaultActor.run
21507	0.24	ray::DefaultActor.run
21504	0.24	ray::DefaultActor.run
21505	0.24	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,302 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.28	ray::DefaultActor.run
21515	0.26	ray::DefaultActor.run
21508	0.26	ray::DefaultActor.run
21512	0.26	ray::DefaultActor.run
21507	0.24	ray::DefaultActor.run
21504	0.24	ray::DefaultActor.run
21505	0.24	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,304 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.72	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.27	ray::DefaultActor.run
21512	0.24	ray::DefaultActor.run
21515	0.24	ray::DefaultActor.run
21508	0.23	ray::DefaultActor.run
21509	0.23	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21507	0.22	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,304 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.72	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.27	ray::DefaultActor.run
21512	0.24	ray::DefaultActor.run
21515	0.24	ray::DefaultActor.run
21508	0.23	ray::DefaultActor.run
21509	0.23	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21507	0.22	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,305 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.17	ray::IDLE
21512	0.17	ray::IDLE
21507	0.17	ray::IDLE
21505	0.17	ray::IDLE
21511	0.17	ray::IDLE
21504	0.17	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,305 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.17	ray::IDLE
21512	0.17	ray::IDLE
21507	0.17	ray::IDLE
21505	0.17	ray::IDLE
21511	0.17	ray::IDLE
21504	0.17	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,306 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.18	ray::IDLE
21516	0.18	ray::IDLE
21505	0.18	ray::IDLE
21507	0.18	ray::IDLE
21499	0.18	ray::IDLE
21504	0.18	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,306 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.18	ray::IDLE
21516	0.18	ray::IDLE
21505	0.18	ray::IDLE
21507	0.18	ray::IDLE
21499	0.18	ray::IDLE
21504	0.18	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,307 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.16	ray::IDLE
21516	0.16	ray::IDLE
21511	0.16	ray::IDLE
21507	0.16	ray::IDLE
21504	0.16	ray::IDLE
21505	0.16	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,307 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.16	ray::IDLE
21516	0.16	ray::IDLE
21511	0.16	ray::IDLE
21507	0.16	ray::IDLE
21504	0.16	ray::IDLE
21505	0.16	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,310 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.72	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.27	ray::DefaultActor.run
21512	0.24	ray::DefaultActor.run
21515	0.24	ray::DefaultActor.run
21508	0.23	ray::DefaultActor.run
21509	0.23	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21507	0.22	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,310 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.72	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.27	ray::DefaultActor.run
21512	0.24	ray::DefaultActor.run
21515	0.24	ray::DefaultActor.run
21508	0.23	ray::DefaultActor.run
21509	0.23	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21507	0.22	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,311 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.99	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.38	ray::DefaultActor.run
21505	0.38	ray::DefaultActor.run
21506	0.37	ray::DefaultActor.run
21512	0.37	ray::DefaultActor.run
21516	0.37	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21025	0.08	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,311 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.99	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.38	ray::DefaultActor.run
21505	0.38	ray::DefaultActor.run
21506	0.37	ray::DefaultActor.run
21512	0.37	ray::DefaultActor.run
21516	0.37	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21025	0.08	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,313 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.75	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.31	ray::DefaultActor.run
21505	0.31	ray::DefaultActor.run
21512	0.31	ray::DefaultActor.run
21506	0.30	ray::DefaultActor.run
21516	0.30	ray::DefaultActor.run
21504	0.30	ray::DefaultActor.run
21503	0.29	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,313 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.75	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.31	ray::DefaultActor.run
21505	0.31	ray::DefaultActor.run
21512	0.31	ray::DefaultActor.run
21506	0.30	ray::DefaultActor.run
21516	0.30	ray::DefaultActor.run
21504	0.30	ray::DefaultActor.run
21503	0.29	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,313 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.29	ray::DefaultActor.run
21508	0.28	ray::DefaultActor.run
21507	0.27	ray::DefaultActor.run
21506	0.27	ray::DefaultActor.run
21512	0.27	ray::DefaultActor.run
21505	0.27	ray::DefaultActor.run
21504	0.26	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,313 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.29	ray::DefaultActor.run
21508	0.28	ray::DefaultActor.run
21507	0.27	ray::DefaultActor.run
21506	0.27	ray::DefaultActor.run
21512	0.27	ray::DefaultActor.run
21505	0.27	ray::DefaultActor.run
21504	0.26	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,314 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.28	ray::DefaultActor.run
21515	0.26	ray::DefaultActor.run
21508	0.26	ray::DefaultActor.run
21512	0.26	ray::DefaultActor.run
21507	0.24	ray::DefaultActor.run
21504	0.24	ray::DefaultActor.run
21505	0.24	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,314 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.28	ray::DefaultActor.run
21515	0.26	ray::DefaultActor.run
21508	0.26	ray::DefaultActor.run
21512	0.26	ray::DefaultActor.run
21507	0.24	ray::DefaultActor.run
21504	0.24	ray::DefaultActor.run
21505	0.24	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,316 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.16	ray::IDLE
21516	0.16	ray::IDLE
21511	0.16	ray::IDLE
21507	0.16	ray::IDLE
21504	0.16	ray::IDLE
21505	0.16	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,316 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.16	ray::IDLE
21516	0.16	ray::IDLE
21511	0.16	ray::IDLE
21507	0.16	ray::IDLE
21504	0.16	ray::IDLE
21505	0.16	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,318 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.72	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.27	ray::DefaultActor.run
21512	0.24	ray::DefaultActor.run
21515	0.24	ray::DefaultActor.run
21508	0.23	ray::DefaultActor.run
21509	0.23	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21507	0.22	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,318 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.72	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.27	ray::DefaultActor.run
21512	0.24	ray::DefaultActor.run
21515	0.24	ray::DefaultActor.run
21508	0.23	ray::DefaultActor.run
21509	0.23	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21507	0.22	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,319 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.19	ray::IDLE
21516	0.19	ray::IDLE
21507	0.19	ray::IDLE
21505	0.19	ray::IDLE
21504	0.19	ray::IDLE
21499	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,319 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.19	ray::IDLE
21516	0.19	ray::IDLE
21507	0.19	ray::IDLE
21505	0.19	ray::IDLE
21504	0.19	ray::IDLE
21499	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,321 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.52	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.26	ray::DefaultActor
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.22	ray::DefaultActor
21505	0.22	ray::DefaultActor
21499	0.22	ray::DefaultActor
21508	0.22	ray::DefaultActor
21504	0.22	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,321 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.52	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.26	ray::DefaultActor
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.22	ray::DefaultActor
21505	0.22	ray::DefaultActor
21499	0.22	ray::DefaultActor
21508	0.22	ray::DefaultActor
21504	0.22	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,322 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.99	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.38	ray::DefaultActor.run
21505	0.38	ray::DefaultActor.run
21506	0.37	ray::DefaultActor.run
21512	0.37	ray::DefaultActor.run
21516	0.37	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21025	0.08	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,322 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.99	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.38	ray::DefaultActor.run
21505	0.38	ray::DefaultActor.run
21506	0.37	ray::DefaultActor.run
21512	0.37	ray::DefaultActor.run
21516	0.37	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21025	0.08	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,325 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.75	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.31	ray::DefaultActor.run
21505	0.31	ray::DefaultActor.run
21512	0.31	ray::DefaultActor.run
21506	0.30	ray::DefaultActor.run
21516	0.30	ray::DefaultActor.run
21504	0.30	ray::DefaultActor.run
21503	0.29	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,325 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.75	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.31	ray::DefaultActor.run
21505	0.31	ray::DefaultActor.run
21512	0.31	ray::DefaultActor.run
21506	0.30	ray::DefaultActor.run
21516	0.30	ray::DefaultActor.run
21504	0.30	ray::DefaultActor.run
21503	0.29	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,327 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.21	ray::IDLE
21512	0.21	ray::IDLE
21507	0.21	ray::IDLE
21505	0.21	ray::IDLE
21504	0.21	ray::IDLE
21515	0.21	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,327 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.21	ray::IDLE
21512	0.21	ray::IDLE
21507	0.21	ray::IDLE
21505	0.21	ray::IDLE
21504	0.21	ray::IDLE
21515	0.21	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,327 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.29	ray::DefaultActor.run
21508	0.28	ray::DefaultActor.run
21507	0.27	ray::DefaultActor.run
21506	0.27	ray::DefaultActor.run
21512	0.27	ray::DefaultActor.run
21505	0.27	ray::DefaultActor.run
21504	0.26	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,327 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.29	ray::DefaultActor.run
21508	0.28	ray::DefaultActor.run
21507	0.27	ray::DefaultActor.run
21506	0.27	ray::DefaultActor.run
21512	0.27	ray::DefaultActor.run
21505	0.27	ray::DefaultActor.run
21504	0.26	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,328 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.89	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.36	ray::DefaultActor.run
21512	0.35	ray::DefaultActor.run
21508	0.35	ray::DefaultActor
21505	0.35	ray::DefaultActor
21516	0.34	ray::DefaultActor
21503	0.34	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,328 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.89	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.36	ray::DefaultActor.run
21512	0.35	ray::DefaultActor.run
21508	0.35	ray::DefaultActor
21505	0.35	ray::DefaultActor
21516	0.34	ray::DefaultActor
21503	0.34	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,329 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.28	ray::DefaultActor.run
21515	0.26	ray::DefaultActor.run
21508	0.26	ray::DefaultActor.run
21512	0.26	ray::DefaultActor.run
21507	0.24	ray::DefaultActor.run
21504	0.24	ray::DefaultActor.run
21505	0.24	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,329 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.28	ray::DefaultActor.run
21515	0.26	ray::DefaultActor.run
21508	0.26	ray::DefaultActor.run
21512	0.26	ray::DefaultActor.run
21507	0.24	ray::DefaultActor.run
21504	0.24	ray::DefaultActor.run
21505	0.24	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,332 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.72	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.27	ray::DefaultActor.run
21512	0.24	ray::DefaultActor.run
21515	0.24	ray::DefaultActor.run
21508	0.23	ray::DefaultActor.run
21509	0.23	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21507	0.22	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,332 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.72	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.27	ray::DefaultActor.run
21512	0.24	ray::DefaultActor.run
21515	0.24	ray::DefaultActor.run
21508	0.23	ray::DefaultActor.run
21509	0.23	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21507	0.22	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,333 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.19	ray::IDLE
21516	0.19	ray::IDLE
21507	0.19	ray::IDLE
21505	0.19	ray::IDLE
21504	0.19	ray::IDLE
21499	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,333 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.19	ray::IDLE
21516	0.19	ray::IDLE
21507	0.19	ray::IDLE
21505	0.19	ray::IDLE
21504	0.19	ray::IDLE
21499	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,335 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.52	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.26	ray::DefaultActor
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.22	ray::DefaultActor
21505	0.22	ray::DefaultActor
21499	0.22	ray::DefaultActor
21508	0.22	ray::DefaultActor
21504	0.22	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,335 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.52	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.26	ray::DefaultActor
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.22	ray::DefaultActor
21505	0.22	ray::DefaultActor
21499	0.22	ray::DefaultActor
21508	0.22	ray::DefaultActor
21504	0.22	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,344 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.21	ray::IDLE
21512	0.21	ray::IDLE
21507	0.21	ray::IDLE
21505	0.21	ray::IDLE
21504	0.21	ray::IDLE
21515	0.21	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,344 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.21	ray::IDLE
21512	0.21	ray::IDLE
21507	0.21	ray::IDLE
21505	0.21	ray::IDLE
21504	0.21	ray::IDLE
21515	0.21	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,346 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.89	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.36	ray::DefaultActor.run
21512	0.35	ray::DefaultActor.run
21508	0.35	ray::DefaultActor
21505	0.35	ray::DefaultActor
21516	0.34	ray::DefaultActor
21503	0.34	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:03:48,346 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.89	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.36	ray::DefaultActor.run
21512	0.35	ray::DefaultActor.run
21508	0.35	ray::DefaultActor
21505	0.35	ray::DefaultActor
21516	0.34	ray::DefaultActor
21503	0.34	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | DEBUG flwr 2024-03-05 15:03:55,976 | server.py:236 | fit_round 5 received 4 results and 15 failures
myFlowerExperiment | DEBUG flwr 2024-03-05 15:03:55,976 | server.py:236 | fit_round 5 received 4 results and 15 failures
myFlowerExperiment | INFO flwr 2024-03-05 15:04:01,498 | server.py:125 | fit progress: (5, 0.7544503808021545, {'accuracy': 0.5235030055046082}, 75.24743984000088)
myFlowerExperiment | INFO flwr 2024-03-05 15:04:01,498 | server.py:125 | fit progress: (5, 0.7544503808021545, {'accuracy': 0.5235030055046082}, 75.24743984000088)
myFlowerExperiment | DEBUG flwr 2024-03-05 15:04:01,500 | server.py:173 | evaluate_round 5: strategy sampled 19 clients (out of 19)
myFlowerExperiment | DEBUG flwr 2024-03-05 15:04:01,500 | server.py:173 | evaluate_round 5: strategy sampled 19 clients (out of 19)
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:01,701 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.21	ray::IDLE
21512	0.21	ray::IDLE
21507	0.21	ray::IDLE
21505	0.21	ray::IDLE
21504	0.21	ray::IDLE
21515	0.21	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:01,701 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.21	ray::IDLE
21512	0.21	ray::IDLE
21507	0.21	ray::IDLE
21505	0.21	ray::IDLE
21504	0.21	ray::IDLE
21515	0.21	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:01,712 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.21	ray::IDLE
21512	0.21	ray::IDLE
21507	0.21	ray::IDLE
21505	0.21	ray::IDLE
21504	0.21	ray::IDLE
21515	0.21	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:01,712 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.21	ray::IDLE
21512	0.21	ray::IDLE
21507	0.21	ray::IDLE
21505	0.21	ray::IDLE
21504	0.21	ray::IDLE
21515	0.21	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:01,715 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.52	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.26	ray::DefaultActor
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.22	ray::DefaultActor
21505	0.22	ray::DefaultActor
21499	0.22	ray::DefaultActor
21508	0.22	ray::DefaultActor
21504	0.22	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:01,715 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.52	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.26	ray::DefaultActor
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.22	ray::DefaultActor
21505	0.22	ray::DefaultActor
21499	0.22	ray::DefaultActor
21508	0.22	ray::DefaultActor
21504	0.22	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:01,717 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.52	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.26	ray::DefaultActor
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.22	ray::DefaultActor
21505	0.22	ray::DefaultActor
21499	0.22	ray::DefaultActor
21508	0.22	ray::DefaultActor
21504	0.22	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:01,717 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.52	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.26	ray::DefaultActor
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.22	ray::DefaultActor
21505	0.22	ray::DefaultActor
21499	0.22	ray::DefaultActor
21508	0.22	ray::DefaultActor
21504	0.22	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:01,721 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.28	ray::DefaultActor.run
21515	0.26	ray::DefaultActor.run
21508	0.26	ray::DefaultActor.run
21512	0.26	ray::DefaultActor.run
21507	0.24	ray::DefaultActor.run
21504	0.24	ray::DefaultActor.run
21505	0.24	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:01,721 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.28	ray::DefaultActor.run
21515	0.26	ray::DefaultActor.run
21508	0.26	ray::DefaultActor.run
21512	0.26	ray::DefaultActor.run
21507	0.24	ray::DefaultActor.run
21504	0.24	ray::DefaultActor.run
21505	0.24	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:01,723 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.72	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.27	ray::DefaultActor.run
21512	0.24	ray::DefaultActor.run
21515	0.24	ray::DefaultActor.run
21508	0.23	ray::DefaultActor.run
21509	0.23	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21507	0.22	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:01,723 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.72	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.27	ray::DefaultActor.run
21512	0.24	ray::DefaultActor.run
21515	0.24	ray::DefaultActor.run
21508	0.23	ray::DefaultActor.run
21509	0.23	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21507	0.22	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:01,724 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.19	ray::IDLE
21516	0.19	ray::IDLE
21507	0.19	ray::IDLE
21505	0.19	ray::IDLE
21504	0.19	ray::IDLE
21499	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:01,724 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.19	ray::IDLE
21516	0.19	ray::IDLE
21507	0.19	ray::IDLE
21505	0.19	ray::IDLE
21504	0.19	ray::IDLE
21499	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:01,726 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.75	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.31	ray::DefaultActor.run
21505	0.31	ray::DefaultActor.run
21512	0.31	ray::DefaultActor.run
21506	0.30	ray::DefaultActor.run
21516	0.30	ray::DefaultActor.run
21504	0.30	ray::DefaultActor.run
21503	0.29	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:01,726 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.75	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.31	ray::DefaultActor.run
21505	0.31	ray::DefaultActor.run
21512	0.31	ray::DefaultActor.run
21506	0.30	ray::DefaultActor.run
21516	0.30	ray::DefaultActor.run
21504	0.30	ray::DefaultActor.run
21503	0.29	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:01,726 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.28	ray::DefaultActor.run
21515	0.26	ray::DefaultActor.run
21508	0.26	ray::DefaultActor.run
21512	0.26	ray::DefaultActor.run
21507	0.24	ray::DefaultActor.run
21504	0.24	ray::DefaultActor.run
21505	0.24	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:01,726 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.28	ray::DefaultActor.run
21515	0.26	ray::DefaultActor.run
21508	0.26	ray::DefaultActor.run
21512	0.26	ray::DefaultActor.run
21507	0.24	ray::DefaultActor.run
21504	0.24	ray::DefaultActor.run
21505	0.24	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:01,727 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.72	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.27	ray::DefaultActor.run
21512	0.24	ray::DefaultActor.run
21515	0.24	ray::DefaultActor.run
21508	0.23	ray::DefaultActor.run
21509	0.23	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21507	0.22	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:01,727 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.72	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.27	ray::DefaultActor.run
21512	0.24	ray::DefaultActor.run
21515	0.24	ray::DefaultActor.run
21508	0.23	ray::DefaultActor.run
21509	0.23	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21507	0.22	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:01,727 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.18	ray::IDLE
21516	0.18	ray::IDLE
21505	0.18	ray::IDLE
21507	0.18	ray::IDLE
21499	0.18	ray::IDLE
21504	0.18	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:01,727 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.18	ray::IDLE
21516	0.18	ray::IDLE
21505	0.18	ray::IDLE
21507	0.18	ray::IDLE
21499	0.18	ray::IDLE
21504	0.18	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:01,728 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.52	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.26	ray::DefaultActor
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.22	ray::DefaultActor
21505	0.22	ray::DefaultActor
21499	0.22	ray::DefaultActor
21508	0.22	ray::DefaultActor
21504	0.22	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:01,728 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.52	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.26	ray::DefaultActor
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.22	ray::DefaultActor
21505	0.22	ray::DefaultActor
21499	0.22	ray::DefaultActor
21508	0.22	ray::DefaultActor
21504	0.22	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:01,728 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.19	ray::IDLE
21516	0.19	ray::IDLE
21507	0.19	ray::IDLE
21505	0.19	ray::IDLE
21504	0.19	ray::IDLE
21499	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:01,728 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.19	ray::IDLE
21516	0.19	ray::IDLE
21507	0.19	ray::IDLE
21505	0.19	ray::IDLE
21504	0.19	ray::IDLE
21499	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:01,733 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.75	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.31	ray::DefaultActor.run
21505	0.31	ray::DefaultActor.run
21512	0.31	ray::DefaultActor.run
21506	0.30	ray::DefaultActor.run
21516	0.30	ray::DefaultActor.run
21504	0.30	ray::DefaultActor.run
21503	0.29	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:01,733 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.75	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.31	ray::DefaultActor.run
21505	0.31	ray::DefaultActor.run
21512	0.31	ray::DefaultActor.run
21506	0.30	ray::DefaultActor.run
21516	0.30	ray::DefaultActor.run
21504	0.30	ray::DefaultActor.run
21503	0.29	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:01,733 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.89	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.36	ray::DefaultActor.run
21512	0.35	ray::DefaultActor.run
21508	0.35	ray::DefaultActor
21505	0.35	ray::DefaultActor
21516	0.34	ray::DefaultActor
21503	0.34	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:01,733 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.89	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.36	ray::DefaultActor.run
21512	0.35	ray::DefaultActor.run
21508	0.35	ray::DefaultActor
21505	0.35	ray::DefaultActor
21516	0.34	ray::DefaultActor
21503	0.34	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:01,736 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.18	ray::IDLE
21516	0.18	ray::IDLE
21505	0.18	ray::IDLE
21507	0.18	ray::IDLE
21499	0.18	ray::IDLE
21504	0.18	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:01,736 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.18	ray::IDLE
21516	0.18	ray::IDLE
21505	0.18	ray::IDLE
21507	0.18	ray::IDLE
21499	0.18	ray::IDLE
21504	0.18	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:01,737 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.52	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.26	ray::DefaultActor
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.22	ray::DefaultActor
21505	0.22	ray::DefaultActor
21499	0.22	ray::DefaultActor
21508	0.22	ray::DefaultActor
21504	0.22	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:01,737 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.52	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.26	ray::DefaultActor
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.22	ray::DefaultActor
21505	0.22	ray::DefaultActor
21499	0.22	ray::DefaultActor
21508	0.22	ray::DefaultActor
21504	0.22	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:01,738 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.99	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.38	ray::DefaultActor.run
21505	0.38	ray::DefaultActor.run
21506	0.37	ray::DefaultActor.run
21512	0.37	ray::DefaultActor.run
21516	0.37	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21025	0.08	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:01,738 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.99	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.38	ray::DefaultActor.run
21505	0.38	ray::DefaultActor.run
21506	0.37	ray::DefaultActor.run
21512	0.37	ray::DefaultActor.run
21516	0.37	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21025	0.08	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:01,740 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.29	ray::DefaultActor.run
21508	0.28	ray::DefaultActor.run
21507	0.27	ray::DefaultActor.run
21506	0.27	ray::DefaultActor.run
21512	0.27	ray::DefaultActor.run
21505	0.27	ray::DefaultActor.run
21504	0.26	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:01,740 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.29	ray::DefaultActor.run
21508	0.28	ray::DefaultActor.run
21507	0.27	ray::DefaultActor.run
21506	0.27	ray::DefaultActor.run
21512	0.27	ray::DefaultActor.run
21505	0.27	ray::DefaultActor.run
21504	0.26	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:01,741 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.17	ray::IDLE
21512	0.17	ray::IDLE
21507	0.17	ray::IDLE
21505	0.17	ray::IDLE
21511	0.17	ray::IDLE
21504	0.17	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:01,741 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.17	ray::IDLE
21512	0.17	ray::IDLE
21507	0.17	ray::IDLE
21505	0.17	ray::IDLE
21511	0.17	ray::IDLE
21504	0.17	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:01,742 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.89	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.36	ray::DefaultActor.run
21512	0.35	ray::DefaultActor.run
21508	0.35	ray::DefaultActor
21505	0.35	ray::DefaultActor
21516	0.34	ray::DefaultActor
21503	0.34	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:01,742 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.89	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.36	ray::DefaultActor.run
21512	0.35	ray::DefaultActor.run
21508	0.35	ray::DefaultActor
21505	0.35	ray::DefaultActor
21516	0.34	ray::DefaultActor
21503	0.34	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:01,744 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.16	ray::IDLE
21516	0.16	ray::IDLE
21511	0.16	ray::IDLE
21507	0.16	ray::IDLE
21504	0.16	ray::IDLE
21505	0.16	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:01,744 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.16	ray::IDLE
21516	0.16	ray::IDLE
21511	0.16	ray::IDLE
21507	0.16	ray::IDLE
21504	0.16	ray::IDLE
21505	0.16	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:01,744 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.21	ray::IDLE
21512	0.21	ray::IDLE
21507	0.21	ray::IDLE
21505	0.21	ray::IDLE
21504	0.21	ray::IDLE
21515	0.21	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:01,744 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.21	ray::IDLE
21512	0.21	ray::IDLE
21507	0.21	ray::IDLE
21505	0.21	ray::IDLE
21504	0.21	ray::IDLE
21515	0.21	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:01,747 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.99	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.38	ray::DefaultActor.run
21505	0.38	ray::DefaultActor.run
21506	0.37	ray::DefaultActor.run
21512	0.37	ray::DefaultActor.run
21516	0.37	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21025	0.08	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:01,747 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.99	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.38	ray::DefaultActor.run
21505	0.38	ray::DefaultActor.run
21506	0.37	ray::DefaultActor.run
21512	0.37	ray::DefaultActor.run
21516	0.37	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21025	0.08	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:01,748 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.29	ray::DefaultActor.run
21508	0.28	ray::DefaultActor.run
21507	0.27	ray::DefaultActor.run
21506	0.27	ray::DefaultActor.run
21512	0.27	ray::DefaultActor.run
21505	0.27	ray::DefaultActor.run
21504	0.26	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:01,748 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.29	ray::DefaultActor.run
21508	0.28	ray::DefaultActor.run
21507	0.27	ray::DefaultActor.run
21506	0.27	ray::DefaultActor.run
21512	0.27	ray::DefaultActor.run
21505	0.27	ray::DefaultActor.run
21504	0.26	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:01,750 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.17	ray::IDLE
21512	0.17	ray::IDLE
21507	0.17	ray::IDLE
21505	0.17	ray::IDLE
21511	0.17	ray::IDLE
21504	0.17	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:01,750 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.17	ray::IDLE
21512	0.17	ray::IDLE
21507	0.17	ray::IDLE
21505	0.17	ray::IDLE
21511	0.17	ray::IDLE
21504	0.17	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:01,752 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.16	ray::IDLE
21516	0.16	ray::IDLE
21511	0.16	ray::IDLE
21507	0.16	ray::IDLE
21504	0.16	ray::IDLE
21505	0.16	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:01,752 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.16	ray::IDLE
21516	0.16	ray::IDLE
21511	0.16	ray::IDLE
21507	0.16	ray::IDLE
21504	0.16	ray::IDLE
21505	0.16	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:01,753 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.21	ray::IDLE
21512	0.21	ray::IDLE
21507	0.21	ray::IDLE
21505	0.21	ray::IDLE
21504	0.21	ray::IDLE
21515	0.21	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:01,753 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.21	ray::IDLE
21512	0.21	ray::IDLE
21507	0.21	ray::IDLE
21505	0.21	ray::IDLE
21504	0.21	ray::IDLE
21515	0.21	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | DEBUG flwr 2024-03-05 15:04:01,758 | server.py:187 | evaluate_round 5 received 0 results and 19 failures
myFlowerExperiment | DEBUG flwr 2024-03-05 15:04:01,758 | server.py:187 | evaluate_round 5 received 0 results and 19 failures
myFlowerExperiment | DEBUG flwr 2024-03-05 15:04:01,760 | server.py:222 | fit_round 6: strategy sampled 19 clients (out of 19)
myFlowerExperiment | DEBUG flwr 2024-03-05 15:04:01,760 | server.py:222 | fit_round 6: strategy sampled 19 clients (out of 19)
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:01,963 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.21	ray::IDLE
21512	0.21	ray::IDLE
21507	0.21	ray::IDLE
21505	0.21	ray::IDLE
21504	0.21	ray::IDLE
21515	0.21	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:01,963 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.21	ray::IDLE
21512	0.21	ray::IDLE
21507	0.21	ray::IDLE
21505	0.21	ray::IDLE
21504	0.21	ray::IDLE
21515	0.21	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:01,974 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.21	ray::IDLE
21512	0.21	ray::IDLE
21507	0.21	ray::IDLE
21505	0.21	ray::IDLE
21504	0.21	ray::IDLE
21515	0.21	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:01,974 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.21	ray::IDLE
21512	0.21	ray::IDLE
21507	0.21	ray::IDLE
21505	0.21	ray::IDLE
21504	0.21	ray::IDLE
21515	0.21	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:01,991 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.89	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.36	ray::DefaultActor.run
21512	0.35	ray::DefaultActor.run
21508	0.35	ray::DefaultActor
21505	0.35	ray::DefaultActor
21516	0.34	ray::DefaultActor
21503	0.34	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:01,991 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.89	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.36	ray::DefaultActor.run
21512	0.35	ray::DefaultActor.run
21508	0.35	ray::DefaultActor
21505	0.35	ray::DefaultActor
21516	0.34	ray::DefaultActor
21503	0.34	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:01,993 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.89	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.36	ray::DefaultActor.run
21512	0.35	ray::DefaultActor.run
21508	0.35	ray::DefaultActor
21505	0.35	ray::DefaultActor
21516	0.34	ray::DefaultActor
21503	0.34	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:01,993 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.89	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.36	ray::DefaultActor.run
21512	0.35	ray::DefaultActor.run
21508	0.35	ray::DefaultActor
21505	0.35	ray::DefaultActor
21516	0.34	ray::DefaultActor
21503	0.34	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:01,996 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.99	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.38	ray::DefaultActor.run
21505	0.38	ray::DefaultActor.run
21506	0.37	ray::DefaultActor.run
21512	0.37	ray::DefaultActor.run
21516	0.37	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21025	0.08	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:01,996 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.99	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.38	ray::DefaultActor.run
21505	0.38	ray::DefaultActor.run
21506	0.37	ray::DefaultActor.run
21512	0.37	ray::DefaultActor.run
21516	0.37	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21025	0.08	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:01,998 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.29	ray::DefaultActor.run
21508	0.28	ray::DefaultActor.run
21507	0.27	ray::DefaultActor.run
21506	0.27	ray::DefaultActor.run
21512	0.27	ray::DefaultActor.run
21505	0.27	ray::DefaultActor.run
21504	0.26	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:01,998 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.29	ray::DefaultActor.run
21508	0.28	ray::DefaultActor.run
21507	0.27	ray::DefaultActor.run
21506	0.27	ray::DefaultActor.run
21512	0.27	ray::DefaultActor.run
21505	0.27	ray::DefaultActor.run
21504	0.26	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:01,999 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.99	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.38	ray::DefaultActor.run
21505	0.38	ray::DefaultActor.run
21506	0.37	ray::DefaultActor.run
21512	0.37	ray::DefaultActor.run
21516	0.37	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21025	0.08	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:01,999 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.99	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.38	ray::DefaultActor.run
21505	0.38	ray::DefaultActor.run
21506	0.37	ray::DefaultActor.run
21512	0.37	ray::DefaultActor.run
21516	0.37	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21025	0.08	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:02,000 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.52	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.26	ray::DefaultActor
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.22	ray::DefaultActor
21505	0.22	ray::DefaultActor
21499	0.22	ray::DefaultActor
21508	0.22	ray::DefaultActor
21504	0.22	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:02,000 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.52	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.26	ray::DefaultActor
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.22	ray::DefaultActor
21505	0.22	ray::DefaultActor
21499	0.22	ray::DefaultActor
21508	0.22	ray::DefaultActor
21504	0.22	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:02,002 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.75	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.31	ray::DefaultActor.run
21505	0.31	ray::DefaultActor.run
21512	0.31	ray::DefaultActor.run
21506	0.30	ray::DefaultActor.run
21516	0.30	ray::DefaultActor.run
21504	0.30	ray::DefaultActor.run
21503	0.29	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:02,002 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.75	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.31	ray::DefaultActor.run
21505	0.31	ray::DefaultActor.run
21512	0.31	ray::DefaultActor.run
21506	0.30	ray::DefaultActor.run
21516	0.30	ray::DefaultActor.run
21504	0.30	ray::DefaultActor.run
21503	0.29	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:02,002 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.19	ray::IDLE
21516	0.19	ray::IDLE
21507	0.19	ray::IDLE
21505	0.19	ray::IDLE
21504	0.19	ray::IDLE
21499	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:02,002 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.19	ray::IDLE
21516	0.19	ray::IDLE
21507	0.19	ray::IDLE
21505	0.19	ray::IDLE
21504	0.19	ray::IDLE
21499	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:02,004 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.29	ray::DefaultActor.run
21508	0.28	ray::DefaultActor.run
21507	0.27	ray::DefaultActor.run
21506	0.27	ray::DefaultActor.run
21512	0.27	ray::DefaultActor.run
21505	0.27	ray::DefaultActor.run
21504	0.26	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:02,004 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.29	ray::DefaultActor.run
21508	0.28	ray::DefaultActor.run
21507	0.27	ray::DefaultActor.run
21506	0.27	ray::DefaultActor.run
21512	0.27	ray::DefaultActor.run
21505	0.27	ray::DefaultActor.run
21504	0.26	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:02,004 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.28	ray::DefaultActor.run
21515	0.26	ray::DefaultActor.run
21508	0.26	ray::DefaultActor.run
21512	0.26	ray::DefaultActor.run
21507	0.24	ray::DefaultActor.run
21504	0.24	ray::DefaultActor.run
21505	0.24	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:02,004 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.28	ray::DefaultActor.run
21515	0.26	ray::DefaultActor.run
21508	0.26	ray::DefaultActor.run
21512	0.26	ray::DefaultActor.run
21507	0.24	ray::DefaultActor.run
21504	0.24	ray::DefaultActor.run
21505	0.24	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:02,007 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.72	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.27	ray::DefaultActor.run
21512	0.24	ray::DefaultActor.run
21515	0.24	ray::DefaultActor.run
21508	0.23	ray::DefaultActor.run
21509	0.23	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21507	0.22	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:02,007 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.72	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.27	ray::DefaultActor.run
21512	0.24	ray::DefaultActor.run
21515	0.24	ray::DefaultActor.run
21508	0.23	ray::DefaultActor.run
21509	0.23	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21507	0.22	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:02,007 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.52	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.26	ray::DefaultActor
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.22	ray::DefaultActor
21505	0.22	ray::DefaultActor
21499	0.22	ray::DefaultActor
21508	0.22	ray::DefaultActor
21504	0.22	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:02,008 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.21	ray::IDLE
21512	0.21	ray::IDLE
21507	0.21	ray::IDLE
21505	0.21	ray::IDLE
21504	0.21	ray::IDLE
21515	0.21	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:02,007 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.52	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.26	ray::DefaultActor
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.22	ray::DefaultActor
21505	0.22	ray::DefaultActor
21499	0.22	ray::DefaultActor
21508	0.22	ray::DefaultActor
21504	0.22	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:02,008 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.21	ray::IDLE
21512	0.21	ray::IDLE
21507	0.21	ray::IDLE
21505	0.21	ray::IDLE
21504	0.21	ray::IDLE
21515	0.21	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:02,010 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.75	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.31	ray::DefaultActor.run
21505	0.31	ray::DefaultActor.run
21512	0.31	ray::DefaultActor.run
21506	0.30	ray::DefaultActor.run
21516	0.30	ray::DefaultActor.run
21504	0.30	ray::DefaultActor.run
21503	0.29	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:02,010 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.75	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.31	ray::DefaultActor.run
21505	0.31	ray::DefaultActor.run
21512	0.31	ray::DefaultActor.run
21506	0.30	ray::DefaultActor.run
21516	0.30	ray::DefaultActor.run
21504	0.30	ray::DefaultActor.run
21503	0.29	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:02,012 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.99	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.38	ray::DefaultActor.run
21505	0.38	ray::DefaultActor.run
21506	0.37	ray::DefaultActor.run
21512	0.37	ray::DefaultActor.run
21516	0.37	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21025	0.08	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:02,012 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.99	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.38	ray::DefaultActor.run
21505	0.38	ray::DefaultActor.run
21506	0.37	ray::DefaultActor.run
21512	0.37	ray::DefaultActor.run
21516	0.37	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21025	0.08	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:02,012 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.17	ray::IDLE
21512	0.17	ray::IDLE
21507	0.17	ray::IDLE
21505	0.17	ray::IDLE
21511	0.17	ray::IDLE
21504	0.17	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:02,012 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.17	ray::IDLE
21512	0.17	ray::IDLE
21507	0.17	ray::IDLE
21505	0.17	ray::IDLE
21511	0.17	ray::IDLE
21504	0.17	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:02,013 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.19	ray::IDLE
21516	0.19	ray::IDLE
21507	0.19	ray::IDLE
21505	0.19	ray::IDLE
21504	0.19	ray::IDLE
21499	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:02,013 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.19	ray::IDLE
21516	0.19	ray::IDLE
21507	0.19	ray::IDLE
21505	0.19	ray::IDLE
21504	0.19	ray::IDLE
21499	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:02,015 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.18	ray::IDLE
21516	0.18	ray::IDLE
21505	0.18	ray::IDLE
21507	0.18	ray::IDLE
21499	0.18	ray::IDLE
21504	0.18	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:02,015 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.18	ray::IDLE
21516	0.18	ray::IDLE
21505	0.18	ray::IDLE
21507	0.18	ray::IDLE
21499	0.18	ray::IDLE
21504	0.18	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:02,018 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.89	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.36	ray::DefaultActor.run
21512	0.35	ray::DefaultActor.run
21508	0.35	ray::DefaultActor
21505	0.35	ray::DefaultActor
21516	0.34	ray::DefaultActor
21503	0.34	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:02,018 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.28	ray::DefaultActor.run
21515	0.26	ray::DefaultActor.run
21508	0.26	ray::DefaultActor.run
21512	0.26	ray::DefaultActor.run
21507	0.24	ray::DefaultActor.run
21504	0.24	ray::DefaultActor.run
21505	0.24	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:02,018 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.89	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.36	ray::DefaultActor.run
21512	0.35	ray::DefaultActor.run
21508	0.35	ray::DefaultActor
21505	0.35	ray::DefaultActor
21516	0.34	ray::DefaultActor
21503	0.34	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:02,018 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.16	ray::IDLE
21516	0.16	ray::IDLE
21511	0.16	ray::IDLE
21507	0.16	ray::IDLE
21504	0.16	ray::IDLE
21505	0.16	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:02,018 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.28	ray::DefaultActor.run
21515	0.26	ray::DefaultActor.run
21508	0.26	ray::DefaultActor.run
21512	0.26	ray::DefaultActor.run
21507	0.24	ray::DefaultActor.run
21504	0.24	ray::DefaultActor.run
21505	0.24	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:02,018 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.16	ray::IDLE
21516	0.16	ray::IDLE
21511	0.16	ray::IDLE
21507	0.16	ray::IDLE
21504	0.16	ray::IDLE
21505	0.16	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:02,019 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.72	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.27	ray::DefaultActor.run
21512	0.24	ray::DefaultActor.run
21515	0.24	ray::DefaultActor.run
21508	0.23	ray::DefaultActor.run
21509	0.23	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21507	0.22	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:02,019 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.72	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.27	ray::DefaultActor.run
21512	0.24	ray::DefaultActor.run
21515	0.24	ray::DefaultActor.run
21508	0.23	ray::DefaultActor.run
21509	0.23	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21507	0.22	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:02,022 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.21	ray::IDLE
21512	0.21	ray::IDLE
21507	0.21	ray::IDLE
21505	0.21	ray::IDLE
21504	0.21	ray::IDLE
21515	0.21	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:02,022 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.21	ray::IDLE
21512	0.21	ray::IDLE
21507	0.21	ray::IDLE
21505	0.21	ray::IDLE
21504	0.21	ray::IDLE
21515	0.21	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:02,027 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.99	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.38	ray::DefaultActor.run
21505	0.38	ray::DefaultActor.run
21506	0.37	ray::DefaultActor.run
21512	0.37	ray::DefaultActor.run
21516	0.37	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21025	0.08	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:02,027 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.99	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.38	ray::DefaultActor.run
21505	0.38	ray::DefaultActor.run
21506	0.37	ray::DefaultActor.run
21512	0.37	ray::DefaultActor.run
21516	0.37	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21025	0.08	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:02,029 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.17	ray::IDLE
21512	0.17	ray::IDLE
21507	0.17	ray::IDLE
21505	0.17	ray::IDLE
21511	0.17	ray::IDLE
21504	0.17	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:02,029 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.17	ray::IDLE
21512	0.17	ray::IDLE
21507	0.17	ray::IDLE
21505	0.17	ray::IDLE
21511	0.17	ray::IDLE
21504	0.17	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:02,033 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.18	ray::IDLE
21516	0.18	ray::IDLE
21505	0.18	ray::IDLE
21507	0.18	ray::IDLE
21499	0.18	ray::IDLE
21504	0.18	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:02,033 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.18	ray::IDLE
21516	0.18	ray::IDLE
21505	0.18	ray::IDLE
21507	0.18	ray::IDLE
21499	0.18	ray::IDLE
21504	0.18	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:02,034 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.89	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.36	ray::DefaultActor.run
21512	0.35	ray::DefaultActor.run
21508	0.35	ray::DefaultActor
21505	0.35	ray::DefaultActor
21516	0.34	ray::DefaultActor
21503	0.34	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:02,034 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.89	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.36	ray::DefaultActor.run
21512	0.35	ray::DefaultActor.run
21508	0.35	ray::DefaultActor
21505	0.35	ray::DefaultActor
21516	0.34	ray::DefaultActor
21503	0.34	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:02,037 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.16	ray::IDLE
21516	0.16	ray::IDLE
21511	0.16	ray::IDLE
21507	0.16	ray::IDLE
21504	0.16	ray::IDLE
21505	0.16	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:02,037 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.16	ray::IDLE
21516	0.16	ray::IDLE
21511	0.16	ray::IDLE
21507	0.16	ray::IDLE
21504	0.16	ray::IDLE
21505	0.16	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | DEBUG flwr 2024-03-05 15:04:09,614 | server.py:236 | fit_round 6 received 4 results and 15 failures
myFlowerExperiment | DEBUG flwr 2024-03-05 15:04:09,614 | server.py:236 | fit_round 6 received 4 results and 15 failures
myFlowerExperiment | INFO flwr 2024-03-05 15:04:15,116 | server.py:125 | fit progress: (6, 0.7448371648788452, {'accuracy': 0.5410522222518921}, 88.86510824699872)
myFlowerExperiment | INFO flwr 2024-03-05 15:04:15,116 | server.py:125 | fit progress: (6, 0.7448371648788452, {'accuracy': 0.5410522222518921}, 88.86510824699872)
myFlowerExperiment | DEBUG flwr 2024-03-05 15:04:15,118 | server.py:173 | evaluate_round 6: strategy sampled 19 clients (out of 19)
myFlowerExperiment | DEBUG flwr 2024-03-05 15:04:15,118 | server.py:173 | evaluate_round 6: strategy sampled 19 clients (out of 19)
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,319 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.89	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.36	ray::DefaultActor.run
21512	0.35	ray::DefaultActor.run
21508	0.35	ray::DefaultActor
21505	0.35	ray::DefaultActor
21516	0.34	ray::DefaultActor
21503	0.34	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,319 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.89	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.36	ray::DefaultActor.run
21512	0.35	ray::DefaultActor.run
21508	0.35	ray::DefaultActor
21505	0.35	ray::DefaultActor
21516	0.34	ray::DefaultActor
21503	0.34	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,330 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.89	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.36	ray::DefaultActor.run
21512	0.35	ray::DefaultActor.run
21508	0.35	ray::DefaultActor
21505	0.35	ray::DefaultActor
21516	0.34	ray::DefaultActor
21503	0.34	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,330 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.89	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.36	ray::DefaultActor.run
21512	0.35	ray::DefaultActor.run
21508	0.35	ray::DefaultActor
21505	0.35	ray::DefaultActor
21516	0.34	ray::DefaultActor
21503	0.34	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,335 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.21	ray::IDLE
21512	0.21	ray::IDLE
21507	0.21	ray::IDLE
21505	0.21	ray::IDLE
21504	0.21	ray::IDLE
21515	0.21	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,335 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.21	ray::IDLE
21512	0.21	ray::IDLE
21507	0.21	ray::IDLE
21505	0.21	ray::IDLE
21504	0.21	ray::IDLE
21515	0.21	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,337 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.21	ray::IDLE
21512	0.21	ray::IDLE
21507	0.21	ray::IDLE
21505	0.21	ray::IDLE
21504	0.21	ray::IDLE
21515	0.21	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,337 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.21	ray::IDLE
21512	0.21	ray::IDLE
21507	0.21	ray::IDLE
21505	0.21	ray::IDLE
21504	0.21	ray::IDLE
21515	0.21	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,341 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.99	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.38	ray::DefaultActor.run
21505	0.38	ray::DefaultActor.run
21506	0.37	ray::DefaultActor.run
21512	0.37	ray::DefaultActor.run
21516	0.37	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21025	0.08	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,341 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.99	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.38	ray::DefaultActor.run
21505	0.38	ray::DefaultActor.run
21506	0.37	ray::DefaultActor.run
21512	0.37	ray::DefaultActor.run
21516	0.37	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21025	0.08	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,342 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.18	ray::IDLE
21516	0.18	ray::IDLE
21505	0.18	ray::IDLE
21507	0.18	ray::IDLE
21499	0.18	ray::IDLE
21504	0.18	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,342 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.18	ray::IDLE
21516	0.18	ray::IDLE
21505	0.18	ray::IDLE
21507	0.18	ray::IDLE
21499	0.18	ray::IDLE
21504	0.18	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,343 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.72	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.27	ray::DefaultActor.run
21512	0.24	ray::DefaultActor.run
21515	0.24	ray::DefaultActor.run
21508	0.23	ray::DefaultActor.run
21509	0.23	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21507	0.22	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,343 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.72	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.27	ray::DefaultActor.run
21512	0.24	ray::DefaultActor.run
21515	0.24	ray::DefaultActor.run
21508	0.23	ray::DefaultActor.run
21509	0.23	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21507	0.22	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,343 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.17	ray::IDLE
21512	0.17	ray::IDLE
21507	0.17	ray::IDLE
21505	0.17	ray::IDLE
21511	0.17	ray::IDLE
21504	0.17	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,343 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.17	ray::IDLE
21512	0.17	ray::IDLE
21507	0.17	ray::IDLE
21505	0.17	ray::IDLE
21511	0.17	ray::IDLE
21504	0.17	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,344 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.19	ray::IDLE
21516	0.19	ray::IDLE
21507	0.19	ray::IDLE
21505	0.19	ray::IDLE
21504	0.19	ray::IDLE
21499	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,344 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.19	ray::IDLE
21516	0.19	ray::IDLE
21507	0.19	ray::IDLE
21505	0.19	ray::IDLE
21504	0.19	ray::IDLE
21499	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,345 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.99	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.38	ray::DefaultActor.run
21505	0.38	ray::DefaultActor.run
21506	0.37	ray::DefaultActor.run
21512	0.37	ray::DefaultActor.run
21516	0.37	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21025	0.08	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,345 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.99	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.38	ray::DefaultActor.run
21505	0.38	ray::DefaultActor.run
21506	0.37	ray::DefaultActor.run
21512	0.37	ray::DefaultActor.run
21516	0.37	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21025	0.08	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,348 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.52	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.26	ray::DefaultActor
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.22	ray::DefaultActor
21505	0.22	ray::DefaultActor
21499	0.22	ray::DefaultActor
21508	0.22	ray::DefaultActor
21504	0.22	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,348 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.52	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.26	ray::DefaultActor
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.22	ray::DefaultActor
21505	0.22	ray::DefaultActor
21499	0.22	ray::DefaultActor
21508	0.22	ray::DefaultActor
21504	0.22	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,349 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.18	ray::IDLE
21516	0.18	ray::IDLE
21505	0.18	ray::IDLE
21507	0.18	ray::IDLE
21499	0.18	ray::IDLE
21504	0.18	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,349 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.18	ray::IDLE
21516	0.18	ray::IDLE
21505	0.18	ray::IDLE
21507	0.18	ray::IDLE
21499	0.18	ray::IDLE
21504	0.18	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,350 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.72	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.27	ray::DefaultActor.run
21512	0.24	ray::DefaultActor.run
21515	0.24	ray::DefaultActor.run
21508	0.23	ray::DefaultActor.run
21509	0.23	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21507	0.22	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,350 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.72	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.27	ray::DefaultActor.run
21512	0.24	ray::DefaultActor.run
21515	0.24	ray::DefaultActor.run
21508	0.23	ray::DefaultActor.run
21509	0.23	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21507	0.22	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,354 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.16	ray::IDLE
21516	0.16	ray::IDLE
21511	0.16	ray::IDLE
21507	0.16	ray::IDLE
21504	0.16	ray::IDLE
21505	0.16	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,354 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.16	ray::IDLE
21516	0.16	ray::IDLE
21511	0.16	ray::IDLE
21507	0.16	ray::IDLE
21504	0.16	ray::IDLE
21505	0.16	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,355 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.17	ray::IDLE
21512	0.17	ray::IDLE
21507	0.17	ray::IDLE
21505	0.17	ray::IDLE
21511	0.17	ray::IDLE
21504	0.17	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,355 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.17	ray::IDLE
21512	0.17	ray::IDLE
21507	0.17	ray::IDLE
21505	0.17	ray::IDLE
21511	0.17	ray::IDLE
21504	0.17	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,357 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.29	ray::DefaultActor.run
21508	0.28	ray::DefaultActor.run
21507	0.27	ray::DefaultActor.run
21506	0.27	ray::DefaultActor.run
21512	0.27	ray::DefaultActor.run
21505	0.27	ray::DefaultActor.run
21504	0.26	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,357 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.19	ray::IDLE
21516	0.19	ray::IDLE
21507	0.19	ray::IDLE
21505	0.19	ray::IDLE
21504	0.19	ray::IDLE
21499	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,357 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.29	ray::DefaultActor.run
21508	0.28	ray::DefaultActor.run
21507	0.27	ray::DefaultActor.run
21506	0.27	ray::DefaultActor.run
21512	0.27	ray::DefaultActor.run
21505	0.27	ray::DefaultActor.run
21504	0.26	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,357 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.19	ray::IDLE
21516	0.19	ray::IDLE
21507	0.19	ray::IDLE
21505	0.19	ray::IDLE
21504	0.19	ray::IDLE
21499	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,358 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.75	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.31	ray::DefaultActor.run
21505	0.31	ray::DefaultActor.run
21512	0.31	ray::DefaultActor.run
21506	0.30	ray::DefaultActor.run
21516	0.30	ray::DefaultActor.run
21504	0.30	ray::DefaultActor.run
21503	0.29	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,358 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.75	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.31	ray::DefaultActor.run
21505	0.31	ray::DefaultActor.run
21512	0.31	ray::DefaultActor.run
21506	0.30	ray::DefaultActor.run
21516	0.30	ray::DefaultActor.run
21504	0.30	ray::DefaultActor.run
21503	0.29	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,359 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.28	ray::DefaultActor.run
21515	0.26	ray::DefaultActor.run
21508	0.26	ray::DefaultActor.run
21512	0.26	ray::DefaultActor.run
21507	0.24	ray::DefaultActor.run
21504	0.24	ray::DefaultActor.run
21505	0.24	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,359 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.28	ray::DefaultActor.run
21515	0.26	ray::DefaultActor.run
21508	0.26	ray::DefaultActor.run
21512	0.26	ray::DefaultActor.run
21507	0.24	ray::DefaultActor.run
21504	0.24	ray::DefaultActor.run
21505	0.24	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,360 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.52	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.26	ray::DefaultActor
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.22	ray::DefaultActor
21505	0.22	ray::DefaultActor
21499	0.22	ray::DefaultActor
21508	0.22	ray::DefaultActor
21504	0.22	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,360 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.52	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.26	ray::DefaultActor
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.22	ray::DefaultActor
21505	0.22	ray::DefaultActor
21499	0.22	ray::DefaultActor
21508	0.22	ray::DefaultActor
21504	0.22	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,365 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.16	ray::IDLE
21516	0.16	ray::IDLE
21511	0.16	ray::IDLE
21507	0.16	ray::IDLE
21504	0.16	ray::IDLE
21505	0.16	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,365 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.16	ray::IDLE
21516	0.16	ray::IDLE
21511	0.16	ray::IDLE
21507	0.16	ray::IDLE
21504	0.16	ray::IDLE
21505	0.16	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,367 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.89	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.36	ray::DefaultActor.run
21512	0.35	ray::DefaultActor.run
21508	0.35	ray::DefaultActor
21505	0.35	ray::DefaultActor
21516	0.34	ray::DefaultActor
21503	0.34	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,367 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.89	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.36	ray::DefaultActor.run
21512	0.35	ray::DefaultActor.run
21508	0.35	ray::DefaultActor
21505	0.35	ray::DefaultActor
21516	0.34	ray::DefaultActor
21503	0.34	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,369 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.29	ray::DefaultActor.run
21508	0.28	ray::DefaultActor.run
21507	0.27	ray::DefaultActor.run
21506	0.27	ray::DefaultActor.run
21512	0.27	ray::DefaultActor.run
21505	0.27	ray::DefaultActor.run
21504	0.26	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,369 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.29	ray::DefaultActor.run
21508	0.28	ray::DefaultActor.run
21507	0.27	ray::DefaultActor.run
21506	0.27	ray::DefaultActor.run
21512	0.27	ray::DefaultActor.run
21505	0.27	ray::DefaultActor.run
21504	0.26	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,370 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.75	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.31	ray::DefaultActor.run
21505	0.31	ray::DefaultActor.run
21512	0.31	ray::DefaultActor.run
21506	0.30	ray::DefaultActor.run
21516	0.30	ray::DefaultActor.run
21504	0.30	ray::DefaultActor.run
21503	0.29	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,370 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.75	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.31	ray::DefaultActor.run
21505	0.31	ray::DefaultActor.run
21512	0.31	ray::DefaultActor.run
21506	0.30	ray::DefaultActor.run
21516	0.30	ray::DefaultActor.run
21504	0.30	ray::DefaultActor.run
21503	0.29	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,371 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.28	ray::DefaultActor.run
21515	0.26	ray::DefaultActor.run
21508	0.26	ray::DefaultActor.run
21512	0.26	ray::DefaultActor.run
21507	0.24	ray::DefaultActor.run
21504	0.24	ray::DefaultActor.run
21505	0.24	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,371 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.28	ray::DefaultActor.run
21515	0.26	ray::DefaultActor.run
21508	0.26	ray::DefaultActor.run
21512	0.26	ray::DefaultActor.run
21507	0.24	ray::DefaultActor.run
21504	0.24	ray::DefaultActor.run
21505	0.24	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,373 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.89	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.36	ray::DefaultActor.run
21512	0.35	ray::DefaultActor.run
21508	0.35	ray::DefaultActor
21505	0.35	ray::DefaultActor
21516	0.34	ray::DefaultActor
21503	0.34	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,373 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.89	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.36	ray::DefaultActor.run
21512	0.35	ray::DefaultActor.run
21508	0.35	ray::DefaultActor
21505	0.35	ray::DefaultActor
21516	0.34	ray::DefaultActor
21503	0.34	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | DEBUG flwr 2024-03-05 15:04:15,379 | server.py:187 | evaluate_round 6 received 0 results and 19 failures
myFlowerExperiment | DEBUG flwr 2024-03-05 15:04:15,379 | server.py:187 | evaluate_round 6 received 0 results and 19 failures
myFlowerExperiment | DEBUG flwr 2024-03-05 15:04:15,381 | server.py:222 | fit_round 7: strategy sampled 19 clients (out of 19)
myFlowerExperiment | DEBUG flwr 2024-03-05 15:04:15,381 | server.py:222 | fit_round 7: strategy sampled 19 clients (out of 19)
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,611 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.89	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.36	ray::DefaultActor.run
21512	0.35	ray::DefaultActor.run
21508	0.35	ray::DefaultActor
21505	0.35	ray::DefaultActor
21516	0.34	ray::DefaultActor
21503	0.34	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,611 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.89	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.36	ray::DefaultActor.run
21512	0.35	ray::DefaultActor.run
21508	0.35	ray::DefaultActor
21505	0.35	ray::DefaultActor
21516	0.34	ray::DefaultActor
21503	0.34	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,629 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.89	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.36	ray::DefaultActor.run
21512	0.35	ray::DefaultActor.run
21508	0.35	ray::DefaultActor
21505	0.35	ray::DefaultActor
21516	0.34	ray::DefaultActor
21503	0.34	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,629 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.89	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.36	ray::DefaultActor.run
21512	0.35	ray::DefaultActor.run
21508	0.35	ray::DefaultActor
21505	0.35	ray::DefaultActor
21516	0.34	ray::DefaultActor
21503	0.34	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,648 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.29	ray::DefaultActor.run
21508	0.28	ray::DefaultActor.run
21507	0.27	ray::DefaultActor.run
21506	0.27	ray::DefaultActor.run
21512	0.27	ray::DefaultActor.run
21505	0.27	ray::DefaultActor.run
21504	0.26	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,648 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.29	ray::DefaultActor.run
21508	0.28	ray::DefaultActor.run
21507	0.27	ray::DefaultActor.run
21506	0.27	ray::DefaultActor.run
21512	0.27	ray::DefaultActor.run
21505	0.27	ray::DefaultActor.run
21504	0.26	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,650 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.28	ray::DefaultActor.run
21515	0.26	ray::DefaultActor.run
21508	0.26	ray::DefaultActor.run
21512	0.26	ray::DefaultActor.run
21507	0.24	ray::DefaultActor.run
21504	0.24	ray::DefaultActor.run
21505	0.24	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,650 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.28	ray::DefaultActor.run
21515	0.26	ray::DefaultActor.run
21508	0.26	ray::DefaultActor.run
21512	0.26	ray::DefaultActor.run
21507	0.24	ray::DefaultActor.run
21504	0.24	ray::DefaultActor.run
21505	0.24	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,653 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.16	ray::IDLE
21516	0.16	ray::IDLE
21511	0.16	ray::IDLE
21507	0.16	ray::IDLE
21504	0.16	ray::IDLE
21505	0.16	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,653 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.16	ray::IDLE
21516	0.16	ray::IDLE
21511	0.16	ray::IDLE
21507	0.16	ray::IDLE
21504	0.16	ray::IDLE
21505	0.16	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,654 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.29	ray::DefaultActor.run
21508	0.28	ray::DefaultActor.run
21507	0.27	ray::DefaultActor.run
21506	0.27	ray::DefaultActor.run
21512	0.27	ray::DefaultActor.run
21505	0.27	ray::DefaultActor.run
21504	0.26	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,654 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.29	ray::DefaultActor.run
21508	0.28	ray::DefaultActor.run
21507	0.27	ray::DefaultActor.run
21506	0.27	ray::DefaultActor.run
21512	0.27	ray::DefaultActor.run
21505	0.27	ray::DefaultActor.run
21504	0.26	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,654 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.99	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.38	ray::DefaultActor.run
21505	0.38	ray::DefaultActor.run
21506	0.37	ray::DefaultActor.run
21512	0.37	ray::DefaultActor.run
21516	0.37	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21025	0.08	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,654 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.99	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.38	ray::DefaultActor.run
21505	0.38	ray::DefaultActor.run
21506	0.37	ray::DefaultActor.run
21512	0.37	ray::DefaultActor.run
21516	0.37	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21025	0.08	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,655 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.28	ray::DefaultActor.run
21515	0.26	ray::DefaultActor.run
21508	0.26	ray::DefaultActor.run
21512	0.26	ray::DefaultActor.run
21507	0.24	ray::DefaultActor.run
21504	0.24	ray::DefaultActor.run
21505	0.24	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,655 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.28	ray::DefaultActor.run
21515	0.26	ray::DefaultActor.run
21508	0.26	ray::DefaultActor.run
21512	0.26	ray::DefaultActor.run
21507	0.24	ray::DefaultActor.run
21504	0.24	ray::DefaultActor.run
21505	0.24	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,656 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.75	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.31	ray::DefaultActor.run
21505	0.31	ray::DefaultActor.run
21512	0.31	ray::DefaultActor.run
21506	0.30	ray::DefaultActor.run
21516	0.30	ray::DefaultActor.run
21504	0.30	ray::DefaultActor.run
21503	0.29	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,656 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.75	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.31	ray::DefaultActor.run
21505	0.31	ray::DefaultActor.run
21512	0.31	ray::DefaultActor.run
21506	0.30	ray::DefaultActor.run
21516	0.30	ray::DefaultActor.run
21504	0.30	ray::DefaultActor.run
21503	0.29	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,656 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.16	ray::IDLE
21516	0.16	ray::IDLE
21511	0.16	ray::IDLE
21507	0.16	ray::IDLE
21504	0.16	ray::IDLE
21505	0.16	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,656 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.16	ray::IDLE
21516	0.16	ray::IDLE
21511	0.16	ray::IDLE
21507	0.16	ray::IDLE
21504	0.16	ray::IDLE
21505	0.16	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,657 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.52	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.26	ray::DefaultActor
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.22	ray::DefaultActor
21505	0.22	ray::DefaultActor
21499	0.22	ray::DefaultActor
21508	0.22	ray::DefaultActor
21504	0.22	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,657 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.52	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.26	ray::DefaultActor
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.22	ray::DefaultActor
21505	0.22	ray::DefaultActor
21499	0.22	ray::DefaultActor
21508	0.22	ray::DefaultActor
21504	0.22	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,659 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.17	ray::IDLE
21512	0.17	ray::IDLE
21507	0.17	ray::IDLE
21505	0.17	ray::IDLE
21511	0.17	ray::IDLE
21504	0.17	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,659 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.17	ray::IDLE
21512	0.17	ray::IDLE
21507	0.17	ray::IDLE
21505	0.17	ray::IDLE
21511	0.17	ray::IDLE
21504	0.17	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,661 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.72	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.27	ray::DefaultActor.run
21512	0.24	ray::DefaultActor.run
21515	0.24	ray::DefaultActor.run
21508	0.23	ray::DefaultActor.run
21509	0.23	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21507	0.22	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,661 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.72	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.27	ray::DefaultActor.run
21512	0.24	ray::DefaultActor.run
21515	0.24	ray::DefaultActor.run
21508	0.23	ray::DefaultActor.run
21509	0.23	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21507	0.22	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,661 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.89	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.36	ray::DefaultActor.run
21512	0.35	ray::DefaultActor.run
21508	0.35	ray::DefaultActor
21505	0.35	ray::DefaultActor
21516	0.34	ray::DefaultActor
21503	0.34	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,661 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.89	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.36	ray::DefaultActor.run
21512	0.35	ray::DefaultActor.run
21508	0.35	ray::DefaultActor
21505	0.35	ray::DefaultActor
21516	0.34	ray::DefaultActor
21503	0.34	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,661 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.21	ray::IDLE
21512	0.21	ray::IDLE
21507	0.21	ray::IDLE
21505	0.21	ray::IDLE
21504	0.21	ray::IDLE
21515	0.21	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,661 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.21	ray::IDLE
21512	0.21	ray::IDLE
21507	0.21	ray::IDLE
21505	0.21	ray::IDLE
21504	0.21	ray::IDLE
21515	0.21	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,662 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.18	ray::IDLE
21516	0.18	ray::IDLE
21505	0.18	ray::IDLE
21507	0.18	ray::IDLE
21499	0.18	ray::IDLE
21504	0.18	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,662 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.18	ray::IDLE
21516	0.18	ray::IDLE
21505	0.18	ray::IDLE
21507	0.18	ray::IDLE
21499	0.18	ray::IDLE
21504	0.18	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,665 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.75	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.31	ray::DefaultActor.run
21505	0.31	ray::DefaultActor.run
21512	0.31	ray::DefaultActor.run
21506	0.30	ray::DefaultActor.run
21516	0.30	ray::DefaultActor.run
21504	0.30	ray::DefaultActor.run
21503	0.29	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,665 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.75	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.31	ray::DefaultActor.run
21505	0.31	ray::DefaultActor.run
21512	0.31	ray::DefaultActor.run
21506	0.30	ray::DefaultActor.run
21516	0.30	ray::DefaultActor.run
21504	0.30	ray::DefaultActor.run
21503	0.29	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,665 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.99	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.38	ray::DefaultActor.run
21505	0.38	ray::DefaultActor.run
21506	0.37	ray::DefaultActor.run
21512	0.37	ray::DefaultActor.run
21516	0.37	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21025	0.08	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,665 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.99	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.38	ray::DefaultActor.run
21505	0.38	ray::DefaultActor.run
21506	0.37	ray::DefaultActor.run
21512	0.37	ray::DefaultActor.run
21516	0.37	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21025	0.08	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,667 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.19	ray::IDLE
21516	0.19	ray::IDLE
21507	0.19	ray::IDLE
21505	0.19	ray::IDLE
21504	0.19	ray::IDLE
21499	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,667 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.19	ray::IDLE
21516	0.19	ray::IDLE
21507	0.19	ray::IDLE
21505	0.19	ray::IDLE
21504	0.19	ray::IDLE
21499	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,667 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.29	ray::DefaultActor.run
21508	0.28	ray::DefaultActor.run
21507	0.27	ray::DefaultActor.run
21506	0.27	ray::DefaultActor.run
21512	0.27	ray::DefaultActor.run
21505	0.27	ray::DefaultActor.run
21504	0.26	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,667 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.29	ray::DefaultActor.run
21508	0.28	ray::DefaultActor.run
21507	0.27	ray::DefaultActor.run
21506	0.27	ray::DefaultActor.run
21512	0.27	ray::DefaultActor.run
21505	0.27	ray::DefaultActor.run
21504	0.26	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,668 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.75	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.31	ray::DefaultActor.run
21505	0.31	ray::DefaultActor.run
21512	0.31	ray::DefaultActor.run
21506	0.30	ray::DefaultActor.run
21516	0.30	ray::DefaultActor.run
21504	0.30	ray::DefaultActor.run
21503	0.29	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,668 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.75	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.31	ray::DefaultActor.run
21505	0.31	ray::DefaultActor.run
21512	0.31	ray::DefaultActor.run
21506	0.30	ray::DefaultActor.run
21516	0.30	ray::DefaultActor.run
21504	0.30	ray::DefaultActor.run
21503	0.29	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,672 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.52	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.26	ray::DefaultActor
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.22	ray::DefaultActor
21505	0.22	ray::DefaultActor
21499	0.22	ray::DefaultActor
21508	0.22	ray::DefaultActor
21504	0.22	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,672 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.52	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.26	ray::DefaultActor
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.22	ray::DefaultActor
21505	0.22	ray::DefaultActor
21499	0.22	ray::DefaultActor
21508	0.22	ray::DefaultActor
21504	0.22	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,674 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.17	ray::IDLE
21512	0.17	ray::IDLE
21507	0.17	ray::IDLE
21505	0.17	ray::IDLE
21511	0.17	ray::IDLE
21504	0.17	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,674 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.17	ray::IDLE
21512	0.17	ray::IDLE
21507	0.17	ray::IDLE
21505	0.17	ray::IDLE
21511	0.17	ray::IDLE
21504	0.17	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,675 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.72	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.27	ray::DefaultActor.run
21512	0.24	ray::DefaultActor.run
21515	0.24	ray::DefaultActor.run
21508	0.23	ray::DefaultActor.run
21509	0.23	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21507	0.22	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,675 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.72	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.27	ray::DefaultActor.run
21512	0.24	ray::DefaultActor.run
21515	0.24	ray::DefaultActor.run
21508	0.23	ray::DefaultActor.run
21509	0.23	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21507	0.22	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,678 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.89	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.36	ray::DefaultActor.run
21512	0.35	ray::DefaultActor.run
21508	0.35	ray::DefaultActor
21505	0.35	ray::DefaultActor
21516	0.34	ray::DefaultActor
21503	0.34	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,678 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.89	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.36	ray::DefaultActor.run
21512	0.35	ray::DefaultActor.run
21508	0.35	ray::DefaultActor
21505	0.35	ray::DefaultActor
21516	0.34	ray::DefaultActor
21503	0.34	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,678 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.21	ray::IDLE
21512	0.21	ray::IDLE
21507	0.21	ray::IDLE
21505	0.21	ray::IDLE
21504	0.21	ray::IDLE
21515	0.21	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,678 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.21	ray::IDLE
21512	0.21	ray::IDLE
21507	0.21	ray::IDLE
21505	0.21	ray::IDLE
21504	0.21	ray::IDLE
21515	0.21	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,681 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.18	ray::IDLE
21516	0.18	ray::IDLE
21505	0.18	ray::IDLE
21507	0.18	ray::IDLE
21499	0.18	ray::IDLE
21504	0.18	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,681 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.18	ray::IDLE
21516	0.18	ray::IDLE
21505	0.18	ray::IDLE
21507	0.18	ray::IDLE
21499	0.18	ray::IDLE
21504	0.18	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,683 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.75	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.31	ray::DefaultActor.run
21505	0.31	ray::DefaultActor.run
21512	0.31	ray::DefaultActor.run
21506	0.30	ray::DefaultActor.run
21516	0.30	ray::DefaultActor.run
21504	0.30	ray::DefaultActor.run
21503	0.29	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,683 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.75	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.31	ray::DefaultActor.run
21505	0.31	ray::DefaultActor.run
21512	0.31	ray::DefaultActor.run
21506	0.30	ray::DefaultActor.run
21516	0.30	ray::DefaultActor.run
21504	0.30	ray::DefaultActor.run
21503	0.29	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,687 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.19	ray::IDLE
21516	0.19	ray::IDLE
21507	0.19	ray::IDLE
21505	0.19	ray::IDLE
21504	0.19	ray::IDLE
21499	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,687 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.19	ray::IDLE
21516	0.19	ray::IDLE
21507	0.19	ray::IDLE
21505	0.19	ray::IDLE
21504	0.19	ray::IDLE
21499	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,689 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.29	ray::DefaultActor.run
21508	0.28	ray::DefaultActor.run
21507	0.27	ray::DefaultActor.run
21506	0.27	ray::DefaultActor.run
21512	0.27	ray::DefaultActor.run
21505	0.27	ray::DefaultActor.run
21504	0.26	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:15,689 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.29	ray::DefaultActor.run
21508	0.28	ray::DefaultActor.run
21507	0.27	ray::DefaultActor.run
21506	0.27	ray::DefaultActor.run
21512	0.27	ray::DefaultActor.run
21505	0.27	ray::DefaultActor.run
21504	0.26	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | DEBUG flwr 2024-03-05 15:04:23,406 | server.py:236 | fit_round 7 received 4 results and 15 failures
myFlowerExperiment | DEBUG flwr 2024-03-05 15:04:23,406 | server.py:236 | fit_round 7 received 4 results and 15 failures
myFlowerExperiment | INFO flwr 2024-03-05 15:04:28,988 | server.py:125 | fit progress: (7, 0.7352955341339111, {'accuracy': 0.559216320514679}, 102.73729575899779)
myFlowerExperiment | INFO flwr 2024-03-05 15:04:28,988 | server.py:125 | fit progress: (7, 0.7352955341339111, {'accuracy': 0.559216320514679}, 102.73729575899779)
myFlowerExperiment | DEBUG flwr 2024-03-05 15:04:28,990 | server.py:173 | evaluate_round 7: strategy sampled 19 clients (out of 19)
myFlowerExperiment | DEBUG flwr 2024-03-05 15:04:28,990 | server.py:173 | evaluate_round 7: strategy sampled 19 clients (out of 19)
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,191 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.29	ray::DefaultActor.run
21508	0.28	ray::DefaultActor.run
21507	0.27	ray::DefaultActor.run
21506	0.27	ray::DefaultActor.run
21512	0.27	ray::DefaultActor.run
21505	0.27	ray::DefaultActor.run
21504	0.26	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,191 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.29	ray::DefaultActor.run
21508	0.28	ray::DefaultActor.run
21507	0.27	ray::DefaultActor.run
21506	0.27	ray::DefaultActor.run
21512	0.27	ray::DefaultActor.run
21505	0.27	ray::DefaultActor.run
21504	0.26	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,202 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.29	ray::DefaultActor.run
21508	0.28	ray::DefaultActor.run
21507	0.27	ray::DefaultActor.run
21506	0.27	ray::DefaultActor.run
21512	0.27	ray::DefaultActor.run
21505	0.27	ray::DefaultActor.run
21504	0.26	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,202 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.29	ray::DefaultActor.run
21508	0.28	ray::DefaultActor.run
21507	0.27	ray::DefaultActor.run
21506	0.27	ray::DefaultActor.run
21512	0.27	ray::DefaultActor.run
21505	0.27	ray::DefaultActor.run
21504	0.26	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,210 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.75	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.31	ray::DefaultActor.run
21505	0.31	ray::DefaultActor.run
21512	0.31	ray::DefaultActor.run
21506	0.30	ray::DefaultActor.run
21516	0.30	ray::DefaultActor.run
21504	0.30	ray::DefaultActor.run
21503	0.29	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,210 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.75	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.31	ray::DefaultActor.run
21505	0.31	ray::DefaultActor.run
21512	0.31	ray::DefaultActor.run
21506	0.30	ray::DefaultActor.run
21516	0.30	ray::DefaultActor.run
21504	0.30	ray::DefaultActor.run
21503	0.29	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,210 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.72	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.27	ray::DefaultActor.run
21512	0.24	ray::DefaultActor.run
21515	0.24	ray::DefaultActor.run
21508	0.23	ray::DefaultActor.run
21509	0.23	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21507	0.22	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,210 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.72	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.27	ray::DefaultActor.run
21512	0.24	ray::DefaultActor.run
21515	0.24	ray::DefaultActor.run
21508	0.23	ray::DefaultActor.run
21509	0.23	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21507	0.22	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,211 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.19	ray::IDLE
21516	0.19	ray::IDLE
21507	0.19	ray::IDLE
21505	0.19	ray::IDLE
21504	0.19	ray::IDLE
21499	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,211 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.19	ray::IDLE
21516	0.19	ray::IDLE
21507	0.19	ray::IDLE
21505	0.19	ray::IDLE
21504	0.19	ray::IDLE
21499	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,215 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.75	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.31	ray::DefaultActor.run
21505	0.31	ray::DefaultActor.run
21512	0.31	ray::DefaultActor.run
21506	0.30	ray::DefaultActor.run
21516	0.30	ray::DefaultActor.run
21504	0.30	ray::DefaultActor.run
21503	0.29	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,215 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.75	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.31	ray::DefaultActor.run
21505	0.31	ray::DefaultActor.run
21512	0.31	ray::DefaultActor.run
21506	0.30	ray::DefaultActor.run
21516	0.30	ray::DefaultActor.run
21504	0.30	ray::DefaultActor.run
21503	0.29	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,215 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.99	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.38	ray::DefaultActor.run
21505	0.38	ray::DefaultActor.run
21506	0.37	ray::DefaultActor.run
21512	0.37	ray::DefaultActor.run
21516	0.37	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21025	0.08	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,215 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.99	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.38	ray::DefaultActor.run
21505	0.38	ray::DefaultActor.run
21506	0.37	ray::DefaultActor.run
21512	0.37	ray::DefaultActor.run
21516	0.37	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21025	0.08	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,216 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.28	ray::DefaultActor.run
21515	0.26	ray::DefaultActor.run
21508	0.26	ray::DefaultActor.run
21512	0.26	ray::DefaultActor.run
21507	0.24	ray::DefaultActor.run
21504	0.24	ray::DefaultActor.run
21505	0.24	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,216 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.28	ray::DefaultActor.run
21515	0.26	ray::DefaultActor.run
21508	0.26	ray::DefaultActor.run
21512	0.26	ray::DefaultActor.run
21507	0.24	ray::DefaultActor.run
21504	0.24	ray::DefaultActor.run
21505	0.24	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,217 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.17	ray::IDLE
21512	0.17	ray::IDLE
21507	0.17	ray::IDLE
21505	0.17	ray::IDLE
21511	0.17	ray::IDLE
21504	0.17	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,217 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.17	ray::IDLE
21512	0.17	ray::IDLE
21507	0.17	ray::IDLE
21505	0.17	ray::IDLE
21511	0.17	ray::IDLE
21504	0.17	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,217 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.16	ray::IDLE
21516	0.16	ray::IDLE
21511	0.16	ray::IDLE
21507	0.16	ray::IDLE
21504	0.16	ray::IDLE
21505	0.16	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,217 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.16	ray::IDLE
21516	0.16	ray::IDLE
21511	0.16	ray::IDLE
21507	0.16	ray::IDLE
21504	0.16	ray::IDLE
21505	0.16	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,218 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.72	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.27	ray::DefaultActor.run
21512	0.24	ray::DefaultActor.run
21515	0.24	ray::DefaultActor.run
21508	0.23	ray::DefaultActor.run
21509	0.23	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21507	0.22	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,218 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.72	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.27	ray::DefaultActor.run
21512	0.24	ray::DefaultActor.run
21515	0.24	ray::DefaultActor.run
21508	0.23	ray::DefaultActor.run
21509	0.23	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21507	0.22	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,219 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.19	ray::IDLE
21516	0.19	ray::IDLE
21507	0.19	ray::IDLE
21505	0.19	ray::IDLE
21504	0.19	ray::IDLE
21499	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,219 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.19	ray::IDLE
21516	0.19	ray::IDLE
21507	0.19	ray::IDLE
21505	0.19	ray::IDLE
21504	0.19	ray::IDLE
21499	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,225 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.18	ray::IDLE
21516	0.18	ray::IDLE
21505	0.18	ray::IDLE
21507	0.18	ray::IDLE
21499	0.18	ray::IDLE
21504	0.18	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,225 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.18	ray::IDLE
21516	0.18	ray::IDLE
21505	0.18	ray::IDLE
21507	0.18	ray::IDLE
21499	0.18	ray::IDLE
21504	0.18	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,225 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.99	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.38	ray::DefaultActor.run
21505	0.38	ray::DefaultActor.run
21506	0.37	ray::DefaultActor.run
21512	0.37	ray::DefaultActor.run
21516	0.37	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21025	0.08	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,225 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.99	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.38	ray::DefaultActor.run
21505	0.38	ray::DefaultActor.run
21506	0.37	ray::DefaultActor.run
21512	0.37	ray::DefaultActor.run
21516	0.37	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21025	0.08	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,225 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.89	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.36	ray::DefaultActor.run
21512	0.35	ray::DefaultActor.run
21508	0.35	ray::DefaultActor
21505	0.35	ray::DefaultActor
21516	0.34	ray::DefaultActor
21503	0.34	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,225 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.89	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.36	ray::DefaultActor.run
21512	0.35	ray::DefaultActor.run
21508	0.35	ray::DefaultActor
21505	0.35	ray::DefaultActor
21516	0.34	ray::DefaultActor
21503	0.34	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,226 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.52	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.26	ray::DefaultActor
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.22	ray::DefaultActor
21505	0.22	ray::DefaultActor
21499	0.22	ray::DefaultActor
21508	0.22	ray::DefaultActor
21504	0.22	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,226 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.52	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.26	ray::DefaultActor
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.22	ray::DefaultActor
21505	0.22	ray::DefaultActor
21499	0.22	ray::DefaultActor
21508	0.22	ray::DefaultActor
21504	0.22	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,227 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.28	ray::DefaultActor.run
21515	0.26	ray::DefaultActor.run
21508	0.26	ray::DefaultActor.run
21512	0.26	ray::DefaultActor.run
21507	0.24	ray::DefaultActor.run
21504	0.24	ray::DefaultActor.run
21505	0.24	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,227 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.28	ray::DefaultActor.run
21515	0.26	ray::DefaultActor.run
21508	0.26	ray::DefaultActor.run
21512	0.26	ray::DefaultActor.run
21507	0.24	ray::DefaultActor.run
21504	0.24	ray::DefaultActor.run
21505	0.24	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,228 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.29	ray::DefaultActor.run
21508	0.28	ray::DefaultActor.run
21507	0.27	ray::DefaultActor.run
21506	0.27	ray::DefaultActor.run
21512	0.27	ray::DefaultActor.run
21505	0.27	ray::DefaultActor.run
21504	0.26	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,228 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.29	ray::DefaultActor.run
21508	0.28	ray::DefaultActor.run
21507	0.27	ray::DefaultActor.run
21506	0.27	ray::DefaultActor.run
21512	0.27	ray::DefaultActor.run
21505	0.27	ray::DefaultActor.run
21504	0.26	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,229 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.17	ray::IDLE
21512	0.17	ray::IDLE
21507	0.17	ray::IDLE
21505	0.17	ray::IDLE
21511	0.17	ray::IDLE
21504	0.17	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,229 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.17	ray::IDLE
21512	0.17	ray::IDLE
21507	0.17	ray::IDLE
21505	0.17	ray::IDLE
21511	0.17	ray::IDLE
21504	0.17	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,230 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.16	ray::IDLE
21516	0.16	ray::IDLE
21511	0.16	ray::IDLE
21507	0.16	ray::IDLE
21504	0.16	ray::IDLE
21505	0.16	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,230 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.16	ray::IDLE
21516	0.16	ray::IDLE
21511	0.16	ray::IDLE
21507	0.16	ray::IDLE
21504	0.16	ray::IDLE
21505	0.16	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,232 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.21	ray::IDLE
21512	0.21	ray::IDLE
21507	0.21	ray::IDLE
21505	0.21	ray::IDLE
21504	0.21	ray::IDLE
21515	0.21	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,232 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.21	ray::IDLE
21512	0.21	ray::IDLE
21507	0.21	ray::IDLE
21505	0.21	ray::IDLE
21504	0.21	ray::IDLE
21515	0.21	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,234 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.18	ray::IDLE
21516	0.18	ray::IDLE
21505	0.18	ray::IDLE
21507	0.18	ray::IDLE
21499	0.18	ray::IDLE
21504	0.18	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,234 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.18	ray::IDLE
21516	0.18	ray::IDLE
21505	0.18	ray::IDLE
21507	0.18	ray::IDLE
21499	0.18	ray::IDLE
21504	0.18	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,237 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.89	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.36	ray::DefaultActor.run
21512	0.35	ray::DefaultActor.run
21508	0.35	ray::DefaultActor
21505	0.35	ray::DefaultActor
21516	0.34	ray::DefaultActor
21503	0.34	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,237 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.89	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.36	ray::DefaultActor.run
21512	0.35	ray::DefaultActor.run
21508	0.35	ray::DefaultActor
21505	0.35	ray::DefaultActor
21516	0.34	ray::DefaultActor
21503	0.34	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,239 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.52	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.26	ray::DefaultActor
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.22	ray::DefaultActor
21505	0.22	ray::DefaultActor
21499	0.22	ray::DefaultActor
21508	0.22	ray::DefaultActor
21504	0.22	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,239 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.52	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.26	ray::DefaultActor
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.22	ray::DefaultActor
21505	0.22	ray::DefaultActor
21499	0.22	ray::DefaultActor
21508	0.22	ray::DefaultActor
21504	0.22	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,242 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.29	ray::DefaultActor.run
21508	0.28	ray::DefaultActor.run
21507	0.27	ray::DefaultActor.run
21506	0.27	ray::DefaultActor.run
21512	0.27	ray::DefaultActor.run
21505	0.27	ray::DefaultActor.run
21504	0.26	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,245 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.21	ray::IDLE
21512	0.21	ray::IDLE
21507	0.21	ray::IDLE
21505	0.21	ray::IDLE
21504	0.21	ray::IDLE
21515	0.21	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,242 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.29	ray::DefaultActor.run
21508	0.28	ray::DefaultActor.run
21507	0.27	ray::DefaultActor.run
21506	0.27	ray::DefaultActor.run
21512	0.27	ray::DefaultActor.run
21505	0.27	ray::DefaultActor.run
21504	0.26	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,245 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.21	ray::IDLE
21512	0.21	ray::IDLE
21507	0.21	ray::IDLE
21505	0.21	ray::IDLE
21504	0.21	ray::IDLE
21515	0.21	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | DEBUG flwr 2024-03-05 15:04:29,252 | server.py:187 | evaluate_round 7 received 0 results and 19 failures
myFlowerExperiment | DEBUG flwr 2024-03-05 15:04:29,252 | server.py:187 | evaluate_round 7 received 0 results and 19 failures
myFlowerExperiment | DEBUG flwr 2024-03-05 15:04:29,254 | server.py:222 | fit_round 8: strategy sampled 19 clients (out of 19)
myFlowerExperiment | DEBUG flwr 2024-03-05 15:04:29,254 | server.py:222 | fit_round 8: strategy sampled 19 clients (out of 19)
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,487 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.21	ray::IDLE
21512	0.21	ray::IDLE
21507	0.21	ray::IDLE
21505	0.21	ray::IDLE
21504	0.21	ray::IDLE
21515	0.21	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,487 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.21	ray::IDLE
21512	0.21	ray::IDLE
21507	0.21	ray::IDLE
21505	0.21	ray::IDLE
21504	0.21	ray::IDLE
21515	0.21	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,499 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.21	ray::IDLE
21512	0.21	ray::IDLE
21507	0.21	ray::IDLE
21505	0.21	ray::IDLE
21504	0.21	ray::IDLE
21515	0.21	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,499 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.21	ray::IDLE
21512	0.21	ray::IDLE
21507	0.21	ray::IDLE
21505	0.21	ray::IDLE
21504	0.21	ray::IDLE
21515	0.21	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,526 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.52	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.26	ray::DefaultActor
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.22	ray::DefaultActor
21505	0.22	ray::DefaultActor
21499	0.22	ray::DefaultActor
21508	0.22	ray::DefaultActor
21504	0.22	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,526 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.52	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.26	ray::DefaultActor
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.22	ray::DefaultActor
21505	0.22	ray::DefaultActor
21499	0.22	ray::DefaultActor
21508	0.22	ray::DefaultActor
21504	0.22	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,528 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.29	ray::DefaultActor.run
21508	0.28	ray::DefaultActor.run
21507	0.27	ray::DefaultActor.run
21506	0.27	ray::DefaultActor.run
21512	0.27	ray::DefaultActor.run
21505	0.27	ray::DefaultActor.run
21504	0.26	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,528 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.29	ray::DefaultActor.run
21508	0.28	ray::DefaultActor.run
21507	0.27	ray::DefaultActor.run
21506	0.27	ray::DefaultActor.run
21512	0.27	ray::DefaultActor.run
21505	0.27	ray::DefaultActor.run
21504	0.26	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,529 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.52	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.26	ray::DefaultActor
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.22	ray::DefaultActor
21505	0.22	ray::DefaultActor
21499	0.22	ray::DefaultActor
21508	0.22	ray::DefaultActor
21504	0.22	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,529 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.52	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.26	ray::DefaultActor
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.22	ray::DefaultActor
21505	0.22	ray::DefaultActor
21499	0.22	ray::DefaultActor
21508	0.22	ray::DefaultActor
21504	0.22	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,531 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.29	ray::DefaultActor.run
21508	0.28	ray::DefaultActor.run
21507	0.27	ray::DefaultActor.run
21506	0.27	ray::DefaultActor.run
21512	0.27	ray::DefaultActor.run
21505	0.27	ray::DefaultActor.run
21504	0.26	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,531 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.29	ray::DefaultActor.run
21508	0.28	ray::DefaultActor.run
21507	0.27	ray::DefaultActor.run
21506	0.27	ray::DefaultActor.run
21512	0.27	ray::DefaultActor.run
21505	0.27	ray::DefaultActor.run
21504	0.26	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,534 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.16	ray::IDLE
21516	0.16	ray::IDLE
21511	0.16	ray::IDLE
21507	0.16	ray::IDLE
21504	0.16	ray::IDLE
21505	0.16	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,534 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.16	ray::IDLE
21516	0.16	ray::IDLE
21511	0.16	ray::IDLE
21507	0.16	ray::IDLE
21504	0.16	ray::IDLE
21505	0.16	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,535 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.99	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.38	ray::DefaultActor.run
21505	0.38	ray::DefaultActor.run
21506	0.37	ray::DefaultActor.run
21512	0.37	ray::DefaultActor.run
21516	0.37	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21025	0.08	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,535 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.99	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.38	ray::DefaultActor.run
21505	0.38	ray::DefaultActor.run
21506	0.37	ray::DefaultActor.run
21512	0.37	ray::DefaultActor.run
21516	0.37	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21025	0.08	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,538 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.21	ray::IDLE
21512	0.21	ray::IDLE
21507	0.21	ray::IDLE
21505	0.21	ray::IDLE
21504	0.21	ray::IDLE
21515	0.21	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,538 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.21	ray::IDLE
21512	0.21	ray::IDLE
21507	0.21	ray::IDLE
21505	0.21	ray::IDLE
21504	0.21	ray::IDLE
21515	0.21	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,539 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.28	ray::DefaultActor.run
21515	0.26	ray::DefaultActor.run
21508	0.26	ray::DefaultActor.run
21512	0.26	ray::DefaultActor.run
21507	0.24	ray::DefaultActor.run
21504	0.24	ray::DefaultActor.run
21505	0.24	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,539 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.28	ray::DefaultActor.run
21515	0.26	ray::DefaultActor.run
21508	0.26	ray::DefaultActor.run
21512	0.26	ray::DefaultActor.run
21507	0.24	ray::DefaultActor.run
21504	0.24	ray::DefaultActor.run
21505	0.24	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,539 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.16	ray::IDLE
21516	0.16	ray::IDLE
21511	0.16	ray::IDLE
21507	0.16	ray::IDLE
21504	0.16	ray::IDLE
21505	0.16	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,539 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.16	ray::IDLE
21516	0.16	ray::IDLE
21511	0.16	ray::IDLE
21507	0.16	ray::IDLE
21504	0.16	ray::IDLE
21505	0.16	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,540 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.52	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.26	ray::DefaultActor
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.22	ray::DefaultActor
21505	0.22	ray::DefaultActor
21499	0.22	ray::DefaultActor
21508	0.22	ray::DefaultActor
21504	0.22	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,540 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.52	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.26	ray::DefaultActor
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.22	ray::DefaultActor
21505	0.22	ray::DefaultActor
21499	0.22	ray::DefaultActor
21508	0.22	ray::DefaultActor
21504	0.22	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,540 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.19	ray::IDLE
21516	0.19	ray::IDLE
21507	0.19	ray::IDLE
21505	0.19	ray::IDLE
21504	0.19	ray::IDLE
21499	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,540 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.19	ray::IDLE
21516	0.19	ray::IDLE
21507	0.19	ray::IDLE
21505	0.19	ray::IDLE
21504	0.19	ray::IDLE
21499	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,541 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.18	ray::IDLE
21516	0.18	ray::IDLE
21505	0.18	ray::IDLE
21507	0.18	ray::IDLE
21499	0.18	ray::IDLE
21504	0.18	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,541 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.18	ray::IDLE
21516	0.18	ray::IDLE
21505	0.18	ray::IDLE
21507	0.18	ray::IDLE
21499	0.18	ray::IDLE
21504	0.18	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,541 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.99	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.38	ray::DefaultActor.run
21505	0.38	ray::DefaultActor.run
21506	0.37	ray::DefaultActor.run
21512	0.37	ray::DefaultActor.run
21516	0.37	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21025	0.08	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,541 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.99	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.38	ray::DefaultActor.run
21505	0.38	ray::DefaultActor.run
21506	0.37	ray::DefaultActor.run
21512	0.37	ray::DefaultActor.run
21516	0.37	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21025	0.08	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,541 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.17	ray::IDLE
21512	0.17	ray::IDLE
21507	0.17	ray::IDLE
21505	0.17	ray::IDLE
21511	0.17	ray::IDLE
21504	0.17	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,541 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.17	ray::IDLE
21512	0.17	ray::IDLE
21507	0.17	ray::IDLE
21505	0.17	ray::IDLE
21511	0.17	ray::IDLE
21504	0.17	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,543 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.75	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.31	ray::DefaultActor.run
21505	0.31	ray::DefaultActor.run
21512	0.31	ray::DefaultActor.run
21506	0.30	ray::DefaultActor.run
21516	0.30	ray::DefaultActor.run
21504	0.30	ray::DefaultActor.run
21503	0.29	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,543 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.75	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.31	ray::DefaultActor.run
21505	0.31	ray::DefaultActor.run
21512	0.31	ray::DefaultActor.run
21506	0.30	ray::DefaultActor.run
21516	0.30	ray::DefaultActor.run
21504	0.30	ray::DefaultActor.run
21503	0.29	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,546 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.29	ray::DefaultActor.run
21508	0.28	ray::DefaultActor.run
21507	0.27	ray::DefaultActor.run
21506	0.27	ray::DefaultActor.run
21512	0.27	ray::DefaultActor.run
21505	0.27	ray::DefaultActor.run
21504	0.26	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,546 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.29	ray::DefaultActor.run
21508	0.28	ray::DefaultActor.run
21507	0.27	ray::DefaultActor.run
21506	0.27	ray::DefaultActor.run
21512	0.27	ray::DefaultActor.run
21505	0.27	ray::DefaultActor.run
21504	0.26	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,548 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.89	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.36	ray::DefaultActor.run
21512	0.35	ray::DefaultActor.run
21508	0.35	ray::DefaultActor
21505	0.35	ray::DefaultActor
21516	0.34	ray::DefaultActor
21503	0.34	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,548 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.89	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.36	ray::DefaultActor.run
21512	0.35	ray::DefaultActor.run
21508	0.35	ray::DefaultActor
21505	0.35	ray::DefaultActor
21516	0.34	ray::DefaultActor
21503	0.34	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,548 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.72	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.27	ray::DefaultActor.run
21512	0.24	ray::DefaultActor.run
21515	0.24	ray::DefaultActor.run
21508	0.23	ray::DefaultActor.run
21509	0.23	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21507	0.22	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,548 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.72	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.27	ray::DefaultActor.run
21512	0.24	ray::DefaultActor.run
21515	0.24	ray::DefaultActor.run
21508	0.23	ray::DefaultActor.run
21509	0.23	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21507	0.22	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,548 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.21	ray::IDLE
21512	0.21	ray::IDLE
21507	0.21	ray::IDLE
21505	0.21	ray::IDLE
21504	0.21	ray::IDLE
21515	0.21	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,548 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.21	ray::IDLE
21512	0.21	ray::IDLE
21507	0.21	ray::IDLE
21505	0.21	ray::IDLE
21504	0.21	ray::IDLE
21515	0.21	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,549 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.28	ray::DefaultActor.run
21515	0.26	ray::DefaultActor.run
21508	0.26	ray::DefaultActor.run
21512	0.26	ray::DefaultActor.run
21507	0.24	ray::DefaultActor.run
21504	0.24	ray::DefaultActor.run
21505	0.24	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,549 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.28	ray::DefaultActor.run
21515	0.26	ray::DefaultActor.run
21508	0.26	ray::DefaultActor.run
21512	0.26	ray::DefaultActor.run
21507	0.24	ray::DefaultActor.run
21504	0.24	ray::DefaultActor.run
21505	0.24	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,554 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.52	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.26	ray::DefaultActor
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.22	ray::DefaultActor
21505	0.22	ray::DefaultActor
21499	0.22	ray::DefaultActor
21508	0.22	ray::DefaultActor
21504	0.22	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,554 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.52	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.26	ray::DefaultActor
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.22	ray::DefaultActor
21505	0.22	ray::DefaultActor
21499	0.22	ray::DefaultActor
21508	0.22	ray::DefaultActor
21504	0.22	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,556 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.19	ray::IDLE
21516	0.19	ray::IDLE
21507	0.19	ray::IDLE
21505	0.19	ray::IDLE
21504	0.19	ray::IDLE
21499	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,556 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.19	ray::IDLE
21516	0.19	ray::IDLE
21507	0.19	ray::IDLE
21505	0.19	ray::IDLE
21504	0.19	ray::IDLE
21499	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,559 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.18	ray::IDLE
21516	0.18	ray::IDLE
21505	0.18	ray::IDLE
21507	0.18	ray::IDLE
21499	0.18	ray::IDLE
21504	0.18	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,559 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.18	ray::IDLE
21516	0.18	ray::IDLE
21505	0.18	ray::IDLE
21507	0.18	ray::IDLE
21499	0.18	ray::IDLE
21504	0.18	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,564 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.17	ray::IDLE
21512	0.17	ray::IDLE
21507	0.17	ray::IDLE
21505	0.17	ray::IDLE
21511	0.17	ray::IDLE
21504	0.17	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,565 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.75	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.31	ray::DefaultActor.run
21505	0.31	ray::DefaultActor.run
21512	0.31	ray::DefaultActor.run
21506	0.30	ray::DefaultActor.run
21516	0.30	ray::DefaultActor.run
21504	0.30	ray::DefaultActor.run
21503	0.29	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,564 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.17	ray::IDLE
21512	0.17	ray::IDLE
21507	0.17	ray::IDLE
21505	0.17	ray::IDLE
21511	0.17	ray::IDLE
21504	0.17	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,565 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.75	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.31	ray::DefaultActor.run
21505	0.31	ray::DefaultActor.run
21512	0.31	ray::DefaultActor.run
21506	0.30	ray::DefaultActor.run
21516	0.30	ray::DefaultActor.run
21504	0.30	ray::DefaultActor.run
21503	0.29	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,566 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.29	ray::DefaultActor.run
21508	0.28	ray::DefaultActor.run
21507	0.27	ray::DefaultActor.run
21506	0.27	ray::DefaultActor.run
21512	0.27	ray::DefaultActor.run
21505	0.27	ray::DefaultActor.run
21504	0.26	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,566 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.29	ray::DefaultActor.run
21508	0.28	ray::DefaultActor.run
21507	0.27	ray::DefaultActor.run
21506	0.27	ray::DefaultActor.run
21512	0.27	ray::DefaultActor.run
21505	0.27	ray::DefaultActor.run
21504	0.26	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,567 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.89	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.36	ray::DefaultActor.run
21512	0.35	ray::DefaultActor.run
21508	0.35	ray::DefaultActor
21505	0.35	ray::DefaultActor
21516	0.34	ray::DefaultActor
21503	0.34	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,567 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.89	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.36	ray::DefaultActor.run
21512	0.35	ray::DefaultActor.run
21508	0.35	ray::DefaultActor
21505	0.35	ray::DefaultActor
21516	0.34	ray::DefaultActor
21503	0.34	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,568 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.72	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.27	ray::DefaultActor.run
21512	0.24	ray::DefaultActor.run
21515	0.24	ray::DefaultActor.run
21508	0.23	ray::DefaultActor.run
21509	0.23	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21507	0.22	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,568 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.72	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.27	ray::DefaultActor.run
21512	0.24	ray::DefaultActor.run
21515	0.24	ray::DefaultActor.run
21508	0.23	ray::DefaultActor.run
21509	0.23	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21507	0.22	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,904 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 0ec556f93983fd754e49558001000000, name=DefaultActor.__init__, pid=21516, memory used=0.43GB) was running was 7.07GB / 7.44GB (0.950049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01e894dafc14c58acc1d58301dbd318d734ed5b0ee61f337692679a3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-01e894dafc14c58acc1d58301dbd318d734ed5b0ee61f337692679a3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.98	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.45	ray::DefaultActor.run
21505	0.44	ray::DefaultActor.run
21512	0.44	ray::DefaultActor.run
21516	0.43	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21025	0.08	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...
21145	0.06	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -u /home/guilherme/cpu-tens...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,904 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 0ec556f93983fd754e49558001000000, name=DefaultActor.__init__, pid=21516, memory used=0.43GB) was running was 7.07GB / 7.44GB (0.950049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01e894dafc14c58acc1d58301dbd318d734ed5b0ee61f337692679a3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-01e894dafc14c58acc1d58301dbd318d734ed5b0ee61f337692679a3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.98	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.45	ray::DefaultActor.run
21505	0.44	ray::DefaultActor.run
21512	0.44	ray::DefaultActor.run
21516	0.43	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21025	0.08	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...
21145	0.06	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -u /home/guilherme/cpu-tens...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,906 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 0ec556f93983fd754e49558001000000, name=DefaultActor.__init__, pid=21516, memory used=0.43GB) was running was 7.07GB / 7.44GB (0.950049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01e894dafc14c58acc1d58301dbd318d734ed5b0ee61f337692679a3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-01e894dafc14c58acc1d58301dbd318d734ed5b0ee61f337692679a3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.98	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.45	ray::DefaultActor.run
21505	0.44	ray::DefaultActor.run
21512	0.44	ray::DefaultActor.run
21516	0.43	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21025	0.08	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...
21145	0.06	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -u /home/guilherme/cpu-tens...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:29,906 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 0ec556f93983fd754e49558001000000, name=DefaultActor.__init__, pid=21516, memory used=0.43GB) was running was 7.07GB / 7.44GB (0.950049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01e894dafc14c58acc1d58301dbd318d734ed5b0ee61f337692679a3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-01e894dafc14c58acc1d58301dbd318d734ed5b0ee61f337692679a3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.98	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.45	ray::DefaultActor.run
21505	0.44	ray::DefaultActor.run
21512	0.44	ray::DefaultActor.run
21516	0.43	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21025	0.08	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...
21145	0.06	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -u /home/guilherme/cpu-tens...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | DEBUG flwr 2024-03-05 15:04:36,632 | server.py:236 | fit_round 8 received 3 results and 16 failures
myFlowerExperiment | DEBUG flwr 2024-03-05 15:04:36,632 | server.py:236 | fit_round 8 received 3 results and 16 failures
myFlowerExperiment | INFO flwr 2024-03-05 15:04:42,262 | server.py:125 | fit progress: (8, 0.7206368446350098, {'accuracy': 0.5837092995643616}, 116.01149144699957)
myFlowerExperiment | INFO flwr 2024-03-05 15:04:42,262 | server.py:125 | fit progress: (8, 0.7206368446350098, {'accuracy': 0.5837092995643616}, 116.01149144699957)
myFlowerExperiment | DEBUG flwr 2024-03-05 15:04:42,264 | server.py:173 | evaluate_round 8: strategy sampled 19 clients (out of 19)
myFlowerExperiment | DEBUG flwr 2024-03-05 15:04:42,264 | server.py:173 | evaluate_round 8: strategy sampled 19 clients (out of 19)
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,467 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 0ec556f93983fd754e49558001000000, name=DefaultActor.__init__, pid=21516, memory used=0.43GB) was running was 7.07GB / 7.44GB (0.950049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01e894dafc14c58acc1d58301dbd318d734ed5b0ee61f337692679a3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-01e894dafc14c58acc1d58301dbd318d734ed5b0ee61f337692679a3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.98	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.45	ray::DefaultActor.run
21505	0.44	ray::DefaultActor.run
21512	0.44	ray::DefaultActor.run
21516	0.43	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21025	0.08	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...
21145	0.06	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -u /home/guilherme/cpu-tens...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,467 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 0ec556f93983fd754e49558001000000, name=DefaultActor.__init__, pid=21516, memory used=0.43GB) was running was 7.07GB / 7.44GB (0.950049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01e894dafc14c58acc1d58301dbd318d734ed5b0ee61f337692679a3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-01e894dafc14c58acc1d58301dbd318d734ed5b0ee61f337692679a3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.98	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.45	ray::DefaultActor.run
21505	0.44	ray::DefaultActor.run
21512	0.44	ray::DefaultActor.run
21516	0.43	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21025	0.08	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...
21145	0.06	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -u /home/guilherme/cpu-tens...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,477 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 0ec556f93983fd754e49558001000000, name=DefaultActor.__init__, pid=21516, memory used=0.43GB) was running was 7.07GB / 7.44GB (0.950049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01e894dafc14c58acc1d58301dbd318d734ed5b0ee61f337692679a3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-01e894dafc14c58acc1d58301dbd318d734ed5b0ee61f337692679a3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.98	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.45	ray::DefaultActor.run
21505	0.44	ray::DefaultActor.run
21512	0.44	ray::DefaultActor.run
21516	0.43	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21025	0.08	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...
21145	0.06	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -u /home/guilherme/cpu-tens...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,477 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 0ec556f93983fd754e49558001000000, name=DefaultActor.__init__, pid=21516, memory used=0.43GB) was running was 7.07GB / 7.44GB (0.950049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01e894dafc14c58acc1d58301dbd318d734ed5b0ee61f337692679a3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-01e894dafc14c58acc1d58301dbd318d734ed5b0ee61f337692679a3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.98	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.45	ray::DefaultActor.run
21505	0.44	ray::DefaultActor.run
21512	0.44	ray::DefaultActor.run
21516	0.43	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21025	0.08	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...
21145	0.06	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -u /home/guilherme/cpu-tens...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,494 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.72	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.27	ray::DefaultActor.run
21512	0.24	ray::DefaultActor.run
21515	0.24	ray::DefaultActor.run
21508	0.23	ray::DefaultActor.run
21509	0.23	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21507	0.22	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,494 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.72	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.27	ray::DefaultActor.run
21512	0.24	ray::DefaultActor.run
21515	0.24	ray::DefaultActor.run
21508	0.23	ray::DefaultActor.run
21509	0.23	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21507	0.22	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,496 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.72	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.27	ray::DefaultActor.run
21512	0.24	ray::DefaultActor.run
21515	0.24	ray::DefaultActor.run
21508	0.23	ray::DefaultActor.run
21509	0.23	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21507	0.22	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,496 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.72	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.27	ray::DefaultActor.run
21512	0.24	ray::DefaultActor.run
21515	0.24	ray::DefaultActor.run
21508	0.23	ray::DefaultActor.run
21509	0.23	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21507	0.22	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,500 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.18	ray::IDLE
21516	0.18	ray::IDLE
21505	0.18	ray::IDLE
21507	0.18	ray::IDLE
21499	0.18	ray::IDLE
21504	0.18	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,500 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.18	ray::IDLE
21516	0.18	ray::IDLE
21505	0.18	ray::IDLE
21507	0.18	ray::IDLE
21499	0.18	ray::IDLE
21504	0.18	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,502 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.29	ray::DefaultActor.run
21508	0.28	ray::DefaultActor.run
21507	0.27	ray::DefaultActor.run
21506	0.27	ray::DefaultActor.run
21512	0.27	ray::DefaultActor.run
21505	0.27	ray::DefaultActor.run
21504	0.26	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,502 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.29	ray::DefaultActor.run
21508	0.28	ray::DefaultActor.run
21507	0.27	ray::DefaultActor.run
21506	0.27	ray::DefaultActor.run
21512	0.27	ray::DefaultActor.run
21505	0.27	ray::DefaultActor.run
21504	0.26	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,502 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.17	ray::IDLE
21512	0.17	ray::IDLE
21507	0.17	ray::IDLE
21505	0.17	ray::IDLE
21511	0.17	ray::IDLE
21504	0.17	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,502 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.17	ray::IDLE
21512	0.17	ray::IDLE
21507	0.17	ray::IDLE
21505	0.17	ray::IDLE
21511	0.17	ray::IDLE
21504	0.17	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,504 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.99	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.38	ray::DefaultActor.run
21505	0.38	ray::DefaultActor.run
21506	0.37	ray::DefaultActor.run
21512	0.37	ray::DefaultActor.run
21516	0.37	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21025	0.08	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,504 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.99	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.38	ray::DefaultActor.run
21505	0.38	ray::DefaultActor.run
21506	0.37	ray::DefaultActor.run
21512	0.37	ray::DefaultActor.run
21516	0.37	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21025	0.08	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,506 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.21	ray::IDLE
21512	0.21	ray::IDLE
21507	0.21	ray::IDLE
21505	0.21	ray::IDLE
21504	0.21	ray::IDLE
21515	0.21	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,506 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.21	ray::IDLE
21512	0.21	ray::IDLE
21507	0.21	ray::IDLE
21505	0.21	ray::IDLE
21504	0.21	ray::IDLE
21515	0.21	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,506 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.18	ray::IDLE
21516	0.18	ray::IDLE
21505	0.18	ray::IDLE
21507	0.18	ray::IDLE
21499	0.18	ray::IDLE
21504	0.18	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,506 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.18	ray::IDLE
21516	0.18	ray::IDLE
21505	0.18	ray::IDLE
21507	0.18	ray::IDLE
21499	0.18	ray::IDLE
21504	0.18	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,507 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.29	ray::DefaultActor.run
21508	0.28	ray::DefaultActor.run
21507	0.27	ray::DefaultActor.run
21506	0.27	ray::DefaultActor.run
21512	0.27	ray::DefaultActor.run
21505	0.27	ray::DefaultActor.run
21504	0.26	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,507 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.29	ray::DefaultActor.run
21508	0.28	ray::DefaultActor.run
21507	0.27	ray::DefaultActor.run
21506	0.27	ray::DefaultActor.run
21512	0.27	ray::DefaultActor.run
21505	0.27	ray::DefaultActor.run
21504	0.26	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,509 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.17	ray::IDLE
21512	0.17	ray::IDLE
21507	0.17	ray::IDLE
21505	0.17	ray::IDLE
21511	0.17	ray::IDLE
21504	0.17	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,509 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.17	ray::IDLE
21512	0.17	ray::IDLE
21507	0.17	ray::IDLE
21505	0.17	ray::IDLE
21511	0.17	ray::IDLE
21504	0.17	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,511 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.99	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.38	ray::DefaultActor.run
21505	0.38	ray::DefaultActor.run
21506	0.37	ray::DefaultActor.run
21512	0.37	ray::DefaultActor.run
21516	0.37	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21025	0.08	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,511 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.99	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.38	ray::DefaultActor.run
21505	0.38	ray::DefaultActor.run
21506	0.37	ray::DefaultActor.run
21512	0.37	ray::DefaultActor.run
21516	0.37	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21025	0.08	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,513 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.75	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.31	ray::DefaultActor.run
21505	0.31	ray::DefaultActor.run
21512	0.31	ray::DefaultActor.run
21506	0.30	ray::DefaultActor.run
21516	0.30	ray::DefaultActor.run
21504	0.30	ray::DefaultActor.run
21503	0.29	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,513 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.75	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.31	ray::DefaultActor.run
21505	0.31	ray::DefaultActor.run
21512	0.31	ray::DefaultActor.run
21506	0.30	ray::DefaultActor.run
21516	0.30	ray::DefaultActor.run
21504	0.30	ray::DefaultActor.run
21503	0.29	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,513 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.21	ray::IDLE
21512	0.21	ray::IDLE
21507	0.21	ray::IDLE
21505	0.21	ray::IDLE
21504	0.21	ray::IDLE
21515	0.21	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,513 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.21	ray::IDLE
21512	0.21	ray::IDLE
21507	0.21	ray::IDLE
21505	0.21	ray::IDLE
21504	0.21	ray::IDLE
21515	0.21	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,516 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.19	ray::IDLE
21516	0.19	ray::IDLE
21507	0.19	ray::IDLE
21505	0.19	ray::IDLE
21504	0.19	ray::IDLE
21499	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,516 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.19	ray::IDLE
21516	0.19	ray::IDLE
21507	0.19	ray::IDLE
21505	0.19	ray::IDLE
21504	0.19	ray::IDLE
21499	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,516 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.52	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.26	ray::DefaultActor
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.22	ray::DefaultActor
21505	0.22	ray::DefaultActor
21499	0.22	ray::DefaultActor
21508	0.22	ray::DefaultActor
21504	0.22	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,516 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.52	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.26	ray::DefaultActor
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.22	ray::DefaultActor
21505	0.22	ray::DefaultActor
21499	0.22	ray::DefaultActor
21508	0.22	ray::DefaultActor
21504	0.22	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,517 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.16	ray::IDLE
21516	0.16	ray::IDLE
21511	0.16	ray::IDLE
21507	0.16	ray::IDLE
21504	0.16	ray::IDLE
21505	0.16	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,518 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.28	ray::DefaultActor.run
21515	0.26	ray::DefaultActor.run
21508	0.26	ray::DefaultActor.run
21512	0.26	ray::DefaultActor.run
21507	0.24	ray::DefaultActor.run
21504	0.24	ray::DefaultActor.run
21505	0.24	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,517 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.16	ray::IDLE
21516	0.16	ray::IDLE
21511	0.16	ray::IDLE
21507	0.16	ray::IDLE
21504	0.16	ray::IDLE
21505	0.16	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,518 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.28	ray::DefaultActor.run
21515	0.26	ray::DefaultActor.run
21508	0.26	ray::DefaultActor.run
21512	0.26	ray::DefaultActor.run
21507	0.24	ray::DefaultActor.run
21504	0.24	ray::DefaultActor.run
21505	0.24	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,518 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 0ec556f93983fd754e49558001000000, name=DefaultActor.__init__, pid=21516, memory used=0.43GB) was running was 7.07GB / 7.44GB (0.950049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01e894dafc14c58acc1d58301dbd318d734ed5b0ee61f337692679a3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-01e894dafc14c58acc1d58301dbd318d734ed5b0ee61f337692679a3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.98	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.45	ray::DefaultActor.run
21505	0.44	ray::DefaultActor.run
21512	0.44	ray::DefaultActor.run
21516	0.43	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21025	0.08	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...
21145	0.06	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -u /home/guilherme/cpu-tens...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,518 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 0ec556f93983fd754e49558001000000, name=DefaultActor.__init__, pid=21516, memory used=0.43GB) was running was 7.07GB / 7.44GB (0.950049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01e894dafc14c58acc1d58301dbd318d734ed5b0ee61f337692679a3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-01e894dafc14c58acc1d58301dbd318d734ed5b0ee61f337692679a3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.98	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.45	ray::DefaultActor.run
21505	0.44	ray::DefaultActor.run
21512	0.44	ray::DefaultActor.run
21516	0.43	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21025	0.08	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...
21145	0.06	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -u /home/guilherme/cpu-tens...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,521 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.89	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.36	ray::DefaultActor.run
21512	0.35	ray::DefaultActor.run
21508	0.35	ray::DefaultActor
21505	0.35	ray::DefaultActor
21516	0.34	ray::DefaultActor
21503	0.34	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,521 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.89	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.36	ray::DefaultActor.run
21512	0.35	ray::DefaultActor.run
21508	0.35	ray::DefaultActor
21505	0.35	ray::DefaultActor
21516	0.34	ray::DefaultActor
21503	0.34	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,522 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.72	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.27	ray::DefaultActor.run
21512	0.24	ray::DefaultActor.run
21515	0.24	ray::DefaultActor.run
21508	0.23	ray::DefaultActor.run
21509	0.23	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21507	0.22	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,522 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.72	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.27	ray::DefaultActor.run
21512	0.24	ray::DefaultActor.run
21515	0.24	ray::DefaultActor.run
21508	0.23	ray::DefaultActor.run
21509	0.23	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21507	0.22	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,523 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.75	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.31	ray::DefaultActor.run
21505	0.31	ray::DefaultActor.run
21512	0.31	ray::DefaultActor.run
21506	0.30	ray::DefaultActor.run
21516	0.30	ray::DefaultActor.run
21504	0.30	ray::DefaultActor.run
21503	0.29	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,523 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.75	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.31	ray::DefaultActor.run
21505	0.31	ray::DefaultActor.run
21512	0.31	ray::DefaultActor.run
21506	0.30	ray::DefaultActor.run
21516	0.30	ray::DefaultActor.run
21504	0.30	ray::DefaultActor.run
21503	0.29	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,527 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.19	ray::IDLE
21516	0.19	ray::IDLE
21507	0.19	ray::IDLE
21505	0.19	ray::IDLE
21504	0.19	ray::IDLE
21499	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,527 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.19	ray::IDLE
21516	0.19	ray::IDLE
21507	0.19	ray::IDLE
21505	0.19	ray::IDLE
21504	0.19	ray::IDLE
21499	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,528 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.52	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.26	ray::DefaultActor
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.22	ray::DefaultActor
21505	0.22	ray::DefaultActor
21499	0.22	ray::DefaultActor
21508	0.22	ray::DefaultActor
21504	0.22	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,528 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.52	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.26	ray::DefaultActor
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.22	ray::DefaultActor
21505	0.22	ray::DefaultActor
21499	0.22	ray::DefaultActor
21508	0.22	ray::DefaultActor
21504	0.22	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,529 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.16	ray::IDLE
21516	0.16	ray::IDLE
21511	0.16	ray::IDLE
21507	0.16	ray::IDLE
21504	0.16	ray::IDLE
21505	0.16	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,529 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.16	ray::IDLE
21516	0.16	ray::IDLE
21511	0.16	ray::IDLE
21507	0.16	ray::IDLE
21504	0.16	ray::IDLE
21505	0.16	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,530 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.28	ray::DefaultActor.run
21515	0.26	ray::DefaultActor.run
21508	0.26	ray::DefaultActor.run
21512	0.26	ray::DefaultActor.run
21507	0.24	ray::DefaultActor.run
21504	0.24	ray::DefaultActor.run
21505	0.24	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,530 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.28	ray::DefaultActor.run
21515	0.26	ray::DefaultActor.run
21508	0.26	ray::DefaultActor.run
21512	0.26	ray::DefaultActor.run
21507	0.24	ray::DefaultActor.run
21504	0.24	ray::DefaultActor.run
21505	0.24	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,532 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 0ec556f93983fd754e49558001000000, name=DefaultActor.__init__, pid=21516, memory used=0.43GB) was running was 7.07GB / 7.44GB (0.950049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01e894dafc14c58acc1d58301dbd318d734ed5b0ee61f337692679a3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-01e894dafc14c58acc1d58301dbd318d734ed5b0ee61f337692679a3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.98	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.45	ray::DefaultActor.run
21505	0.44	ray::DefaultActor.run
21512	0.44	ray::DefaultActor.run
21516	0.43	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21025	0.08	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...
21145	0.06	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -u /home/guilherme/cpu-tens...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,532 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 0ec556f93983fd754e49558001000000, name=DefaultActor.__init__, pid=21516, memory used=0.43GB) was running was 7.07GB / 7.44GB (0.950049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01e894dafc14c58acc1d58301dbd318d734ed5b0ee61f337692679a3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-01e894dafc14c58acc1d58301dbd318d734ed5b0ee61f337692679a3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.98	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.45	ray::DefaultActor.run
21505	0.44	ray::DefaultActor.run
21512	0.44	ray::DefaultActor.run
21516	0.43	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21025	0.08	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...
21145	0.06	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -u /home/guilherme/cpu-tens...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,532 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.89	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.36	ray::DefaultActor.run
21512	0.35	ray::DefaultActor.run
21508	0.35	ray::DefaultActor
21505	0.35	ray::DefaultActor
21516	0.34	ray::DefaultActor
21503	0.34	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,532 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.89	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.36	ray::DefaultActor.run
21512	0.35	ray::DefaultActor.run
21508	0.35	ray::DefaultActor
21505	0.35	ray::DefaultActor
21516	0.34	ray::DefaultActor
21503	0.34	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,533 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.72	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.27	ray::DefaultActor.run
21512	0.24	ray::DefaultActor.run
21515	0.24	ray::DefaultActor.run
21508	0.23	ray::DefaultActor.run
21509	0.23	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21507	0.22	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,533 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.72	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.27	ray::DefaultActor.run
21512	0.24	ray::DefaultActor.run
21515	0.24	ray::DefaultActor.run
21508	0.23	ray::DefaultActor.run
21509	0.23	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21507	0.22	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | DEBUG flwr 2024-03-05 15:04:42,543 | server.py:187 | evaluate_round 8 received 0 results and 19 failures
myFlowerExperiment | DEBUG flwr 2024-03-05 15:04:42,543 | server.py:187 | evaluate_round 8 received 0 results and 19 failures
myFlowerExperiment | DEBUG flwr 2024-03-05 15:04:42,545 | server.py:222 | fit_round 9: strategy sampled 19 clients (out of 19)
myFlowerExperiment | DEBUG flwr 2024-03-05 15:04:42,545 | server.py:222 | fit_round 9: strategy sampled 19 clients (out of 19)
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,784 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.72	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.27	ray::DefaultActor.run
21512	0.24	ray::DefaultActor.run
21515	0.24	ray::DefaultActor.run
21508	0.23	ray::DefaultActor.run
21509	0.23	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21507	0.22	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,784 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.72	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.27	ray::DefaultActor.run
21512	0.24	ray::DefaultActor.run
21515	0.24	ray::DefaultActor.run
21508	0.23	ray::DefaultActor.run
21509	0.23	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21507	0.22	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,799 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.72	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.27	ray::DefaultActor.run
21512	0.24	ray::DefaultActor.run
21515	0.24	ray::DefaultActor.run
21508	0.23	ray::DefaultActor.run
21509	0.23	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21507	0.22	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,799 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.72	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.27	ray::DefaultActor.run
21512	0.24	ray::DefaultActor.run
21515	0.24	ray::DefaultActor.run
21508	0.23	ray::DefaultActor.run
21509	0.23	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21507	0.22	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,803 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.52	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.26	ray::DefaultActor
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.22	ray::DefaultActor
21505	0.22	ray::DefaultActor
21499	0.22	ray::DefaultActor
21508	0.22	ray::DefaultActor
21504	0.22	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,803 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.52	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.26	ray::DefaultActor
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.22	ray::DefaultActor
21505	0.22	ray::DefaultActor
21499	0.22	ray::DefaultActor
21508	0.22	ray::DefaultActor
21504	0.22	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,817 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.52	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.26	ray::DefaultActor
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.22	ray::DefaultActor
21505	0.22	ray::DefaultActor
21499	0.22	ray::DefaultActor
21508	0.22	ray::DefaultActor
21504	0.22	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,817 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.52	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.26	ray::DefaultActor
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.22	ray::DefaultActor
21505	0.22	ray::DefaultActor
21499	0.22	ray::DefaultActor
21508	0.22	ray::DefaultActor
21504	0.22	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,822 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.28	ray::DefaultActor.run
21515	0.26	ray::DefaultActor.run
21508	0.26	ray::DefaultActor.run
21512	0.26	ray::DefaultActor.run
21507	0.24	ray::DefaultActor.run
21504	0.24	ray::DefaultActor.run
21505	0.24	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,822 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.28	ray::DefaultActor.run
21515	0.26	ray::DefaultActor.run
21508	0.26	ray::DefaultActor.run
21512	0.26	ray::DefaultActor.run
21507	0.24	ray::DefaultActor.run
21504	0.24	ray::DefaultActor.run
21505	0.24	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,824 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.28	ray::DefaultActor.run
21515	0.26	ray::DefaultActor.run
21508	0.26	ray::DefaultActor.run
21512	0.26	ray::DefaultActor.run
21507	0.24	ray::DefaultActor.run
21504	0.24	ray::DefaultActor.run
21505	0.24	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,824 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.28	ray::DefaultActor.run
21515	0.26	ray::DefaultActor.run
21508	0.26	ray::DefaultActor.run
21512	0.26	ray::DefaultActor.run
21507	0.24	ray::DefaultActor.run
21504	0.24	ray::DefaultActor.run
21505	0.24	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,829 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.89	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.36	ray::DefaultActor.run
21512	0.35	ray::DefaultActor.run
21508	0.35	ray::DefaultActor
21505	0.35	ray::DefaultActor
21516	0.34	ray::DefaultActor
21503	0.34	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,829 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.89	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.36	ray::DefaultActor.run
21512	0.35	ray::DefaultActor.run
21508	0.35	ray::DefaultActor
21505	0.35	ray::DefaultActor
21516	0.34	ray::DefaultActor
21503	0.34	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,829 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 0ec556f93983fd754e49558001000000, name=DefaultActor.__init__, pid=21516, memory used=0.43GB) was running was 7.07GB / 7.44GB (0.950049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01e894dafc14c58acc1d58301dbd318d734ed5b0ee61f337692679a3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-01e894dafc14c58acc1d58301dbd318d734ed5b0ee61f337692679a3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.98	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.45	ray::DefaultActor.run
21505	0.44	ray::DefaultActor.run
21512	0.44	ray::DefaultActor.run
21516	0.43	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21025	0.08	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...
21145	0.06	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -u /home/guilherme/cpu-tens...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,829 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 0ec556f93983fd754e49558001000000, name=DefaultActor.__init__, pid=21516, memory used=0.43GB) was running was 7.07GB / 7.44GB (0.950049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01e894dafc14c58acc1d58301dbd318d734ed5b0ee61f337692679a3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-01e894dafc14c58acc1d58301dbd318d734ed5b0ee61f337692679a3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.98	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.45	ray::DefaultActor.run
21505	0.44	ray::DefaultActor.run
21512	0.44	ray::DefaultActor.run
21516	0.43	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21025	0.08	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...
21145	0.06	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -u /home/guilherme/cpu-tens...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,831 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.16	ray::IDLE
21516	0.16	ray::IDLE
21511	0.16	ray::IDLE
21507	0.16	ray::IDLE
21504	0.16	ray::IDLE
21505	0.16	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,831 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.16	ray::IDLE
21516	0.16	ray::IDLE
21511	0.16	ray::IDLE
21507	0.16	ray::IDLE
21504	0.16	ray::IDLE
21505	0.16	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,833 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.17	ray::IDLE
21512	0.17	ray::IDLE
21507	0.17	ray::IDLE
21505	0.17	ray::IDLE
21511	0.17	ray::IDLE
21504	0.17	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,833 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.17	ray::IDLE
21512	0.17	ray::IDLE
21507	0.17	ray::IDLE
21505	0.17	ray::IDLE
21511	0.17	ray::IDLE
21504	0.17	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,834 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.75	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.31	ray::DefaultActor.run
21505	0.31	ray::DefaultActor.run
21512	0.31	ray::DefaultActor.run
21506	0.30	ray::DefaultActor.run
21516	0.30	ray::DefaultActor.run
21504	0.30	ray::DefaultActor.run
21503	0.29	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,834 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.75	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.31	ray::DefaultActor.run
21505	0.31	ray::DefaultActor.run
21512	0.31	ray::DefaultActor.run
21506	0.30	ray::DefaultActor.run
21516	0.30	ray::DefaultActor.run
21504	0.30	ray::DefaultActor.run
21503	0.29	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,835 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.89	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.36	ray::DefaultActor.run
21512	0.35	ray::DefaultActor.run
21508	0.35	ray::DefaultActor
21505	0.35	ray::DefaultActor
21516	0.34	ray::DefaultActor
21503	0.34	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,835 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.89	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.36	ray::DefaultActor.run
21512	0.35	ray::DefaultActor.run
21508	0.35	ray::DefaultActor
21505	0.35	ray::DefaultActor
21516	0.34	ray::DefaultActor
21503	0.34	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,836 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 0ec556f93983fd754e49558001000000, name=DefaultActor.__init__, pid=21516, memory used=0.43GB) was running was 7.07GB / 7.44GB (0.950049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01e894dafc14c58acc1d58301dbd318d734ed5b0ee61f337692679a3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-01e894dafc14c58acc1d58301dbd318d734ed5b0ee61f337692679a3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.98	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.45	ray::DefaultActor.run
21505	0.44	ray::DefaultActor.run
21512	0.44	ray::DefaultActor.run
21516	0.43	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21025	0.08	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...
21145	0.06	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -u /home/guilherme/cpu-tens...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,836 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 0ec556f93983fd754e49558001000000, name=DefaultActor.__init__, pid=21516, memory used=0.43GB) was running was 7.07GB / 7.44GB (0.950049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01e894dafc14c58acc1d58301dbd318d734ed5b0ee61f337692679a3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-01e894dafc14c58acc1d58301dbd318d734ed5b0ee61f337692679a3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.98	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.45	ray::DefaultActor.run
21505	0.44	ray::DefaultActor.run
21512	0.44	ray::DefaultActor.run
21516	0.43	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21025	0.08	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...
21145	0.06	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -u /home/guilherme/cpu-tens...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,838 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.16	ray::IDLE
21516	0.16	ray::IDLE
21511	0.16	ray::IDLE
21507	0.16	ray::IDLE
21504	0.16	ray::IDLE
21505	0.16	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,838 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.16	ray::IDLE
21516	0.16	ray::IDLE
21511	0.16	ray::IDLE
21507	0.16	ray::IDLE
21504	0.16	ray::IDLE
21505	0.16	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,839 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.29	ray::DefaultActor.run
21508	0.28	ray::DefaultActor.run
21507	0.27	ray::DefaultActor.run
21506	0.27	ray::DefaultActor.run
21512	0.27	ray::DefaultActor.run
21505	0.27	ray::DefaultActor.run
21504	0.26	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,839 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.29	ray::DefaultActor.run
21508	0.28	ray::DefaultActor.run
21507	0.27	ray::DefaultActor.run
21506	0.27	ray::DefaultActor.run
21512	0.27	ray::DefaultActor.run
21505	0.27	ray::DefaultActor.run
21504	0.26	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,839 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.72	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.27	ray::DefaultActor.run
21512	0.24	ray::DefaultActor.run
21515	0.24	ray::DefaultActor.run
21508	0.23	ray::DefaultActor.run
21509	0.23	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21507	0.22	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,839 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.72	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.27	ray::DefaultActor.run
21512	0.24	ray::DefaultActor.run
21515	0.24	ray::DefaultActor.run
21508	0.23	ray::DefaultActor.run
21509	0.23	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21507	0.22	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,843 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.17	ray::IDLE
21512	0.17	ray::IDLE
21507	0.17	ray::IDLE
21505	0.17	ray::IDLE
21511	0.17	ray::IDLE
21504	0.17	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,843 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.17	ray::IDLE
21512	0.17	ray::IDLE
21507	0.17	ray::IDLE
21505	0.17	ray::IDLE
21511	0.17	ray::IDLE
21504	0.17	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,843 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.18	ray::IDLE
21516	0.18	ray::IDLE
21505	0.18	ray::IDLE
21507	0.18	ray::IDLE
21499	0.18	ray::IDLE
21504	0.18	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,843 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.18	ray::IDLE
21516	0.18	ray::IDLE
21505	0.18	ray::IDLE
21507	0.18	ray::IDLE
21499	0.18	ray::IDLE
21504	0.18	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,844 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.19	ray::IDLE
21516	0.19	ray::IDLE
21507	0.19	ray::IDLE
21505	0.19	ray::IDLE
21504	0.19	ray::IDLE
21499	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,844 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.19	ray::IDLE
21516	0.19	ray::IDLE
21507	0.19	ray::IDLE
21505	0.19	ray::IDLE
21504	0.19	ray::IDLE
21499	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,845 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.21	ray::IDLE
21512	0.21	ray::IDLE
21507	0.21	ray::IDLE
21505	0.21	ray::IDLE
21504	0.21	ray::IDLE
21515	0.21	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,845 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.21	ray::IDLE
21512	0.21	ray::IDLE
21507	0.21	ray::IDLE
21505	0.21	ray::IDLE
21504	0.21	ray::IDLE
21515	0.21	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,845 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.75	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.31	ray::DefaultActor.run
21505	0.31	ray::DefaultActor.run
21512	0.31	ray::DefaultActor.run
21506	0.30	ray::DefaultActor.run
21516	0.30	ray::DefaultActor.run
21504	0.30	ray::DefaultActor.run
21503	0.29	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,845 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.75	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.31	ray::DefaultActor.run
21505	0.31	ray::DefaultActor.run
21512	0.31	ray::DefaultActor.run
21506	0.30	ray::DefaultActor.run
21516	0.30	ray::DefaultActor.run
21504	0.30	ray::DefaultActor.run
21503	0.29	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,846 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.99	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.38	ray::DefaultActor.run
21505	0.38	ray::DefaultActor.run
21506	0.37	ray::DefaultActor.run
21512	0.37	ray::DefaultActor.run
21516	0.37	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21025	0.08	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,846 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.99	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.38	ray::DefaultActor.run
21505	0.38	ray::DefaultActor.run
21506	0.37	ray::DefaultActor.run
21512	0.37	ray::DefaultActor.run
21516	0.37	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21025	0.08	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,849 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.89	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.36	ray::DefaultActor.run
21512	0.35	ray::DefaultActor.run
21508	0.35	ray::DefaultActor
21505	0.35	ray::DefaultActor
21516	0.34	ray::DefaultActor
21503	0.34	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,849 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.89	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.36	ray::DefaultActor.run
21512	0.35	ray::DefaultActor.run
21508	0.35	ray::DefaultActor
21505	0.35	ray::DefaultActor
21516	0.34	ray::DefaultActor
21503	0.34	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,849 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.52	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.26	ray::DefaultActor
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.22	ray::DefaultActor
21505	0.22	ray::DefaultActor
21499	0.22	ray::DefaultActor
21508	0.22	ray::DefaultActor
21504	0.22	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,849 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.52	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.26	ray::DefaultActor
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.22	ray::DefaultActor
21505	0.22	ray::DefaultActor
21499	0.22	ray::DefaultActor
21508	0.22	ray::DefaultActor
21504	0.22	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,854 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.29	ray::DefaultActor.run
21508	0.28	ray::DefaultActor.run
21507	0.27	ray::DefaultActor.run
21506	0.27	ray::DefaultActor.run
21512	0.27	ray::DefaultActor.run
21505	0.27	ray::DefaultActor.run
21504	0.26	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,856 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.72	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.27	ray::DefaultActor.run
21512	0.24	ray::DefaultActor.run
21515	0.24	ray::DefaultActor.run
21508	0.23	ray::DefaultActor.run
21509	0.23	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21507	0.22	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,854 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.29	ray::DefaultActor.run
21508	0.28	ray::DefaultActor.run
21507	0.27	ray::DefaultActor.run
21506	0.27	ray::DefaultActor.run
21512	0.27	ray::DefaultActor.run
21505	0.27	ray::DefaultActor.run
21504	0.26	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,858 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.18	ray::IDLE
21516	0.18	ray::IDLE
21505	0.18	ray::IDLE
21507	0.18	ray::IDLE
21499	0.18	ray::IDLE
21504	0.18	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,856 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.72	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.27	ray::DefaultActor.run
21512	0.24	ray::DefaultActor.run
21515	0.24	ray::DefaultActor.run
21508	0.23	ray::DefaultActor.run
21509	0.23	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21507	0.22	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,858 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.18	ray::IDLE
21516	0.18	ray::IDLE
21505	0.18	ray::IDLE
21507	0.18	ray::IDLE
21499	0.18	ray::IDLE
21504	0.18	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,860 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.19	ray::IDLE
21516	0.19	ray::IDLE
21507	0.19	ray::IDLE
21505	0.19	ray::IDLE
21504	0.19	ray::IDLE
21499	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,860 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.19	ray::IDLE
21516	0.19	ray::IDLE
21507	0.19	ray::IDLE
21505	0.19	ray::IDLE
21504	0.19	ray::IDLE
21499	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,861 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.21	ray::IDLE
21512	0.21	ray::IDLE
21507	0.21	ray::IDLE
21505	0.21	ray::IDLE
21504	0.21	ray::IDLE
21515	0.21	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,861 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.21	ray::IDLE
21512	0.21	ray::IDLE
21507	0.21	ray::IDLE
21505	0.21	ray::IDLE
21504	0.21	ray::IDLE
21515	0.21	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,866 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.99	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.38	ray::DefaultActor.run
21505	0.38	ray::DefaultActor.run
21506	0.37	ray::DefaultActor.run
21512	0.37	ray::DefaultActor.run
21516	0.37	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21025	0.08	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,866 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.99	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.38	ray::DefaultActor.run
21505	0.38	ray::DefaultActor.run
21506	0.37	ray::DefaultActor.run
21512	0.37	ray::DefaultActor.run
21516	0.37	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21025	0.08	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,867 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.89	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.36	ray::DefaultActor.run
21512	0.35	ray::DefaultActor.run
21508	0.35	ray::DefaultActor
21505	0.35	ray::DefaultActor
21516	0.34	ray::DefaultActor
21503	0.34	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,867 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.89	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.36	ray::DefaultActor.run
21512	0.35	ray::DefaultActor.run
21508	0.35	ray::DefaultActor
21505	0.35	ray::DefaultActor
21516	0.34	ray::DefaultActor
21503	0.34	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,868 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.52	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.26	ray::DefaultActor
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.22	ray::DefaultActor
21505	0.22	ray::DefaultActor
21499	0.22	ray::DefaultActor
21508	0.22	ray::DefaultActor
21504	0.22	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:42,868 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.52	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.26	ray::DefaultActor
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.22	ray::DefaultActor
21505	0.22	ray::DefaultActor
21499	0.22	ray::DefaultActor
21508	0.22	ray::DefaultActor
21504	0.22	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | DEBUG flwr 2024-03-05 15:04:50,017 | server.py:236 | fit_round 9 received 3 results and 16 failures
myFlowerExperiment | DEBUG flwr 2024-03-05 15:04:50,017 | server.py:236 | fit_round 9 received 3 results and 16 failures
myFlowerExperiment | INFO flwr 2024-03-05 15:04:55,524 | server.py:125 | fit progress: (9, 0.7143182158470154, {'accuracy': 0.6002097725868225}, 129.2739016759988)
myFlowerExperiment | INFO flwr 2024-03-05 15:04:55,524 | server.py:125 | fit progress: (9, 0.7143182158470154, {'accuracy': 0.6002097725868225}, 129.2739016759988)
myFlowerExperiment | DEBUG flwr 2024-03-05 15:04:55,526 | server.py:173 | evaluate_round 9: strategy sampled 19 clients (out of 19)
myFlowerExperiment | DEBUG flwr 2024-03-05 15:04:55,526 | server.py:173 | evaluate_round 9: strategy sampled 19 clients (out of 19)
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:55,736 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.52	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.26	ray::DefaultActor
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.22	ray::DefaultActor
21505	0.22	ray::DefaultActor
21499	0.22	ray::DefaultActor
21508	0.22	ray::DefaultActor
21504	0.22	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:55,736 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.52	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.26	ray::DefaultActor
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.22	ray::DefaultActor
21505	0.22	ray::DefaultActor
21499	0.22	ray::DefaultActor
21508	0.22	ray::DefaultActor
21504	0.22	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:55,747 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.52	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.26	ray::DefaultActor
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.22	ray::DefaultActor
21505	0.22	ray::DefaultActor
21499	0.22	ray::DefaultActor
21508	0.22	ray::DefaultActor
21504	0.22	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:55,747 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.52	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.26	ray::DefaultActor
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.22	ray::DefaultActor
21505	0.22	ray::DefaultActor
21499	0.22	ray::DefaultActor
21508	0.22	ray::DefaultActor
21504	0.22	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:55,764 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.72	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.27	ray::DefaultActor.run
21512	0.24	ray::DefaultActor.run
21515	0.24	ray::DefaultActor.run
21508	0.23	ray::DefaultActor.run
21509	0.23	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21507	0.22	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:55,764 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.72	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.27	ray::DefaultActor.run
21512	0.24	ray::DefaultActor.run
21515	0.24	ray::DefaultActor.run
21508	0.23	ray::DefaultActor.run
21509	0.23	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21507	0.22	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:55,766 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.72	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.27	ray::DefaultActor.run
21512	0.24	ray::DefaultActor.run
21515	0.24	ray::DefaultActor.run
21508	0.23	ray::DefaultActor.run
21509	0.23	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21507	0.22	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:55,766 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.72	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.27	ray::DefaultActor.run
21512	0.24	ray::DefaultActor.run
21515	0.24	ray::DefaultActor.run
21508	0.23	ray::DefaultActor.run
21509	0.23	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21507	0.22	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:55,769 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.89	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.36	ray::DefaultActor.run
21512	0.35	ray::DefaultActor.run
21508	0.35	ray::DefaultActor
21505	0.35	ray::DefaultActor
21516	0.34	ray::DefaultActor
21503	0.34	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:55,769 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.89	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.36	ray::DefaultActor.run
21512	0.35	ray::DefaultActor.run
21508	0.35	ray::DefaultActor
21505	0.35	ray::DefaultActor
21516	0.34	ray::DefaultActor
21503	0.34	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:55,771 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.89	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.36	ray::DefaultActor.run
21512	0.35	ray::DefaultActor.run
21508	0.35	ray::DefaultActor
21505	0.35	ray::DefaultActor
21516	0.34	ray::DefaultActor
21503	0.34	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:55,771 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.89	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.36	ray::DefaultActor.run
21512	0.35	ray::DefaultActor.run
21508	0.35	ray::DefaultActor
21505	0.35	ray::DefaultActor
21516	0.34	ray::DefaultActor
21503	0.34	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:55,772 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.19	ray::IDLE
21516	0.19	ray::IDLE
21507	0.19	ray::IDLE
21505	0.19	ray::IDLE
21504	0.19	ray::IDLE
21499	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:55,772 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.19	ray::IDLE
21516	0.19	ray::IDLE
21507	0.19	ray::IDLE
21505	0.19	ray::IDLE
21504	0.19	ray::IDLE
21499	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:55,775 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.75	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.31	ray::DefaultActor.run
21505	0.31	ray::DefaultActor.run
21512	0.31	ray::DefaultActor.run
21506	0.30	ray::DefaultActor.run
21516	0.30	ray::DefaultActor.run
21504	0.30	ray::DefaultActor.run
21503	0.29	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:55,775 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.75	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.31	ray::DefaultActor.run
21505	0.31	ray::DefaultActor.run
21512	0.31	ray::DefaultActor.run
21506	0.30	ray::DefaultActor.run
21516	0.30	ray::DefaultActor.run
21504	0.30	ray::DefaultActor.run
21503	0.29	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:55,776 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.16	ray::IDLE
21516	0.16	ray::IDLE
21511	0.16	ray::IDLE
21507	0.16	ray::IDLE
21504	0.16	ray::IDLE
21505	0.16	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:55,776 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.16	ray::IDLE
21516	0.16	ray::IDLE
21511	0.16	ray::IDLE
21507	0.16	ray::IDLE
21504	0.16	ray::IDLE
21505	0.16	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:55,776 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 0ec556f93983fd754e49558001000000, name=DefaultActor.__init__, pid=21516, memory used=0.43GB) was running was 7.07GB / 7.44GB (0.950049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01e894dafc14c58acc1d58301dbd318d734ed5b0ee61f337692679a3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-01e894dafc14c58acc1d58301dbd318d734ed5b0ee61f337692679a3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.98	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.45	ray::DefaultActor.run
21505	0.44	ray::DefaultActor.run
21512	0.44	ray::DefaultActor.run
21516	0.43	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21025	0.08	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...
21145	0.06	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -u /home/guilherme/cpu-tens...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:55,776 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 0ec556f93983fd754e49558001000000, name=DefaultActor.__init__, pid=21516, memory used=0.43GB) was running was 7.07GB / 7.44GB (0.950049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01e894dafc14c58acc1d58301dbd318d734ed5b0ee61f337692679a3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-01e894dafc14c58acc1d58301dbd318d734ed5b0ee61f337692679a3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.98	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.45	ray::DefaultActor.run
21505	0.44	ray::DefaultActor.run
21512	0.44	ray::DefaultActor.run
21516	0.43	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21025	0.08	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...
21145	0.06	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -u /home/guilherme/cpu-tens...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:55,777 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.19	ray::IDLE
21516	0.19	ray::IDLE
21507	0.19	ray::IDLE
21505	0.19	ray::IDLE
21504	0.19	ray::IDLE
21499	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:55,777 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.19	ray::IDLE
21516	0.19	ray::IDLE
21507	0.19	ray::IDLE
21505	0.19	ray::IDLE
21504	0.19	ray::IDLE
21499	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:55,778 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.52	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.26	ray::DefaultActor
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.22	ray::DefaultActor
21505	0.22	ray::DefaultActor
21499	0.22	ray::DefaultActor
21508	0.22	ray::DefaultActor
21504	0.22	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:55,778 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.52	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.26	ray::DefaultActor
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.22	ray::DefaultActor
21505	0.22	ray::DefaultActor
21499	0.22	ray::DefaultActor
21508	0.22	ray::DefaultActor
21504	0.22	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:55,779 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.75	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.31	ray::DefaultActor.run
21505	0.31	ray::DefaultActor.run
21512	0.31	ray::DefaultActor.run
21506	0.30	ray::DefaultActor.run
21516	0.30	ray::DefaultActor.run
21504	0.30	ray::DefaultActor.run
21503	0.29	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:55,779 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.75	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.31	ray::DefaultActor.run
21505	0.31	ray::DefaultActor.run
21512	0.31	ray::DefaultActor.run
21506	0.30	ray::DefaultActor.run
21516	0.30	ray::DefaultActor.run
21504	0.30	ray::DefaultActor.run
21503	0.29	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:55,780 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.21	ray::IDLE
21512	0.21	ray::IDLE
21507	0.21	ray::IDLE
21505	0.21	ray::IDLE
21504	0.21	ray::IDLE
21515	0.21	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:55,780 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.21	ray::IDLE
21512	0.21	ray::IDLE
21507	0.21	ray::IDLE
21505	0.21	ray::IDLE
21504	0.21	ray::IDLE
21515	0.21	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:55,782 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.16	ray::IDLE
21516	0.16	ray::IDLE
21511	0.16	ray::IDLE
21507	0.16	ray::IDLE
21504	0.16	ray::IDLE
21505	0.16	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:55,782 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.16	ray::IDLE
21516	0.16	ray::IDLE
21511	0.16	ray::IDLE
21507	0.16	ray::IDLE
21504	0.16	ray::IDLE
21505	0.16	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:55,784 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.29	ray::DefaultActor.run
21508	0.28	ray::DefaultActor.run
21507	0.27	ray::DefaultActor.run
21506	0.27	ray::DefaultActor.run
21512	0.27	ray::DefaultActor.run
21505	0.27	ray::DefaultActor.run
21504	0.26	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:55,784 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.29	ray::DefaultActor.run
21508	0.28	ray::DefaultActor.run
21507	0.27	ray::DefaultActor.run
21506	0.27	ray::DefaultActor.run
21512	0.27	ray::DefaultActor.run
21505	0.27	ray::DefaultActor.run
21504	0.26	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:55,784 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 0ec556f93983fd754e49558001000000, name=DefaultActor.__init__, pid=21516, memory used=0.43GB) was running was 7.07GB / 7.44GB (0.950049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01e894dafc14c58acc1d58301dbd318d734ed5b0ee61f337692679a3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-01e894dafc14c58acc1d58301dbd318d734ed5b0ee61f337692679a3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.98	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.45	ray::DefaultActor.run
21505	0.44	ray::DefaultActor.run
21512	0.44	ray::DefaultActor.run
21516	0.43	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21025	0.08	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...
21145	0.06	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -u /home/guilherme/cpu-tens...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:55,784 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 0ec556f93983fd754e49558001000000, name=DefaultActor.__init__, pid=21516, memory used=0.43GB) was running was 7.07GB / 7.44GB (0.950049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01e894dafc14c58acc1d58301dbd318d734ed5b0ee61f337692679a3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-01e894dafc14c58acc1d58301dbd318d734ed5b0ee61f337692679a3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.98	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.45	ray::DefaultActor.run
21505	0.44	ray::DefaultActor.run
21512	0.44	ray::DefaultActor.run
21516	0.43	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21025	0.08	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...
21145	0.06	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -u /home/guilherme/cpu-tens...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:55,784 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.99	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.38	ray::DefaultActor.run
21505	0.38	ray::DefaultActor.run
21506	0.37	ray::DefaultActor.run
21512	0.37	ray::DefaultActor.run
21516	0.37	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21025	0.08	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:55,784 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.99	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.38	ray::DefaultActor.run
21505	0.38	ray::DefaultActor.run
21506	0.37	ray::DefaultActor.run
21512	0.37	ray::DefaultActor.run
21516	0.37	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21025	0.08	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:55,784 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.17	ray::IDLE
21512	0.17	ray::IDLE
21507	0.17	ray::IDLE
21505	0.17	ray::IDLE
21511	0.17	ray::IDLE
21504	0.17	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:55,784 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.17	ray::IDLE
21512	0.17	ray::IDLE
21507	0.17	ray::IDLE
21505	0.17	ray::IDLE
21511	0.17	ray::IDLE
21504	0.17	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:55,786 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.52	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.26	ray::DefaultActor
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.22	ray::DefaultActor
21505	0.22	ray::DefaultActor
21499	0.22	ray::DefaultActor
21508	0.22	ray::DefaultActor
21504	0.22	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:55,786 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.52	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.26	ray::DefaultActor
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.22	ray::DefaultActor
21505	0.22	ray::DefaultActor
21499	0.22	ray::DefaultActor
21508	0.22	ray::DefaultActor
21504	0.22	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:55,787 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.89	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.36	ray::DefaultActor.run
21512	0.35	ray::DefaultActor.run
21508	0.35	ray::DefaultActor
21505	0.35	ray::DefaultActor
21516	0.34	ray::DefaultActor
21503	0.34	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:55,787 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.89	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.36	ray::DefaultActor.run
21512	0.35	ray::DefaultActor.run
21508	0.35	ray::DefaultActor
21505	0.35	ray::DefaultActor
21516	0.34	ray::DefaultActor
21503	0.34	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:55,788 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.18	ray::IDLE
21516	0.18	ray::IDLE
21505	0.18	ray::IDLE
21507	0.18	ray::IDLE
21499	0.18	ray::IDLE
21504	0.18	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:55,788 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.18	ray::IDLE
21516	0.18	ray::IDLE
21505	0.18	ray::IDLE
21507	0.18	ray::IDLE
21499	0.18	ray::IDLE
21504	0.18	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:55,790 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.28	ray::DefaultActor.run
21515	0.26	ray::DefaultActor.run
21508	0.26	ray::DefaultActor.run
21512	0.26	ray::DefaultActor.run
21507	0.24	ray::DefaultActor.run
21504	0.24	ray::DefaultActor.run
21505	0.24	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:55,790 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.28	ray::DefaultActor.run
21515	0.26	ray::DefaultActor.run
21508	0.26	ray::DefaultActor.run
21512	0.26	ray::DefaultActor.run
21507	0.24	ray::DefaultActor.run
21504	0.24	ray::DefaultActor.run
21505	0.24	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:55,792 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.21	ray::IDLE
21512	0.21	ray::IDLE
21507	0.21	ray::IDLE
21505	0.21	ray::IDLE
21504	0.21	ray::IDLE
21515	0.21	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:55,792 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.21	ray::IDLE
21512	0.21	ray::IDLE
21507	0.21	ray::IDLE
21505	0.21	ray::IDLE
21504	0.21	ray::IDLE
21515	0.21	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:55,794 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.29	ray::DefaultActor.run
21508	0.28	ray::DefaultActor.run
21507	0.27	ray::DefaultActor.run
21506	0.27	ray::DefaultActor.run
21512	0.27	ray::DefaultActor.run
21505	0.27	ray::DefaultActor.run
21504	0.26	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:55,794 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.29	ray::DefaultActor.run
21508	0.28	ray::DefaultActor.run
21507	0.27	ray::DefaultActor.run
21506	0.27	ray::DefaultActor.run
21512	0.27	ray::DefaultActor.run
21505	0.27	ray::DefaultActor.run
21504	0.26	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:55,799 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.99	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.38	ray::DefaultActor.run
21505	0.38	ray::DefaultActor.run
21506	0.37	ray::DefaultActor.run
21512	0.37	ray::DefaultActor.run
21516	0.37	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21025	0.08	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:55,799 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.99	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.38	ray::DefaultActor.run
21505	0.38	ray::DefaultActor.run
21506	0.37	ray::DefaultActor.run
21512	0.37	ray::DefaultActor.run
21516	0.37	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21025	0.08	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:55,801 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.17	ray::IDLE
21512	0.17	ray::IDLE
21507	0.17	ray::IDLE
21505	0.17	ray::IDLE
21511	0.17	ray::IDLE
21504	0.17	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:55,801 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.17	ray::IDLE
21512	0.17	ray::IDLE
21507	0.17	ray::IDLE
21505	0.17	ray::IDLE
21511	0.17	ray::IDLE
21504	0.17	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:55,803 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.89	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.36	ray::DefaultActor.run
21512	0.35	ray::DefaultActor.run
21508	0.35	ray::DefaultActor
21505	0.35	ray::DefaultActor
21516	0.34	ray::DefaultActor
21503	0.34	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:55,803 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.89	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.36	ray::DefaultActor.run
21512	0.35	ray::DefaultActor.run
21508	0.35	ray::DefaultActor
21505	0.35	ray::DefaultActor
21516	0.34	ray::DefaultActor
21503	0.34	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:55,804 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.18	ray::IDLE
21516	0.18	ray::IDLE
21505	0.18	ray::IDLE
21507	0.18	ray::IDLE
21499	0.18	ray::IDLE
21504	0.18	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:55,804 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.18	ray::IDLE
21516	0.18	ray::IDLE
21505	0.18	ray::IDLE
21507	0.18	ray::IDLE
21499	0.18	ray::IDLE
21504	0.18	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:55,806 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.28	ray::DefaultActor.run
21515	0.26	ray::DefaultActor.run
21508	0.26	ray::DefaultActor.run
21512	0.26	ray::DefaultActor.run
21507	0.24	ray::DefaultActor.run
21504	0.24	ray::DefaultActor.run
21505	0.24	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:55,806 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.28	ray::DefaultActor.run
21515	0.26	ray::DefaultActor.run
21508	0.26	ray::DefaultActor.run
21512	0.26	ray::DefaultActor.run
21507	0.24	ray::DefaultActor.run
21504	0.24	ray::DefaultActor.run
21505	0.24	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | DEBUG flwr 2024-03-05 15:04:55,814 | server.py:187 | evaluate_round 9 received 0 results and 19 failures
myFlowerExperiment | DEBUG flwr 2024-03-05 15:04:55,814 | server.py:187 | evaluate_round 9 received 0 results and 19 failures
myFlowerExperiment | DEBUG flwr 2024-03-05 15:04:55,815 | server.py:222 | fit_round 10: strategy sampled 19 clients (out of 19)
myFlowerExperiment | DEBUG flwr 2024-03-05 15:04:55,815 | server.py:222 | fit_round 10: strategy sampled 19 clients (out of 19)
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:56,010 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.28	ray::DefaultActor.run
21515	0.26	ray::DefaultActor.run
21508	0.26	ray::DefaultActor.run
21512	0.26	ray::DefaultActor.run
21507	0.24	ray::DefaultActor.run
21504	0.24	ray::DefaultActor.run
21505	0.24	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:56,010 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.28	ray::DefaultActor.run
21515	0.26	ray::DefaultActor.run
21508	0.26	ray::DefaultActor.run
21512	0.26	ray::DefaultActor.run
21507	0.24	ray::DefaultActor.run
21504	0.24	ray::DefaultActor.run
21505	0.24	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:56,022 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.28	ray::DefaultActor.run
21515	0.26	ray::DefaultActor.run
21508	0.26	ray::DefaultActor.run
21512	0.26	ray::DefaultActor.run
21507	0.24	ray::DefaultActor.run
21504	0.24	ray::DefaultActor.run
21505	0.24	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:56,022 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.28	ray::DefaultActor.run
21515	0.26	ray::DefaultActor.run
21508	0.26	ray::DefaultActor.run
21512	0.26	ray::DefaultActor.run
21507	0.24	ray::DefaultActor.run
21504	0.24	ray::DefaultActor.run
21505	0.24	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:56,026 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.18	ray::IDLE
21516	0.18	ray::IDLE
21505	0.18	ray::IDLE
21507	0.18	ray::IDLE
21499	0.18	ray::IDLE
21504	0.18	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:56,026 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.18	ray::IDLE
21516	0.18	ray::IDLE
21505	0.18	ray::IDLE
21507	0.18	ray::IDLE
21499	0.18	ray::IDLE
21504	0.18	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:56,040 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.18	ray::IDLE
21516	0.18	ray::IDLE
21505	0.18	ray::IDLE
21507	0.18	ray::IDLE
21499	0.18	ray::IDLE
21504	0.18	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:56,040 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.18	ray::IDLE
21516	0.18	ray::IDLE
21505	0.18	ray::IDLE
21507	0.18	ray::IDLE
21499	0.18	ray::IDLE
21504	0.18	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:56,047 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.72	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.27	ray::DefaultActor.run
21512	0.24	ray::DefaultActor.run
21515	0.24	ray::DefaultActor.run
21508	0.23	ray::DefaultActor.run
21509	0.23	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21507	0.22	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:56,047 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.72	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.27	ray::DefaultActor.run
21512	0.24	ray::DefaultActor.run
21515	0.24	ray::DefaultActor.run
21508	0.23	ray::DefaultActor.run
21509	0.23	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21507	0.22	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:56,050 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.17	ray::IDLE
21512	0.17	ray::IDLE
21507	0.17	ray::IDLE
21505	0.17	ray::IDLE
21511	0.17	ray::IDLE
21504	0.17	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:56,050 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.17	ray::IDLE
21512	0.17	ray::IDLE
21507	0.17	ray::IDLE
21505	0.17	ray::IDLE
21511	0.17	ray::IDLE
21504	0.17	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:56,051 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.72	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.27	ray::DefaultActor.run
21512	0.24	ray::DefaultActor.run
21515	0.24	ray::DefaultActor.run
21508	0.23	ray::DefaultActor.run
21509	0.23	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21507	0.22	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:56,051 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.72	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.27	ray::DefaultActor.run
21512	0.24	ray::DefaultActor.run
21515	0.24	ray::DefaultActor.run
21508	0.23	ray::DefaultActor.run
21509	0.23	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21507	0.22	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:56,052 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.29	ray::DefaultActor.run
21508	0.28	ray::DefaultActor.run
21507	0.27	ray::DefaultActor.run
21506	0.27	ray::DefaultActor.run
21512	0.27	ray::DefaultActor.run
21505	0.27	ray::DefaultActor.run
21504	0.26	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:56,052 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.29	ray::DefaultActor.run
21508	0.28	ray::DefaultActor.run
21507	0.27	ray::DefaultActor.run
21506	0.27	ray::DefaultActor.run
21512	0.27	ray::DefaultActor.run
21505	0.27	ray::DefaultActor.run
21504	0.26	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:56,053 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.21	ray::IDLE
21512	0.21	ray::IDLE
21507	0.21	ray::IDLE
21505	0.21	ray::IDLE
21504	0.21	ray::IDLE
21515	0.21	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:56,053 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.21	ray::IDLE
21512	0.21	ray::IDLE
21507	0.21	ray::IDLE
21505	0.21	ray::IDLE
21504	0.21	ray::IDLE
21515	0.21	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:56,055 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.89	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.36	ray::DefaultActor.run
21512	0.35	ray::DefaultActor.run
21508	0.35	ray::DefaultActor
21505	0.35	ray::DefaultActor
21516	0.34	ray::DefaultActor
21503	0.34	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:56,055 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.16	ray::IDLE
21516	0.16	ray::IDLE
21511	0.16	ray::IDLE
21507	0.16	ray::IDLE
21504	0.16	ray::IDLE
21505	0.16	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:56,055 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.89	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.36	ray::DefaultActor.run
21512	0.35	ray::DefaultActor.run
21508	0.35	ray::DefaultActor
21505	0.35	ray::DefaultActor
21516	0.34	ray::DefaultActor
21503	0.34	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:56,055 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.16	ray::IDLE
21516	0.16	ray::IDLE
21511	0.16	ray::IDLE
21507	0.16	ray::IDLE
21504	0.16	ray::IDLE
21505	0.16	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:56,056 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.17	ray::IDLE
21512	0.17	ray::IDLE
21507	0.17	ray::IDLE
21505	0.17	ray::IDLE
21511	0.17	ray::IDLE
21504	0.17	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:56,056 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.17	ray::IDLE
21512	0.17	ray::IDLE
21507	0.17	ray::IDLE
21505	0.17	ray::IDLE
21511	0.17	ray::IDLE
21504	0.17	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:56,057 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.52	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.26	ray::DefaultActor
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.22	ray::DefaultActor
21505	0.22	ray::DefaultActor
21499	0.22	ray::DefaultActor
21508	0.22	ray::DefaultActor
21504	0.22	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:56,057 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.52	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.26	ray::DefaultActor
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.22	ray::DefaultActor
21505	0.22	ray::DefaultActor
21499	0.22	ray::DefaultActor
21508	0.22	ray::DefaultActor
21504	0.22	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:56,059 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.19	ray::IDLE
21516	0.19	ray::IDLE
21507	0.19	ray::IDLE
21505	0.19	ray::IDLE
21504	0.19	ray::IDLE
21499	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:56,059 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.19	ray::IDLE
21516	0.19	ray::IDLE
21507	0.19	ray::IDLE
21505	0.19	ray::IDLE
21504	0.19	ray::IDLE
21499	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:56,059 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 0ec556f93983fd754e49558001000000, name=DefaultActor.__init__, pid=21516, memory used=0.43GB) was running was 7.07GB / 7.44GB (0.950049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01e894dafc14c58acc1d58301dbd318d734ed5b0ee61f337692679a3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-01e894dafc14c58acc1d58301dbd318d734ed5b0ee61f337692679a3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.98	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.45	ray::DefaultActor.run
21505	0.44	ray::DefaultActor.run
21512	0.44	ray::DefaultActor.run
21516	0.43	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21025	0.08	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...
21145	0.06	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -u /home/guilherme/cpu-tens...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:56,059 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 0ec556f93983fd754e49558001000000, name=DefaultActor.__init__, pid=21516, memory used=0.43GB) was running was 7.07GB / 7.44GB (0.950049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01e894dafc14c58acc1d58301dbd318d734ed5b0ee61f337692679a3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-01e894dafc14c58acc1d58301dbd318d734ed5b0ee61f337692679a3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.98	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.45	ray::DefaultActor.run
21505	0.44	ray::DefaultActor.run
21512	0.44	ray::DefaultActor.run
21516	0.43	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21025	0.08	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...
21145	0.06	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -u /home/guilherme/cpu-tens...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:56,061 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.29	ray::DefaultActor.run
21508	0.28	ray::DefaultActor.run
21507	0.27	ray::DefaultActor.run
21506	0.27	ray::DefaultActor.run
21512	0.27	ray::DefaultActor.run
21505	0.27	ray::DefaultActor.run
21504	0.26	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:56,061 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.29	ray::DefaultActor.run
21508	0.28	ray::DefaultActor.run
21507	0.27	ray::DefaultActor.run
21506	0.27	ray::DefaultActor.run
21512	0.27	ray::DefaultActor.run
21505	0.27	ray::DefaultActor.run
21504	0.26	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:56,063 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.28	ray::DefaultActor.run
21515	0.26	ray::DefaultActor.run
21508	0.26	ray::DefaultActor.run
21512	0.26	ray::DefaultActor.run
21507	0.24	ray::DefaultActor.run
21504	0.24	ray::DefaultActor.run
21505	0.24	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:56,063 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.28	ray::DefaultActor.run
21515	0.26	ray::DefaultActor.run
21508	0.26	ray::DefaultActor.run
21512	0.26	ray::DefaultActor.run
21507	0.24	ray::DefaultActor.run
21504	0.24	ray::DefaultActor.run
21505	0.24	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:56,068 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.75	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.31	ray::DefaultActor.run
21505	0.31	ray::DefaultActor.run
21512	0.31	ray::DefaultActor.run
21506	0.30	ray::DefaultActor.run
21516	0.30	ray::DefaultActor.run
21504	0.30	ray::DefaultActor.run
21503	0.29	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:56,068 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.21	ray::IDLE
21512	0.21	ray::IDLE
21507	0.21	ray::IDLE
21505	0.21	ray::IDLE
21504	0.21	ray::IDLE
21515	0.21	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:56,068 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.75	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.31	ray::DefaultActor.run
21505	0.31	ray::DefaultActor.run
21512	0.31	ray::DefaultActor.run
21506	0.30	ray::DefaultActor.run
21516	0.30	ray::DefaultActor.run
21504	0.30	ray::DefaultActor.run
21503	0.29	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:56,068 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.21	ray::IDLE
21512	0.21	ray::IDLE
21507	0.21	ray::IDLE
21505	0.21	ray::IDLE
21504	0.21	ray::IDLE
21515	0.21	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:56,069 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.89	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.36	ray::DefaultActor.run
21512	0.35	ray::DefaultActor.run
21508	0.35	ray::DefaultActor
21505	0.35	ray::DefaultActor
21516	0.34	ray::DefaultActor
21503	0.34	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:56,069 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.89	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.36	ray::DefaultActor.run
21512	0.35	ray::DefaultActor.run
21508	0.35	ray::DefaultActor
21505	0.35	ray::DefaultActor
21516	0.34	ray::DefaultActor
21503	0.34	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:56,070 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.16	ray::IDLE
21516	0.16	ray::IDLE
21511	0.16	ray::IDLE
21507	0.16	ray::IDLE
21504	0.16	ray::IDLE
21505	0.16	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:56,070 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.16	ray::IDLE
21516	0.16	ray::IDLE
21511	0.16	ray::IDLE
21507	0.16	ray::IDLE
21504	0.16	ray::IDLE
21505	0.16	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:56,071 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.99	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.38	ray::DefaultActor.run
21505	0.38	ray::DefaultActor.run
21506	0.37	ray::DefaultActor.run
21512	0.37	ray::DefaultActor.run
21516	0.37	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21025	0.08	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:56,071 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.99	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.38	ray::DefaultActor.run
21505	0.38	ray::DefaultActor.run
21506	0.37	ray::DefaultActor.run
21512	0.37	ray::DefaultActor.run
21516	0.37	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21025	0.08	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:56,073 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.52	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.26	ray::DefaultActor
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.22	ray::DefaultActor
21505	0.22	ray::DefaultActor
21499	0.22	ray::DefaultActor
21508	0.22	ray::DefaultActor
21504	0.22	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:56,073 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.52	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.26	ray::DefaultActor
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.22	ray::DefaultActor
21505	0.22	ray::DefaultActor
21499	0.22	ray::DefaultActor
21508	0.22	ray::DefaultActor
21504	0.22	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:56,073 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.89	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.36	ray::DefaultActor.run
21512	0.35	ray::DefaultActor.run
21508	0.35	ray::DefaultActor
21505	0.35	ray::DefaultActor
21516	0.34	ray::DefaultActor
21503	0.34	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:56,073 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.89	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.36	ray::DefaultActor.run
21512	0.35	ray::DefaultActor.run
21508	0.35	ray::DefaultActor
21505	0.35	ray::DefaultActor
21516	0.34	ray::DefaultActor
21503	0.34	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:56,074 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.19	ray::IDLE
21516	0.19	ray::IDLE
21507	0.19	ray::IDLE
21505	0.19	ray::IDLE
21504	0.19	ray::IDLE
21499	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:56,074 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.19	ray::IDLE
21516	0.19	ray::IDLE
21507	0.19	ray::IDLE
21505	0.19	ray::IDLE
21504	0.19	ray::IDLE
21499	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:56,074 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.18	ray::IDLE
21516	0.18	ray::IDLE
21505	0.18	ray::IDLE
21507	0.18	ray::IDLE
21499	0.18	ray::IDLE
21504	0.18	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:56,074 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.18	ray::IDLE
21516	0.18	ray::IDLE
21505	0.18	ray::IDLE
21507	0.18	ray::IDLE
21499	0.18	ray::IDLE
21504	0.18	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:04:56,076 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 0ec556f93983fd754e49558001000000, name=DefaultActor.__init__, pid=21516, memory used=0.43GB) was running was 7.07GB / 7.44GB (0.950049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01e894dafc14c58acc1d58301dbd318d734ed5b0ee61f337692679a3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-01e894dafc14c58acc1d58301dbd318d734ed5b0ee61f337692679a3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.98	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.45	ray::DefaultActor.run
21505	0.44	ray::DefaultActor.run
21512	0.44	ray::DefaultActor.run
21516	0.43	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21025	0.08	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...
21145	0.06	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -u /home/guilherme/cpu-tens...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:56,076 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 0ec556f93983fd754e49558001000000, name=DefaultActor.__init__, pid=21516, memory used=0.43GB) was running was 7.07GB / 7.44GB (0.950049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01e894dafc14c58acc1d58301dbd318d734ed5b0ee61f337692679a3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-01e894dafc14c58acc1d58301dbd318d734ed5b0ee61f337692679a3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.98	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.45	ray::DefaultActor.run
21505	0.44	ray::DefaultActor.run
21512	0.44	ray::DefaultActor.run
21516	0.43	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21025	0.08	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...
21145	0.06	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -u /home/guilherme/cpu-tens...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:56,082 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.28	ray::DefaultActor.run
21515	0.26	ray::DefaultActor.run
21508	0.26	ray::DefaultActor.run
21512	0.26	ray::DefaultActor.run
21507	0.24	ray::DefaultActor.run
21504	0.24	ray::DefaultActor.run
21505	0.24	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:56,082 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.28	ray::DefaultActor.run
21515	0.26	ray::DefaultActor.run
21508	0.26	ray::DefaultActor.run
21512	0.26	ray::DefaultActor.run
21507	0.24	ray::DefaultActor.run
21504	0.24	ray::DefaultActor.run
21505	0.24	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:56,084 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.75	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.31	ray::DefaultActor.run
21505	0.31	ray::DefaultActor.run
21512	0.31	ray::DefaultActor.run
21506	0.30	ray::DefaultActor.run
21516	0.30	ray::DefaultActor.run
21504	0.30	ray::DefaultActor.run
21503	0.29	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:56,084 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.75	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.31	ray::DefaultActor.run
21505	0.31	ray::DefaultActor.run
21512	0.31	ray::DefaultActor.run
21506	0.30	ray::DefaultActor.run
21516	0.30	ray::DefaultActor.run
21504	0.30	ray::DefaultActor.run
21503	0.29	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:56,089 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.99	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.38	ray::DefaultActor.run
21505	0.38	ray::DefaultActor.run
21506	0.37	ray::DefaultActor.run
21512	0.37	ray::DefaultActor.run
21516	0.37	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21025	0.08	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:56,089 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.99	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.38	ray::DefaultActor.run
21505	0.38	ray::DefaultActor.run
21506	0.37	ray::DefaultActor.run
21512	0.37	ray::DefaultActor.run
21516	0.37	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21025	0.08	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:56,091 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.89	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.36	ray::DefaultActor.run
21512	0.35	ray::DefaultActor.run
21508	0.35	ray::DefaultActor
21505	0.35	ray::DefaultActor
21516	0.34	ray::DefaultActor
21503	0.34	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:56,091 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.89	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.36	ray::DefaultActor.run
21512	0.35	ray::DefaultActor.run
21508	0.35	ray::DefaultActor
21505	0.35	ray::DefaultActor
21516	0.34	ray::DefaultActor
21503	0.34	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:56,093 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.18	ray::IDLE
21516	0.18	ray::IDLE
21505	0.18	ray::IDLE
21507	0.18	ray::IDLE
21499	0.18	ray::IDLE
21504	0.18	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:04:56,093 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.18	ray::IDLE
21516	0.18	ray::IDLE
21505	0.18	ray::IDLE
21507	0.18	ray::IDLE
21499	0.18	ray::IDLE
21504	0.18	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | DEBUG flwr 2024-03-05 15:05:03,066 | server.py:236 | fit_round 10 received 3 results and 16 failures
myFlowerExperiment | DEBUG flwr 2024-03-05 15:05:03,066 | server.py:236 | fit_round 10 received 3 results and 16 failures
myFlowerExperiment | INFO flwr 2024-03-05 15:05:08,591 | server.py:125 | fit progress: (10, 0.7127873301506042, {'accuracy': 0.6153992414474487}, 142.3405186529999)
myFlowerExperiment | INFO flwr 2024-03-05 15:05:08,591 | server.py:125 | fit progress: (10, 0.7127873301506042, {'accuracy': 0.6153992414474487}, 142.3405186529999)
myFlowerExperiment | DEBUG flwr 2024-03-05 15:05:08,593 | server.py:173 | evaluate_round 10: strategy sampled 19 clients (out of 19)
myFlowerExperiment | DEBUG flwr 2024-03-05 15:05:08,593 | server.py:173 | evaluate_round 10: strategy sampled 19 clients (out of 19)
myFlowerExperiment | ERROR flwr 2024-03-05 15:05:08,792 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.18	ray::IDLE
21516	0.18	ray::IDLE
21505	0.18	ray::IDLE
21507	0.18	ray::IDLE
21499	0.18	ray::IDLE
21504	0.18	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:05:08,792 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.18	ray::IDLE
21516	0.18	ray::IDLE
21505	0.18	ray::IDLE
21507	0.18	ray::IDLE
21499	0.18	ray::IDLE
21504	0.18	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:05:08,803 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.18	ray::IDLE
21516	0.18	ray::IDLE
21505	0.18	ray::IDLE
21507	0.18	ray::IDLE
21499	0.18	ray::IDLE
21504	0.18	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:05:08,803 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.18	ray::IDLE
21516	0.18	ray::IDLE
21505	0.18	ray::IDLE
21507	0.18	ray::IDLE
21499	0.18	ray::IDLE
21504	0.18	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:05:08,821 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.52	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.26	ray::DefaultActor
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.22	ray::DefaultActor
21505	0.22	ray::DefaultActor
21499	0.22	ray::DefaultActor
21508	0.22	ray::DefaultActor
21504	0.22	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:05:08,821 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.52	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.26	ray::DefaultActor
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.22	ray::DefaultActor
21505	0.22	ray::DefaultActor
21499	0.22	ray::DefaultActor
21508	0.22	ray::DefaultActor
21504	0.22	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:05:08,822 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 0ec556f93983fd754e49558001000000, name=DefaultActor.__init__, pid=21516, memory used=0.43GB) was running was 7.07GB / 7.44GB (0.950049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01e894dafc14c58acc1d58301dbd318d734ed5b0ee61f337692679a3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-01e894dafc14c58acc1d58301dbd318d734ed5b0ee61f337692679a3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.98	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.45	ray::DefaultActor.run
21505	0.44	ray::DefaultActor.run
21512	0.44	ray::DefaultActor.run
21516	0.43	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21025	0.08	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...
21145	0.06	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -u /home/guilherme/cpu-tens...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:05:08,822 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 0ec556f93983fd754e49558001000000, name=DefaultActor.__init__, pid=21516, memory used=0.43GB) was running was 7.07GB / 7.44GB (0.950049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01e894dafc14c58acc1d58301dbd318d734ed5b0ee61f337692679a3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-01e894dafc14c58acc1d58301dbd318d734ed5b0ee61f337692679a3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.98	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.45	ray::DefaultActor.run
21505	0.44	ray::DefaultActor.run
21512	0.44	ray::DefaultActor.run
21516	0.43	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21025	0.08	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...
21145	0.06	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -u /home/guilherme/cpu-tens...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:05:08,824 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.52	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.26	ray::DefaultActor
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.22	ray::DefaultActor
21505	0.22	ray::DefaultActor
21499	0.22	ray::DefaultActor
21508	0.22	ray::DefaultActor
21504	0.22	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:05:08,824 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.52	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.26	ray::DefaultActor
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.22	ray::DefaultActor
21505	0.22	ray::DefaultActor
21499	0.22	ray::DefaultActor
21508	0.22	ray::DefaultActor
21504	0.22	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:05:08,825 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 0ec556f93983fd754e49558001000000, name=DefaultActor.__init__, pid=21516, memory used=0.43GB) was running was 7.07GB / 7.44GB (0.950049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01e894dafc14c58acc1d58301dbd318d734ed5b0ee61f337692679a3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-01e894dafc14c58acc1d58301dbd318d734ed5b0ee61f337692679a3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.98	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.45	ray::DefaultActor.run
21505	0.44	ray::DefaultActor.run
21512	0.44	ray::DefaultActor.run
21516	0.43	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21025	0.08	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...
21145	0.06	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -u /home/guilherme/cpu-tens...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:05:08,825 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 0ec556f93983fd754e49558001000000, name=DefaultActor.__init__, pid=21516, memory used=0.43GB) was running was 7.07GB / 7.44GB (0.950049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 01e894dafc14c58acc1d58301dbd318d734ed5b0ee61f337692679a3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-01e894dafc14c58acc1d58301dbd318d734ed5b0ee61f337692679a3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.98	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.45	ray::DefaultActor.run
21505	0.44	ray::DefaultActor.run
21512	0.44	ray::DefaultActor.run
21516	0.43	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21025	0.08	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...
21145	0.06	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -u /home/guilherme/cpu-tens...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:05:08,829 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.89	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.36	ray::DefaultActor.run
21512	0.35	ray::DefaultActor.run
21508	0.35	ray::DefaultActor
21505	0.35	ray::DefaultActor
21516	0.34	ray::DefaultActor
21503	0.34	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:05:08,829 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.89	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.36	ray::DefaultActor.run
21512	0.35	ray::DefaultActor.run
21508	0.35	ray::DefaultActor
21505	0.35	ray::DefaultActor
21516	0.34	ray::DefaultActor
21503	0.34	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:05:08,829 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.75	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.31	ray::DefaultActor.run
21505	0.31	ray::DefaultActor.run
21512	0.31	ray::DefaultActor.run
21506	0.30	ray::DefaultActor.run
21516	0.30	ray::DefaultActor.run
21504	0.30	ray::DefaultActor.run
21503	0.29	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:05:08,829 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.75	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.31	ray::DefaultActor.run
21505	0.31	ray::DefaultActor.run
21512	0.31	ray::DefaultActor.run
21506	0.30	ray::DefaultActor.run
21516	0.30	ray::DefaultActor.run
21504	0.30	ray::DefaultActor.run
21503	0.29	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:05:08,831 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.72	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.27	ray::DefaultActor.run
21512	0.24	ray::DefaultActor.run
21515	0.24	ray::DefaultActor.run
21508	0.23	ray::DefaultActor.run
21509	0.23	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21507	0.22	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:05:08,831 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.72	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.27	ray::DefaultActor.run
21512	0.24	ray::DefaultActor.run
21515	0.24	ray::DefaultActor.run
21508	0.23	ray::DefaultActor.run
21509	0.23	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21507	0.22	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:05:08,831 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.28	ray::DefaultActor.run
21515	0.26	ray::DefaultActor.run
21508	0.26	ray::DefaultActor.run
21512	0.26	ray::DefaultActor.run
21507	0.24	ray::DefaultActor.run
21504	0.24	ray::DefaultActor.run
21505	0.24	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:05:08,831 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.28	ray::DefaultActor.run
21515	0.26	ray::DefaultActor.run
21508	0.26	ray::DefaultActor.run
21512	0.26	ray::DefaultActor.run
21507	0.24	ray::DefaultActor.run
21504	0.24	ray::DefaultActor.run
21505	0.24	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:05:08,834 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.89	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.36	ray::DefaultActor.run
21512	0.35	ray::DefaultActor.run
21508	0.35	ray::DefaultActor
21505	0.35	ray::DefaultActor
21516	0.34	ray::DefaultActor
21503	0.34	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:05:08,834 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: e8859a6251113c8d6840797e01000000, name=DefaultActor.__init__, pid=21503, memory used=0.34GB) was running was 7.18GB / 7.44GB (0.965361), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-c7857aa375ad75b82d085247667f98242b3fbb246c1fc4a75912b88d*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.89	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21506	0.36	ray::DefaultActor.run
21512	0.35	ray::DefaultActor.run
21508	0.35	ray::DefaultActor
21505	0.35	ray::DefaultActor
21516	0.34	ray::DefaultActor
21503	0.34	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:05:08,835 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.21	ray::IDLE
21512	0.21	ray::IDLE
21507	0.21	ray::IDLE
21505	0.21	ray::IDLE
21504	0.21	ray::IDLE
21515	0.21	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:05:08,835 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.21	ray::IDLE
21512	0.21	ray::IDLE
21507	0.21	ray::IDLE
21505	0.21	ray::IDLE
21504	0.21	ray::IDLE
21515	0.21	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:05:08,836 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.18	ray::IDLE
21516	0.18	ray::IDLE
21505	0.18	ray::IDLE
21507	0.18	ray::IDLE
21499	0.18	ray::IDLE
21504	0.18	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:05:08,836 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.18	ray::IDLE
21516	0.18	ray::IDLE
21505	0.18	ray::IDLE
21507	0.18	ray::IDLE
21499	0.18	ray::IDLE
21504	0.18	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:05:08,837 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.75	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.31	ray::DefaultActor.run
21505	0.31	ray::DefaultActor.run
21512	0.31	ray::DefaultActor.run
21506	0.30	ray::DefaultActor.run
21516	0.30	ray::DefaultActor.run
21504	0.30	ray::DefaultActor.run
21503	0.29	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:05:08,838 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.29	ray::DefaultActor.run
21508	0.28	ray::DefaultActor.run
21507	0.27	ray::DefaultActor.run
21506	0.27	ray::DefaultActor.run
21512	0.27	ray::DefaultActor.run
21505	0.27	ray::DefaultActor.run
21504	0.26	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:05:08,837 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 246f0f1e9f1497164caa005d01000000, name=DefaultActor.__init__, pid=21504, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950059), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-52e9b83cb66f395cf13b3793ce935958a416a0bea10616fafdf1642f*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.75	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.31	ray::DefaultActor.run
21505	0.31	ray::DefaultActor.run
21512	0.31	ray::DefaultActor.run
21506	0.30	ray::DefaultActor.run
21516	0.30	ray::DefaultActor.run
21504	0.30	ray::DefaultActor.run
21503	0.29	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:05:08,838 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.72	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.27	ray::DefaultActor.run
21512	0.24	ray::DefaultActor.run
21515	0.24	ray::DefaultActor.run
21508	0.23	ray::DefaultActor.run
21509	0.23	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21507	0.22	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:05:08,838 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.29	ray::DefaultActor.run
21508	0.28	ray::DefaultActor.run
21507	0.27	ray::DefaultActor.run
21506	0.27	ray::DefaultActor.run
21512	0.27	ray::DefaultActor.run
21505	0.27	ray::DefaultActor.run
21504	0.26	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:05:08,841 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.28	ray::DefaultActor.run
21515	0.26	ray::DefaultActor.run
21508	0.26	ray::DefaultActor.run
21512	0.26	ray::DefaultActor.run
21507	0.24	ray::DefaultActor.run
21504	0.24	ray::DefaultActor.run
21505	0.24	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:05:08,838 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 96a2588f9d422783e44bc78701000000, name=DefaultActor.__init__, pid=21509, memory used=0.23GB) was running was 7.14GB / 7.44GB (0.960094), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-fca7fa52a7a7f6d80dd6072c40a66b0f1228ea27e990d7eb42effa41*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.72	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.27	ray::DefaultActor.run
21512	0.24	ray::DefaultActor.run
21515	0.24	ray::DefaultActor.run
21508	0.23	ray::DefaultActor.run
21509	0.23	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21507	0.22	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:05:08,841 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: a666d7e05df695dc79da602901000000, name=DefaultActor.__init__, pid=21515, memory used=0.26GB) was running was 7.14GB / 7.44GB (0.959888), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-6e66c12d69cb0c61ab83be432c7134b43213cd64e828f1ca341047d5*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.28	ray::DefaultActor.run
21515	0.26	ray::DefaultActor.run
21508	0.26	ray::DefaultActor.run
21512	0.26	ray::DefaultActor.run
21507	0.24	ray::DefaultActor.run
21504	0.24	ray::DefaultActor.run
21505	0.24	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:05:08,842 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.99	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.38	ray::DefaultActor.run
21505	0.38	ray::DefaultActor.run
21506	0.37	ray::DefaultActor.run
21512	0.37	ray::DefaultActor.run
21516	0.37	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21025	0.08	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:05:08,842 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.99	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.38	ray::DefaultActor.run
21505	0.38	ray::DefaultActor.run
21506	0.37	ray::DefaultActor.run
21512	0.37	ray::DefaultActor.run
21516	0.37	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21025	0.08	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:05:08,844 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.19	ray::IDLE
21516	0.19	ray::IDLE
21507	0.19	ray::IDLE
21505	0.19	ray::IDLE
21504	0.19	ray::IDLE
21499	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:05:08,844 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.19	ray::IDLE
21516	0.19	ray::IDLE
21507	0.19	ray::IDLE
21505	0.19	ray::IDLE
21504	0.19	ray::IDLE
21499	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:05:08,846 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.16	ray::IDLE
21516	0.16	ray::IDLE
21511	0.16	ray::IDLE
21507	0.16	ray::IDLE
21504	0.16	ray::IDLE
21505	0.16	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:05:08,847 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.52	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.26	ray::DefaultActor
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.22	ray::DefaultActor
21505	0.22	ray::DefaultActor
21499	0.22	ray::DefaultActor
21508	0.22	ray::DefaultActor
21504	0.22	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:05:08,846 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.16	ray::IDLE
21516	0.16	ray::IDLE
21511	0.16	ray::IDLE
21507	0.16	ray::IDLE
21504	0.16	ray::IDLE
21505	0.16	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:05:08,847 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.52	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.26	ray::DefaultActor
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.22	ray::DefaultActor
21505	0.22	ray::DefaultActor
21499	0.22	ray::DefaultActor
21508	0.22	ray::DefaultActor
21504	0.22	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:05:08,847 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.17	ray::IDLE
21512	0.17	ray::IDLE
21507	0.17	ray::IDLE
21505	0.17	ray::IDLE
21511	0.17	ray::IDLE
21504	0.17	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:05:08,847 | ray_client_proxy.py:161 | Traceback (most recent call last):
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 151, in _submit_job
    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 425, in get_client_result
    return self._fetch_future_result(cid)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 306, in _fetch_future_result
    res_cid, res, updated_context = ray.get(
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py", line 2526, in get
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.17	ray::IDLE
21512	0.17	ray::IDLE
21507	0.17	ray::IDLE
21505	0.17	ray::IDLE
21511	0.17	ray::IDLE
21504	0.17	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

myFlowerExperiment | ERROR flwr 2024-03-05 15:05:08,848 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.21	ray::IDLE
21512	0.21	ray::IDLE
21507	0.21	ray::IDLE
21505	0.21	ray::IDLE
21504	0.21	ray::IDLE
21515	0.21	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:05:08,848 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffff9a03b494aea6965d646c7db901000000, name=DefaultActor.__init__, pid=21510, memory used=0.21GB) was running was 7.07GB / 7.44GB (0.95027), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-dd245d5e15209bddeb95cfc219474ad8133aa650e4f4f26bde5071e3*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.21	ray::IDLE
21512	0.21	ray::IDLE
21507	0.21	ray::IDLE
21505	0.21	ray::IDLE
21504	0.21	ray::IDLE
21515	0.21	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:05:08,850 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.18	ray::IDLE
21516	0.18	ray::IDLE
21505	0.18	ray::IDLE
21507	0.18	ray::IDLE
21499	0.18	ray::IDLE
21504	0.18	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:05:08,850 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffad9c07d916b8fba81a3a59d401000000, name=DefaultActor.__init__, pid=21511, memory used=0.18GB) was running was 7.15GB / 7.44GB (0.960651), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7bb73a0a50885135ca02774680bc50484aa9fe62106a8041d4bc850e*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.18	ray::IDLE
21516	0.18	ray::IDLE
21505	0.18	ray::IDLE
21507	0.18	ray::IDLE
21499	0.18	ray::IDLE
21504	0.18	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:05:08,854 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.29	ray::DefaultActor.run
21508	0.28	ray::DefaultActor.run
21507	0.27	ray::DefaultActor.run
21506	0.27	ray::DefaultActor.run
21512	0.27	ray::DefaultActor.run
21505	0.27	ray::DefaultActor.run
21504	0.26	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:05:08,854 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 1f956f5c6f0de86d002b274f01000000, name=DefaultActor.__init__, pid=21507, memory used=0.27GB) was running was 7.11GB / 7.44GB (0.955986), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-13d730fcba794b6a8f30b403583b49bf4e48a74174eacc829f3aba6a*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.78	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.29	ray::DefaultActor.run
21508	0.28	ray::DefaultActor.run
21507	0.27	ray::DefaultActor.run
21506	0.27	ray::DefaultActor.run
21512	0.27	ray::DefaultActor.run
21505	0.27	ray::DefaultActor.run
21504	0.26	ray::DefaultActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:05:08,856 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.99	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.38	ray::DefaultActor.run
21505	0.38	ray::DefaultActor.run
21506	0.37	ray::DefaultActor.run
21512	0.37	ray::DefaultActor.run
21516	0.37	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21025	0.08	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:05:08,856 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 2784f7c2c2a69aaf7313407e01000000, name=DefaultActor.__init__, pid=21508, memory used=0.38GB) was running was 7.11GB / 7.44GB (0.955564), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-967e6220ad09bee807e4db4ae03176ee4d46745b710c24429794b4d9*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.99	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21508	0.38	ray::DefaultActor.run
21505	0.38	ray::DefaultActor.run
21506	0.37	ray::DefaultActor.run
21512	0.37	ray::DefaultActor.run
21516	0.37	ray::DefaultActor.run
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21025	0.08	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/core/s...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:05:08,859 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.19	ray::IDLE
21516	0.19	ray::IDLE
21507	0.19	ray::IDLE
21505	0.19	ray::IDLE
21504	0.19	ray::IDLE
21499	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:05:08,859 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffdb7ad1cca7a8586b959a277301000000, name=DefaultActor.__init__, pid=21500, memory used=0.19GB) was running was 7.07GB / 7.44GB (0.951153), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ad9c3b885c8826b3e86308ef95d6d2010ff1221579b6cb1ecf837e46*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.19	ray::IDLE
21516	0.19	ray::IDLE
21507	0.19	ray::IDLE
21505	0.19	ray::IDLE
21504	0.19	ray::IDLE
21499	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:05:08,860 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.16	ray::IDLE
21516	0.16	ray::IDLE
21511	0.16	ray::IDLE
21507	0.16	ray::IDLE
21504	0.16	ray::IDLE
21505	0.16	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:05:08,860 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: ffffffffffffffffcc2c5bee3e15ebf8d5343ab001000000, name=DefaultActor.__init__, pid=21514, memory used=0.16GB) was running was 7.10GB / 7.44GB (0.954942), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-2dbbb281d9d90a4cdae3b63e2483605cf3741408fc00f6a8c97db813*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.16	ray::IDLE
21516	0.16	ray::IDLE
21511	0.16	ray::IDLE
21507	0.16	ray::IDLE
21504	0.16	ray::IDLE
21505	0.16	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:05:08,860 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.52	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.26	ray::DefaultActor
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.22	ray::DefaultActor
21505	0.22	ray::DefaultActor
21499	0.22	ray::DefaultActor
21508	0.22	ray::DefaultActor
21504	0.22	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:05:08,860 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (actor ID: 54301ea457a4a9056ffe04a001000000, name=DefaultActor.__init__, pid=21499, memory used=0.22GB) was running was 7.07GB / 7.44GB (0.950464), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-109ad4743f450756e0f4847da2eba32e2672036bb77701c6b7b6c095*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.52	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.26	ray::DefaultActor
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21512	0.22	ray::DefaultActor
21505	0.22	ray::DefaultActor
21499	0.22	ray::DefaultActor
21508	0.22	ray::DefaultActor
21504	0.22	ray::DefaultActor
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:05:08,862 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.17	ray::IDLE
21512	0.17	ray::IDLE
21507	0.17	ray::IDLE
21505	0.17	ray::IDLE
21511	0.17	ray::IDLE
21504	0.17	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | ERROR flwr 2024-03-05 15:05:08,862 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.
Memory on the node (IP: 172.30.126.159, ID: 899bcfe601613ee87600cac07e4408039ba67e531291030850a987fa) where the task (task ID: fffffffffffffffffe868306ab70bc2f38d6bee501000000, name=DefaultActor.__init__, pid=21501, memory used=0.17GB) was running was 7.12GB / 7.44GB (0.957382), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-010f530d559d18865cd53f887cbe7ef93ea0f06b41ee832b80533de0*out -ip 172.30.126.159. Top 10 memory users:
PID	MEM(GB)	COMMAND
10303	1.61	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
533	1.10	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20942	0.49	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
20894	0.23	/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...
21516	0.17	ray::IDLE
21512	0.17	ray::IDLE
21507	0.17	ray::IDLE
21505	0.17	ray::IDLE
21511	0.17	ray::IDLE
21504	0.17	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
myFlowerExperiment | DEBUG flwr 2024-03-05 15:05:08,871 | server.py:187 | evaluate_round 10 received 0 results and 19 failures
myFlowerExperiment | DEBUG flwr 2024-03-05 15:05:08,871 | server.py:187 | evaluate_round 10 received 0 results and 19 failures
myFlowerExperiment | INFO flwr 2024-03-05 15:05:08,872 | server.py:153 | FL finished in 142.62188674400022
myFlowerExperiment | INFO flwr 2024-03-05 15:05:08,872 | server.py:153 | FL finished in 142.62188674400022
myFlowerExperiment | INFO flwr 2024-03-05 15:05:08,874 | app.py:226 | app_fit: losses_distributed []
myFlowerExperiment | INFO flwr 2024-03-05 15:05:08,874 | app.py:226 | app_fit: losses_distributed []
myFlowerExperiment | INFO flwr 2024-03-05 15:05:08,875 | app.py:227 | app_fit: metrics_distributed_fit {}
myFlowerExperiment | INFO flwr 2024-03-05 15:05:08,875 | app.py:227 | app_fit: metrics_distributed_fit {}
myFlowerExperiment | INFO flwr 2024-03-05 15:05:08,877 | app.py:228 | app_fit: metrics_distributed {}
myFlowerExperiment | INFO flwr 2024-03-05 15:05:08,877 | app.py:228 | app_fit: metrics_distributed {}
myFlowerExperiment | INFO flwr 2024-03-05 15:05:08,879 | app.py:229 | app_fit: losses_centralized [(0, 0.693403422832489), (1, 0.7716444134712219), (2, 0.7965250015258789), (3, 0.7837085127830505), (4, 0.785237729549408), (5, 0.7544503808021545), (6, 0.7448371648788452), (7, 0.7352955341339111), (8, 0.7206368446350098), (9, 0.7143182158470154), (10, 0.7127873301506042)]
myFlowerExperiment | INFO flwr 2024-03-05 15:05:08,879 | app.py:229 | app_fit: losses_centralized [(0, 0.693403422832489), (1, 0.7716444134712219), (2, 0.7965250015258789), (3, 0.7837085127830505), (4, 0.785237729549408), (5, 0.7544503808021545), (6, 0.7448371648788452), (7, 0.7352955341339111), (8, 0.7206368446350098), (9, 0.7143182158470154), (10, 0.7127873301506042)]
myFlowerExperiment | INFO flwr 2024-03-05 15:05:08,880 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.5219749808311462), (1, 0.3431371748447418), (2, 0.4252054691314697), (3, 0.46832364797592163), (4, 0.48932668566703796), (5, 0.5235030055046082), (6, 0.5410522222518921), (7, 0.559216320514679), (8, 0.5837092995643616), (9, 0.6002097725868225), (10, 0.6153992414474487)]}
myFlowerExperiment | INFO flwr 2024-03-05 15:05:08,880 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.5219749808311462), (1, 0.3431371748447418), (2, 0.4252054691314697), (3, 0.46832364797592163), (4, 0.48932668566703796), (5, 0.5235030055046082), (6, 0.5410522222518921), (7, 0.559216320514679), (8, 0.5837092995643616), (9, 0.6002097725868225), (10, 0.6153992414474487)]}
