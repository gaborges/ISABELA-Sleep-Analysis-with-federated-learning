{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0c80d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-15 18:03:27.916676: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-15 18:03:28.259076: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-15 18:03:29.064355: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-02-15 18:03:29.064432: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-02-15 18:03:29.064439: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import sys\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n",
    "from multiprocessing import Process\n",
    "import gc\n",
    "\n",
    "import flwr as fl\n",
    "from flwr.server.strategy import FedAvg\n",
    "import tensorflow as tf\n",
    "import sklearn\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "\n",
    "import collections\n",
    "\n",
    "#!export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX/lib/\n",
    "# demonstration of calculating metrics for a neural network model using sklearn\n",
    "from sklearn.datasets import make_circles\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from flwr.common.logger import log\n",
    "from flwr.server.client_manager import ClientManager\n",
    "from flwr.server.client_proxy import ClientProxy\n",
    "from flwr.common import (\n",
    "    EvaluateIns,\n",
    "    EvaluateRes,\n",
    "    FitIns,\n",
    "    FitRes,\n",
    "    MetricsAggregationFn,\n",
    "    NDArrays,\n",
    "    Parameters,\n",
    "    Scalar,\n",
    "    ndarrays_to_parameters,\n",
    "    parameters_to_ndarrays,\n",
    ")\n",
    "\n",
    "# Make TensorFlow log less verbose\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2257ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total arguments passed: 3\n",
      "arg: /home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ipykernel_launcher.py\n",
      "arg: -f\n",
      "arg: /home/guilherme/.local/share/jupyter/runtime/kernel-84608fe3-8da3-4d07-b54d-4129e8145b56.json\n",
      "iteracoes: 0\n",
      "cycle: 1\n"
     ]
    }
   ],
   "source": [
    "# argumentos\n",
    "n = len(sys.argv)\n",
    "print(\"Total arguments passed:\", n)\n",
    "iteracoes = 0\n",
    "cycle_index = 1\n",
    "finalIterations = 0\n",
    "checkpoint_iteration = 0\n",
    "if(n > 0):\n",
    "    for value in sys.argv:\n",
    "        print(\"arg:\", value)\n",
    "        if(\"iterations=\" in value):\n",
    "            try:\n",
    "                iteracoes = int(value.replace(\"iterations=\",\"\"))\n",
    "            except:\n",
    "                print(\"no\")\n",
    "        \n",
    "        if(\"cycle=\" in value):\n",
    "            try:\n",
    "                cycle_index = int(value.replace(\"cycle=\",\"\"))\n",
    "            except:\n",
    "                print(\"no\")\n",
    "        if(\"checkpoint_iteration=\" in value):\n",
    "            try:\n",
    "                iteracoes = int(value.replace(\"iterations=\",\"\"))\n",
    "            except:\n",
    "                print(\"no\")\n",
    "print(\"iteracoes:\",iteracoes)      \n",
    "print(\"cycle:\",cycle_index)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a4484bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input folder\n",
    "#inputFolders = \"../02-transformed-data-new-testes/dados2019/\"\n",
    "inputFolderPath = \"../data_2019_processed/\"\n",
    "\n",
    "# General configuration\n",
    "NUMBER_OF_ITERATIONS_FINAL = 200\n",
    "    \n",
    "NUM_EPOCHS = 1\n",
    "BATCH_SIZE = 32\n",
    "VERBOSE = 0\n",
    "\n",
    "# usado para experimentos\n",
    "if(iteracoes > 0):\n",
    "    NUMBER_OF_ITERATIONS_FINAL = iteracoes\n",
    "    \n",
    "NUMBER_OF_ITERATIONS = NUMBER_OF_ITERATIONS_FINAL\n",
    "\n",
    "# output folder\n",
    "outputFolder = \"result_unbalanced_epoch_\"+str(NUM_EPOCHS)+\"_rounds_\"+str(NUMBER_OF_ITERATIONS_FINAL)+\"_cycle_\"+str(cycle_index)\n",
    "\n",
    "#outputFolder = \"test_checkpoint\"\n",
    "checkPointFolder = outputFolder+\"/checkpoints\"\n",
    "\n",
    "# last cycle\n",
    "last_cycle_index = cycle_index - 1\n",
    "lastCycleOutputFolder = \"result_unbalanced_epoch_\"+str(NUM_EPOCHS)+\"_rounds_\"+str(200)+\"_cycle_\"+str(last_cycle_index)\n",
    "lastCycleOutputFolder = \"result_unbalanced_epoch_\"+str(NUM_EPOCHS)+\"_rounds_\"+str(NUMBER_OF_ITERATIONS_FINAL)+\"_cycle_\"+str(last_cycle_index)\n",
    "iferredCycleDataFolder = lastCycleOutputFolder+\"/inferred_datasets\"\n",
    "\n",
    "# train file name modifier\n",
    "fileSufixTrain = \"\" # _smote for smote\n",
    "\n",
    "fl.common.logger.configure(identifier=\"myFlowerExperiment\", filename=\"log_\"+outputFolder+\".txt\")\n",
    "\n",
    "# usado para checkpoints\n",
    "if(checkpoint_iteration > 0):\n",
    "    NUMBER_OF_ITERATIONS_FINAL = checkpoint_iteration\n",
    "    \n",
    "NUMBER_OF_ITERATIONS = NUMBER_OF_ITERATIONS_FINAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0929b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether the folder exists or not\n",
      "The new directory is created!\n"
     ]
    }
   ],
   "source": [
    "print(\"Checking whether the folder exists or not\")\n",
    "isExist = os.path.exists(outputFolder)\n",
    "if not isExist:\n",
    "    # Create a new directory because it does not exist\n",
    "    os.makedirs(outputFolder)\n",
    "    print(\"The new directory is created!\")\n",
    "else:\n",
    "    print(\"The directory exists!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5249b935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether the checkpoint folder exists or not\n",
      "The new checkpoint directory is created!\n"
     ]
    }
   ],
   "source": [
    "print(\"Checking whether the checkpoint folder exists or not\")\n",
    "isExist = os.path.exists(checkPointFolder)\n",
    "if not isExist:\n",
    "    # Create a new directory because it does not exist\n",
    "    os.makedirs(checkPointFolder)\n",
    "    print(\"The new checkpoint directory is created!\")\n",
    "else:\n",
    "    print(\"The checkpoint directory exists!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58203650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check whether the cycle is 0 > or not, if so, the folder of inference must exist\n",
      "result_unbalanced_epoch_1_rounds_200_cycle_0/inferred_datasets\n",
      "The checkpoint directory exists!\n"
     ]
    }
   ],
   "source": [
    "print(\"check whether the cycle is 0 > or not, if so, the folder of inference must exist\")\n",
    "if(cycle_index > 0):\n",
    "    isExist = os.path.exists(iferredCycleDataFolder)\n",
    "    print(iferredCycleDataFolder)\n",
    "    if not isExist:\n",
    "        print(\"The folder of inference not exists!\")\n",
    "        sys.exit(\"The folder of inference not exists!\")\n",
    "    else:\n",
    "        print(\"The checkpoint directory exists!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e75b3967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected features\n",
    "inputFeatures = [\"activity\",\"location\",\"day_of_week\",\"light\",\"phone_lock\",\"proximity\",\"sound\",\"time_to_next_alarm\", \"minutes_day\"]\n",
    "outputClasses = [\"awake\",\"asleep\"]\n",
    "#outputClasses = [\"class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f7fa6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# client datasets used on the training process (75% of data)\n",
    "trainFolders =  ['0Jf4TH9Zzse0Z1Jjh7SnTOe2MMzeSnFi7feTnkG6vgs',\n",
    "                '0tdmm6rwW3KquQ73ATYYJ5JkpMtvbppJ0VzA2GExdA', \n",
    "                '2cyV53lVyUtlMj0BRwilEWtYJwUiviYoL48cZBPBq0', \n",
    "                '2J22RukYnEbKTk7t+iUVDBkorcyL5NKN6TrLe89ys', \n",
    "                #['5FLZBTVAPwdq9QezHE2sVCJIs7p+r6mCemA2gp9jATk'], #does not have the file\n",
    "                '7EYF5I04EVqisUJCVNHlqn77UAuOmwL2Dahxd3cA', \n",
    "                'a9Qgj8ENWrHvl9QqlXcIPKmyGMKgbfHk9Dbqon1HQP4', \n",
    "                'ae4JJBZDycEcY8McJF+3BxyvZ1619y03BNdCxzpZTc', \n",
    "                'Ch3u5Oaz96VSrQbf0z31X6jEIbeIekkC0mwPzCdeJ1U', \n",
    "                'CH8f0yZkZL13zWuE9ks1CkVJRVrr+jsGdUXHrZ6YeA', \n",
    "                'DHO1K4jgiwZJOfQTrxvKE2vn7hkjamigroGD5IaeRc', \n",
    "                #'DHPqzSqSttiba1L3BD1cptNJPjSxZ8rXxF9mY3za6WA', # does not have asleep data\n",
    "                'dQEFscjqnIlug8Tgq97JohhSQPG2DEOWJqS86wCrcY', \n",
    "                'HFvs2CohmhHte+AaCzFasjzegGzxZKPhkrX23iI6Xo', \n",
    "                'jgB9E8v3Z6PKdTRTCMAijBllA9YEMtrmHbe4qsbmJWw', \n",
    "                'JkY++R7E8myldLN3on6iQ78Ee78zCbrLuggfwGju3I', \n",
    "                'K4SLohf+TN1Ak8Dn8iE3Lme7rEMPISfppB2sXfHX8', \n",
    "                'oGaWetJJJEWHuvYdWYo826SQxfhCExVVQ2da8LE1Y7Q', \n",
    "                'pyt24oiDAHsmgWMvkFKz2fn2pwcHiXchd6KchLM', \n",
    "                #'PZCf1nfvhR+6fk+7+sPNMYOgb8BAMmtQtfoRS83Suc', # does not have asleep data\n",
    "                'QUNCATForxzK0HHw46LrGOMWh0eVA8Y5XWEiUXX+cQ', \n",
    "                #'rIl2UK9+bQ+tzpFdbJAdbBxEa5GbgrgC030yEaENLw', \n",
    "                #'RoBW3cDOO9wWRMPO2twQff83MPc+OXn6gJ+a1DafreI', \n",
    "                'SH3kQeyd5volraxw8vOyhlowNqWBPr1IJ9URNXUL4']\n",
    "                #'VVpwFNMrEglveh6MDN8lrRzTy5OwzglD4FURfM4A2is', \n",
    "                #'Wa1mcNmbh66S7VS6GIzyfCFMD3SGhbtDQyFP1ywJEsw', \n",
    "                #'XCKRE0BWRHxfP1kZIihgtT+jUjSp2GE8v5ZlhcIhVmA', \n",
    "                #'YI5Y79K6GXqAUoGP6PNyII8WKlAoel4urDxWSVVOvBw', \n",
    "                #'ypklj+8GJ15rOIH1lpKQtFJOuK+VdvyCuBPqhY3aoM', \n",
    "                #'ZSsAZ0Pq+MCqFrnjsRFn5Ua09pMCVaOV9c8ZuYb7XQY']\n",
    "            \n",
    "# client datasets used on the training process (25% of data)\n",
    "testFolders =  [#'0Jf4TH9Zzse0Z1Jjh7SnTOe2MMzeSnFi7feTnkG6vgs',\n",
    "                #'0tdmm6rwW3KquQ73ATYYJ5JkpMtvbppJ0VzA2GExdA', \n",
    "                #'2cyV53lVyUtlMj0BRwilEWtYJwUiviYoL48cZBPBq0', \n",
    "                #'2J22RukYnEbKTk7t+iUVDBkorcyL5NKN6TrLe89ys', \n",
    "                #['5FLZBTVAPwdq9QezHE2sVCJIs7p+r6mCemA2gp9jATk'], #does not have the file\n",
    "                #'7EYF5I04EVqisUJCVNHlqn77UAuOmwL2Dahxd3cA', \n",
    "                #'a9Qgj8ENWrHvl9QqlXcIPKmyGMKgbfHk9Dbqon1HQP4', \n",
    "                #'ae4JJBZDycEcY8McJF+3BxyvZ1619y03BNdCxzpZTc', \n",
    "                #'Ch3u5Oaz96VSrQbf0z31X6jEIbeIekkC0mwPzCdeJ1U', \n",
    "                #'CH8f0yZkZL13zWuE9ks1CkVJRVrr+jsGdUXHrZ6YeA', \n",
    "                #'DHO1K4jgiwZJOfQTrxvKE2vn7hkjamigroGD5IaeRc', \n",
    "                #'DHPqzSqSttiba1L3BD1cptNJPjSxZ8rXxF9mY3za6WA', # does not have asleep data\n",
    "                #'dQEFscjqnIlug8Tgq97JohhSQPG2DEOWJqS86wCrcY', \n",
    "                #'HFvs2CohmhHte+AaCzFasjzegGzxZKPhkrX23iI6Xo', \n",
    "                #'jgB9E8v3Z6PKdTRTCMAijBllA9YEMtrmHbe4qsbmJWw', \n",
    "                #'JkY++R7E8myldLN3on6iQ78Ee78zCbrLuggfwGju3I', \n",
    "                #'K4SLohf+TN1Ak8Dn8iE3Lme7rEMPISfppB2sXfHX8', \n",
    "                #'oGaWetJJJEWHuvYdWYo826SQxfhCExVVQ2da8LE1Y7Q', \n",
    "                #'pyt24oiDAHsmgWMvkFKz2fn2pwcHiXchd6KchLM', \n",
    "                #'PZCf1nfvhR+6fk+7+sPNMYOgb8BAMmtQtfoRS83Suc', # does not have asleep data\n",
    "                #'QUNCATForxzK0HHw46LrGOMWh0eVA8Y5XWEiUXX+cQ', \n",
    "                'rIl2UK9+bQ+tzpFdbJAdbBxEa5GbgrgC030yEaENLw', \n",
    "                'RoBW3cDOO9wWRMPO2twQff83MPc+OXn6gJ+a1DafreI', \n",
    "                #'SH3kQeyd5volraxw8vOyhlowNqWBPr1IJ9URNXUL4'] \n",
    "                'VVpwFNMrEglveh6MDN8lrRzTy5OwzglD4FURfM4A2is', \n",
    "                'Wa1mcNmbh66S7VS6GIzyfCFMD3SGhbtDQyFP1ywJEsw', \n",
    "                'XCKRE0BWRHxfP1kZIihgtT+jUjSp2GE8v5ZlhcIhVmA', \n",
    "                'YI5Y79K6GXqAUoGP6PNyII8WKlAoel4urDxWSVVOvBw', \n",
    "                'ypklj+8GJ15rOIH1lpKQtFJOuK+VdvyCuBPqhY3aoM', \n",
    "                'ZSsAZ0Pq+MCqFrnjsRFn5Ua09pMCVaOV9c8ZuYb7XQY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f59ec6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20a8bdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateMetrics(y_test,yhat_probs):\n",
    "    # predict crisp classes for test set deprecated\n",
    "    #yhat_classes = model.predict_classes(X_test, verbose=0)\n",
    "    #yhat_classes = np.argmax(yhat_probs,axis=1)\n",
    "    yhat_classes = yhat_probs.round()\n",
    "    # accuracy: (tp + tn) / (p + n)\n",
    "    accuracy = accuracy_score(y_test, yhat_classes)\n",
    "    # precision tp / (tp + fp)\n",
    "    precision = precision_score(y_test, yhat_classes)\n",
    "    # recall: tp / (tp + fn)\n",
    "    recall = recall_score(y_test, yhat_classes)\n",
    "    # f1: 2 tp / (2 tp + fp + fn)\n",
    "    f1 = f1_score(y_test, yhat_classes)\n",
    "    # kappa\n",
    "    kappa = cohen_kappa_score(y_test, yhat_classes)\n",
    "    # ROC AUC\n",
    "    auc = roc_auc_score(y_test, yhat_probs)\n",
    "    # confusion matrix\n",
    "    matrix = confusion_matrix(y_test, yhat_classes)\n",
    "    #print(matrix)\n",
    "    \n",
    "    array = []\n",
    "    results = dict()\n",
    "    results['accuracy'] = accuracy\n",
    "    results['precision'] = precision\n",
    "    results['recall'] = recall\n",
    "    results['f1_score'] = f1\n",
    "    results['cohen_kappa_score'] = kappa\n",
    "    results['roc_auc_score'] = auc\n",
    "    results['matrix'] = (\"[[ \" +str(matrix[0][0]) + \" \" +str(matrix[0][1]) +\"][ \" +str(matrix[1][0]) + \" \" + str(matrix[1][1]) +\"]]\") # array.append(np.array(matrix,dtype=object))\n",
    "    results['TP'] = matrix[0][0]\n",
    "    results['FP'] = matrix[0][1]\n",
    "    results['FN'] = matrix[1][0]\n",
    "    results['TN'] = matrix[1][1]\n",
    "    \n",
    "    array.append(accuracy)\n",
    "    array.append(precision)\n",
    "    array.append(recall)\n",
    "    array.append(f1)\n",
    "    array.append(kappa)\n",
    "    array.append(auc)\n",
    "    array.append(\"[[ \" +str(matrix[0][0]) + \" \" +str(matrix[0][1]) +\"][ \" +str(matrix[1][0]) + \" \" + str(matrix[1][1]) +\"]]\") # array.append(np.array(matrix,dtype=object))\n",
    "    array.append(matrix[0][0]) # TP\n",
    "    array.append(matrix[0][1]) # FP\n",
    "    array.append(matrix[1][0]) # FN\n",
    "    array.append(matrix[1][1]) # TN\n",
    "    \n",
    "    return results, array\n",
    "\n",
    "# y_test     = Array with real values\n",
    "# yhat_probs = Array with predicted values\n",
    "def printMetrics(y_test,yhat_probs):\n",
    "    # generate metrics\n",
    "    results, array= generateMetrics(y_test,yhat_probs)\n",
    "\n",
    "    # accuracy: (tp + tn) / (p + n)\n",
    "    accuracy = results['accuracy']\n",
    "    print('Accuracy: %f' % accuracy)\n",
    "    # precision tp / (tp + fp)\n",
    "    precision = results['precision']\n",
    "    print('Precision: %f' % precision)\n",
    "    # recall: tp / (tp + fn)\n",
    "    recall = results['recall'] \n",
    "    print('Recall: %f' % recall)\n",
    "    # f1: 2 tp / (2 tp + fp + fn)\n",
    "    f1 = results['f1_score']\n",
    "    print('F1 score: %f' % f1)\n",
    "    # kappa\n",
    "    kappa = results['cohen_kappa_score']\n",
    "    print('Cohens kappa: %f' % kappa)\n",
    "    # ROC AUC\n",
    "    auc = results['roc_auc_score']\n",
    "    print('ROC AUC: %f' % auc)\n",
    "    # confusion matrix\n",
    "    print(\"Confusion Matrix\")\n",
    "    matrix = results['matrix']\n",
    "    print(matrix)\n",
    "    \n",
    "    return results, array\n",
    "\n",
    "def generateGlobalMetrics(metrics):\n",
    "    accuracy,precision,recall,f1_score,cohen_kappa_score,roc_auc_score = 0,0,0,0,0,0\n",
    "    for metric in metrics:\n",
    "        accuracy = accuracy + metric['accuracy']\n",
    "        precision = precision + metric['precision']\n",
    "        recall = recall + metric['recall']\n",
    "        f1_score = f1_score + metric['f1_score']\n",
    "        cohen_kappa_score = cohen_kappa_score + metric['cohen_kappa_score']\n",
    "        roc_auc_score = roc_auc_score + metric['roc_auc_score']\n",
    "        \n",
    "    # mean\n",
    "    size = len(metrics)\n",
    "    print(size)\n",
    "    accuracy = accuracy / size\n",
    "    precision = precision / size\n",
    "    recall = recall / size\n",
    "    f1_score = f1_score / size\n",
    "    cohen_kappa_score = cohen_kappa_score / size\n",
    "    roc_auc_score = roc_auc_score / size\n",
    "    \n",
    "    return [accuracy,precision,recall,f1_score,cohen_kappa_score,roc_auc_score]\n",
    "\n",
    "def showGlobalMetrics(metrics):\n",
    "    res = generateGlobalMetrics(metrics)\n",
    "    \n",
    "    accuracy = res[0]\n",
    "    precision = res[1]\n",
    "    recall = res[2]\n",
    "    f1_score = res[3]\n",
    "    cohen_kappa_score = res[4]\n",
    "    roc_auc_score = res[5]\n",
    "    \n",
    "    #show:\\\n",
    "    print(\"accuracy: \",accuracy)\n",
    "    print(\"precision: \",precision)\n",
    "    print(\"recall: \",recall)\n",
    "    print(\"f1_score: \",f1_score)\n",
    "    print(\"cohen_kappa_score: \",cohen_kappa_score)\n",
    "    print(\"roc_auc_score: \",roc_auc_score)\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0789f112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the list of directories and concat them\n",
    "def loadDataFromFolders(foldersToLoad,inputFolders,fileType = \"\"):\n",
    "    print(len(foldersToLoad), \"datasets\")\n",
    "    for i in range(0,len(foldersToLoad)):\n",
    "        currentFolder = foldersToLoad[i]\n",
    "        print(i , \"-\", currentFolder,inputFolders+\"student_\"+currentFolder+\"_transformed\"+fileType+\".csv\")\n",
    "        #print(trainingDataSet[i])\n",
    "        if(i == 0):\n",
    "            temp_data = pd.read_csv(inputFolders+\"student_\"+currentFolder+\"_transformed\"+fileType+\".csv\")\n",
    "        else:\n",
    "            dataset = pd.read_csv(inputFolders+\"student_\"+currentFolder+\"_transformed\"+fileType+\".csv\")\n",
    "            temp_data = pd.concat([temp_data, dataset])\n",
    "    # return the dataset        \n",
    "    return temp_data\n",
    "\n",
    "# take the list of directories and concat them\n",
    "def loadDataFromLastCycleFoldersOnList():\n",
    "    clientList = []\n",
    "    foldersToLoad = trainFolders\n",
    "    print(len(foldersToLoad), \"datasets\")\n",
    "    for i in range(0,len(foldersToLoad)):\n",
    "        currentFolder = foldersToLoad[i]\n",
    "        print(i , \"-\", currentFolder,iferredCycleDataFolder+\"/student_\"+currentFolder+\".csv\")\n",
    "        #print(trainingDataSet[i])\n",
    "        temp_data = pd.read_csv(iferredCycleDataFolder+\"/student_\"+currentFolder+\".csv\")\n",
    "        temp_data['class'] = temp_data['awake']\n",
    "        temp_data['class'] = temp_data['class'].astype('int32')\n",
    "        \n",
    "        mapper = {0: 'asleep', 1: 'awake'}\n",
    "\n",
    "        temp_data['class'] = temp_data['class'].map(mapper)\n",
    "        #temp_data['class'] = temp_data['class'].apply(lambda tpl: [mapper.get(x) for x in tpl])\n",
    "\n",
    "        #temp_data['fullcoursenames'] = [[mapper.get(x) for x in tpl] for tpl in temp_data['itemsets']]\n",
    "\n",
    "        #del temp_data['awake']\n",
    "        #del temp_data['asleep']\n",
    "        \n",
    "        print(\"Adding to the list: \", temp_data.shape)\n",
    "        clientList.append(temp_data)\n",
    "    # return the dataset        \n",
    "    return clientList\n",
    "\n",
    "# take the list of directories and concat them\n",
    "def loadDataFromFoldersOnList(foldersToLoad,inputFolders,fileType = \"\"):\n",
    "    clientList = []\n",
    "    print(len(foldersToLoad), \"datasets\")\n",
    "    for i in range(0,len(foldersToLoad)):\n",
    "        currentFolder = foldersToLoad[i]\n",
    "        print(i , \"-\", currentFolder,inputFolders+\"student_\"+currentFolder+\"_transformed\"+fileType+\".csv\")\n",
    "        #print(trainingDataSet[i])\n",
    "        temp_data = pd.read_csv(inputFolders+\"student_\"+currentFolder+\"_transformed\"+fileType+\".csv\")\n",
    "        print(\"Adding to the list: \", temp_data.shape)\n",
    "        clientList.append(temp_data)\n",
    "    # return the dataset        \n",
    "    return clientList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0170ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing test data\n",
      "8 datasets\n",
      "0 - rIl2UK9+bQ+tzpFdbJAdbBxEa5GbgrgC030yEaENLw ../data_2019_processed/student_rIl2UK9+bQ+tzpFdbJAdbBxEa5GbgrgC030yEaENLw_transformed.csv\n",
      "1 - RoBW3cDOO9wWRMPO2twQff83MPc+OXn6gJ+a1DafreI ../data_2019_processed/student_RoBW3cDOO9wWRMPO2twQff83MPc+OXn6gJ+a1DafreI_transformed.csv\n",
      "2 - VVpwFNMrEglveh6MDN8lrRzTy5OwzglD4FURfM4A2is ../data_2019_processed/student_VVpwFNMrEglveh6MDN8lrRzTy5OwzglD4FURfM4A2is_transformed.csv\n",
      "3 - Wa1mcNmbh66S7VS6GIzyfCFMD3SGhbtDQyFP1ywJEsw ../data_2019_processed/student_Wa1mcNmbh66S7VS6GIzyfCFMD3SGhbtDQyFP1ywJEsw_transformed.csv\n",
      "4 - XCKRE0BWRHxfP1kZIihgtT+jUjSp2GE8v5ZlhcIhVmA ../data_2019_processed/student_XCKRE0BWRHxfP1kZIihgtT+jUjSp2GE8v5ZlhcIhVmA_transformed.csv\n",
      "5 - YI5Y79K6GXqAUoGP6PNyII8WKlAoel4urDxWSVVOvBw ../data_2019_processed/student_YI5Y79K6GXqAUoGP6PNyII8WKlAoel4urDxWSVVOvBw_transformed.csv\n",
      "6 - ypklj+8GJ15rOIH1lpKQtFJOuK+VdvyCuBPqhY3aoM ../data_2019_processed/student_ypklj+8GJ15rOIH1lpKQtFJOuK+VdvyCuBPqhY3aoM_transformed.csv\n",
      "7 - ZSsAZ0Pq+MCqFrnjsRFn5Ua09pMCVaOV9c8ZuYb7XQY ../data_2019_processed/student_ZSsAZ0Pq+MCqFrnjsRFn5Ua09pMCVaOV9c8ZuYb7XQY_transformed.csv\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 134888 entries, 0 to 23751\n",
      "Data columns (total 12 columns):\n",
      " #   Column              Non-Null Count   Dtype  \n",
      "---  ------              --------------   -----  \n",
      " 0   activity            134888 non-null  float64\n",
      " 1   location            134888 non-null  float64\n",
      " 2   timestamp           134888 non-null  float64\n",
      " 3   time_to_next_alarm  134888 non-null  float64\n",
      " 4   sound               134888 non-null  float64\n",
      " 5   proximity           134888 non-null  float64\n",
      " 6   phone_lock          134888 non-null  float64\n",
      " 7   light               134888 non-null  float64\n",
      " 8   day_of_week         134888 non-null  float64\n",
      " 9   minutes_day         134888 non-null  float64\n",
      " 10  timestamp_text      134888 non-null  object \n",
      " 11  class               134888 non-null  object \n",
      "dtypes: float64(10), object(2)\n",
      "memory usage: 13.4+ MB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activity</th>\n",
       "      <th>location</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>time_to_next_alarm</th>\n",
       "      <th>sound</th>\n",
       "      <th>proximity</th>\n",
       "      <th>phone_lock</th>\n",
       "      <th>light</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>minutes_day</th>\n",
       "      <th>timestamp_text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.75</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.515992</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.678249</td>\n",
       "      <td>2018-05-14 16:16:08+00:00</td>\n",
       "      <td>asleep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.211282e-07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.542171</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.678944</td>\n",
       "      <td>2018-05-14 16:17:39+00:00</td>\n",
       "      <td>asleep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.422564e-07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.515992</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.679639</td>\n",
       "      <td>2018-05-14 16:18:39+00:00</td>\n",
       "      <td>asleep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.422564e-07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.515992</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.680334</td>\n",
       "      <td>2018-05-14 16:19:09+00:00</td>\n",
       "      <td>asleep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.422564e-07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.531341</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.681028</td>\n",
       "      <td>2018-05-14 16:20:09+00:00</td>\n",
       "      <td>asleep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23747</th>\n",
       "      <td>0.25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.819100e-03</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000236</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.510076</td>\n",
       "      <td>2018-06-13 12:14:37+00:00</td>\n",
       "      <td>awake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23748</th>\n",
       "      <td>0.25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.819743e-03</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000325</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.512856</td>\n",
       "      <td>2018-06-13 12:18:08+00:00</td>\n",
       "      <td>awake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23749</th>\n",
       "      <td>0.25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.819743e-03</td>\n",
       "      <td>0.000595</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000325</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.513551</td>\n",
       "      <td>2018-06-13 12:19:08+00:00</td>\n",
       "      <td>awake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23750</th>\n",
       "      <td>0.25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.820064e-03</td>\n",
       "      <td>0.000595</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000354</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.513551</td>\n",
       "      <td>2018-06-13 12:19:38+00:00</td>\n",
       "      <td>awake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23751</th>\n",
       "      <td>0.50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.820064e-03</td>\n",
       "      <td>0.000496</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.514246</td>\n",
       "      <td>2018-06-13 12:20:08+00:00</td>\n",
       "      <td>awake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>134888 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       activity  location     timestamp  time_to_next_alarm     sound  \\\n",
       "0          0.75       1.0  0.000000e+00            0.000000  0.515992   \n",
       "1          0.25       1.0  3.211282e-07            0.000000  0.542171   \n",
       "2          0.25       1.0  6.422564e-07            0.000000  0.515992   \n",
       "3          0.00       1.0  6.422564e-07            0.000000  0.515992   \n",
       "4          0.25       1.0  6.422564e-07            0.000000  0.531341   \n",
       "...         ...       ...           ...                 ...       ...   \n",
       "23747      0.25       1.0  5.819100e-03            0.000099  0.000000   \n",
       "23748      0.25       1.0  5.819743e-03            0.000694  0.000000   \n",
       "23749      0.25       1.0  5.819743e-03            0.000595  0.000000   \n",
       "23750      0.25       1.0  5.820064e-03            0.000595  0.000000   \n",
       "23751      0.50       1.0  5.820064e-03            0.000496  0.000000   \n",
       "\n",
       "       proximity  phone_lock     light  day_of_week  minutes_day  \\\n",
       "0            1.0         0.0  0.000000     1.000000     0.678249   \n",
       "1            0.0         1.0  0.000007     1.000000     0.678944   \n",
       "2            0.0         1.0  0.000000     1.000000     0.679639   \n",
       "3            0.0         1.0  0.000000     1.000000     0.680334   \n",
       "4            0.0         1.0  0.000000     1.000000     0.681028   \n",
       "...          ...         ...       ...          ...          ...   \n",
       "23747        1.0         1.0  0.000236     0.166667     0.510076   \n",
       "23748        1.0         1.0  0.000325     0.166667     0.512856   \n",
       "23749        1.0         1.0  0.000325     0.166667     0.513551   \n",
       "23750        1.0         1.0  0.000354     0.166667     0.513551   \n",
       "23751        0.0         1.0  0.000000     0.166667     0.514246   \n",
       "\n",
       "                  timestamp_text   class  \n",
       "0      2018-05-14 16:16:08+00:00  asleep  \n",
       "1      2018-05-14 16:17:39+00:00  asleep  \n",
       "2      2018-05-14 16:18:39+00:00  asleep  \n",
       "3      2018-05-14 16:19:09+00:00  asleep  \n",
       "4      2018-05-14 16:20:09+00:00  asleep  \n",
       "...                          ...     ...  \n",
       "23747  2018-06-13 12:14:37+00:00   awake  \n",
       "23748  2018-06-13 12:18:08+00:00   awake  \n",
       "23749  2018-06-13 12:19:08+00:00   awake  \n",
       "23750  2018-06-13 12:19:38+00:00   awake  \n",
       "23751  2018-06-13 12:20:08+00:00   awake  \n",
       "\n",
       "[134888 rows x 12 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Preparing test data\")\n",
    " \n",
    "# test data comprising 25% of the data. It must be fixed to all models being evaluated\n",
    "#X_test  = pd.read_csv(inputFolders+\"test/allData-classification-numeric-normalized.csv\")\n",
    "X_test = loadDataFromFolders(testFolders,inputFolderPath,\"\")\n",
    "\n",
    "print()\n",
    "# undestand the dataset by looking on their infos\n",
    "print(X_test.info())\n",
    "\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77bed070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing X_train data\n",
      "Gething data from human inference folder\n",
      "19 datasets\n",
      "0 - 0Jf4TH9Zzse0Z1Jjh7SnTOe2MMzeSnFi7feTnkG6vgs result_unbalanced_epoch_1_rounds_200_cycle_0/inferred_datasets/student_0Jf4TH9Zzse0Z1Jjh7SnTOe2MMzeSnFi7feTnkG6vgs.csv\n",
      "Adding to the list:  (17993, 12)\n",
      "1 - 0tdmm6rwW3KquQ73ATYYJ5JkpMtvbppJ0VzA2GExdA result_unbalanced_epoch_1_rounds_200_cycle_0/inferred_datasets/student_0tdmm6rwW3KquQ73ATYYJ5JkpMtvbppJ0VzA2GExdA.csv\n",
      "Adding to the list:  (11561, 12)\n",
      "2 - 2cyV53lVyUtlMj0BRwilEWtYJwUiviYoL48cZBPBq0 result_unbalanced_epoch_1_rounds_200_cycle_0/inferred_datasets/student_2cyV53lVyUtlMj0BRwilEWtYJwUiviYoL48cZBPBq0.csv\n",
      "Adding to the list:  (3383, 12)\n",
      "3 - 2J22RukYnEbKTk7t+iUVDBkorcyL5NKN6TrLe89ys result_unbalanced_epoch_1_rounds_200_cycle_0/inferred_datasets/student_2J22RukYnEbKTk7t+iUVDBkorcyL5NKN6TrLe89ys.csv\n",
      "Adding to the list:  (19389, 12)\n",
      "4 - 7EYF5I04EVqisUJCVNHlqn77UAuOmwL2Dahxd3cA result_unbalanced_epoch_1_rounds_200_cycle_0/inferred_datasets/student_7EYF5I04EVqisUJCVNHlqn77UAuOmwL2Dahxd3cA.csv\n",
      "Adding to the list:  (2753, 12)\n",
      "5 - a9Qgj8ENWrHvl9QqlXcIPKmyGMKgbfHk9Dbqon1HQP4 result_unbalanced_epoch_1_rounds_200_cycle_0/inferred_datasets/student_a9Qgj8ENWrHvl9QqlXcIPKmyGMKgbfHk9Dbqon1HQP4.csv\n",
      "Adding to the list:  (26567, 12)\n",
      "6 - ae4JJBZDycEcY8McJF+3BxyvZ1619y03BNdCxzpZTc result_unbalanced_epoch_1_rounds_200_cycle_0/inferred_datasets/student_ae4JJBZDycEcY8McJF+3BxyvZ1619y03BNdCxzpZTc.csv\n",
      "Adding to the list:  (24534, 12)\n",
      "7 - Ch3u5Oaz96VSrQbf0z31X6jEIbeIekkC0mwPzCdeJ1U result_unbalanced_epoch_1_rounds_200_cycle_0/inferred_datasets/student_Ch3u5Oaz96VSrQbf0z31X6jEIbeIekkC0mwPzCdeJ1U.csv\n",
      "Adding to the list:  (33903, 12)\n",
      "8 - CH8f0yZkZL13zWuE9ks1CkVJRVrr+jsGdUXHrZ6YeA result_unbalanced_epoch_1_rounds_200_cycle_0/inferred_datasets/student_CH8f0yZkZL13zWuE9ks1CkVJRVrr+jsGdUXHrZ6YeA.csv\n",
      "Adding to the list:  (26440, 12)\n",
      "9 - DHO1K4jgiwZJOfQTrxvKE2vn7hkjamigroGD5IaeRc result_unbalanced_epoch_1_rounds_200_cycle_0/inferred_datasets/student_DHO1K4jgiwZJOfQTrxvKE2vn7hkjamigroGD5IaeRc.csv\n",
      "Adding to the list:  (24484, 12)\n",
      "10 - dQEFscjqnIlug8Tgq97JohhSQPG2DEOWJqS86wCrcY result_unbalanced_epoch_1_rounds_200_cycle_0/inferred_datasets/student_dQEFscjqnIlug8Tgq97JohhSQPG2DEOWJqS86wCrcY.csv\n",
      "Adding to the list:  (26020, 12)\n",
      "11 - HFvs2CohmhHte+AaCzFasjzegGzxZKPhkrX23iI6Xo result_unbalanced_epoch_1_rounds_200_cycle_0/inferred_datasets/student_HFvs2CohmhHte+AaCzFasjzegGzxZKPhkrX23iI6Xo.csv\n",
      "Adding to the list:  (25932, 12)\n",
      "12 - jgB9E8v3Z6PKdTRTCMAijBllA9YEMtrmHbe4qsbmJWw result_unbalanced_epoch_1_rounds_200_cycle_0/inferred_datasets/student_jgB9E8v3Z6PKdTRTCMAijBllA9YEMtrmHbe4qsbmJWw.csv\n",
      "Adding to the list:  (31873, 12)\n",
      "13 - JkY++R7E8myldLN3on6iQ78Ee78zCbrLuggfwGju3I result_unbalanced_epoch_1_rounds_200_cycle_0/inferred_datasets/student_JkY++R7E8myldLN3on6iQ78Ee78zCbrLuggfwGju3I.csv\n",
      "Adding to the list:  (23244, 12)\n",
      "14 - K4SLohf+TN1Ak8Dn8iE3Lme7rEMPISfppB2sXfHX8 result_unbalanced_epoch_1_rounds_200_cycle_0/inferred_datasets/student_K4SLohf+TN1Ak8Dn8iE3Lme7rEMPISfppB2sXfHX8.csv\n",
      "Adding to the list:  (19595, 12)\n",
      "15 - oGaWetJJJEWHuvYdWYo826SQxfhCExVVQ2da8LE1Y7Q result_unbalanced_epoch_1_rounds_200_cycle_0/inferred_datasets/student_oGaWetJJJEWHuvYdWYo826SQxfhCExVVQ2da8LE1Y7Q.csv\n",
      "Adding to the list:  (21669, 12)\n",
      "16 - pyt24oiDAHsmgWMvkFKz2fn2pwcHiXchd6KchLM result_unbalanced_epoch_1_rounds_200_cycle_0/inferred_datasets/student_pyt24oiDAHsmgWMvkFKz2fn2pwcHiXchd6KchLM.csv\n",
      "Adding to the list:  (33344, 12)\n",
      "17 - QUNCATForxzK0HHw46LrGOMWh0eVA8Y5XWEiUXX+cQ result_unbalanced_epoch_1_rounds_200_cycle_0/inferred_datasets/student_QUNCATForxzK0HHw46LrGOMWh0eVA8Y5XWEiUXX+cQ.csv\n",
      "Adding to the list:  (22059, 12)\n",
      "18 - SH3kQeyd5volraxw8vOyhlowNqWBPr1IJ9URNXUL4 result_unbalanced_epoch_1_rounds_200_cycle_0/inferred_datasets/student_SH3kQeyd5volraxw8vOyhlowNqWBPr1IJ9URNXUL4.csv\n",
      "Adding to the list:  (12709, 12)\n",
      "Total 19\n"
     ]
    }
   ],
   "source": [
    "print(\"Preparing X_train data\")\n",
    "# load cliend data\n",
    "if(cycle_index == 0):\n",
    "    print(\"Gething data from training folder\")\n",
    "    clientList = loadDataFromFoldersOnList(trainFolders,inputFolderPath,fileSufixTrain)\n",
    "else:\n",
    "    print(\"Gething data from human inference folder\")\n",
    "    clientList = loadDataFromLastCycleFoldersOnList()\n",
    "\n",
    "NUMBER_OF_CLIENTS = len(clientList)\n",
    "print(\"Total\",(len(clientList)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7f3d997",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clientList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2de66611",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for t in clientList:\n",
    "#    print(collections.Counter(t['class']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65fe16dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train =  loadDataFromFoldersOnList(trainFolders,inputFolderPath,fileSufixTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0e7f2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c86ff2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 134888 entries, 0 to 23751\n",
      "Data columns (total 14 columns):\n",
      " #   Column              Non-Null Count   Dtype  \n",
      "---  ------              --------------   -----  \n",
      " 0   activity            134888 non-null  float64\n",
      " 1   location            134888 non-null  float64\n",
      " 2   timestamp           134888 non-null  float64\n",
      " 3   time_to_next_alarm  134888 non-null  float64\n",
      " 4   sound               134888 non-null  float64\n",
      " 5   proximity           134888 non-null  float64\n",
      " 6   phone_lock          134888 non-null  float64\n",
      " 7   light               134888 non-null  float64\n",
      " 8   day_of_week         134888 non-null  float64\n",
      " 9   minutes_day         134888 non-null  float64\n",
      " 10  timestamp_text      134888 non-null  object \n",
      " 11  class               134888 non-null  object \n",
      " 12  awake               134888 non-null  bool   \n",
      " 13  asleep              134888 non-null  bool   \n",
      "dtypes: bool(2), float64(10), object(2)\n",
      "memory usage: 13.6+ MB\n"
     ]
    }
   ],
   "source": [
    "# one-hot encoding function\n",
    "def transform_output_nominal_class_into_one_hot_encoding(dataset):\n",
    "    # create two classes based on the single class\n",
    "    one_hot_encoded_data = pd.get_dummies(dataset['class'])\n",
    "    #print(one_hot_encoded_data)\n",
    "    dataset['awake'] = one_hot_encoded_data['awake']\n",
    "    dataset['asleep'] = one_hot_encoded_data['asleep']\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# one-hot encoding function\n",
    "def transform_output_numerical_class_into_one_hot_encoding(dataset):\n",
    "    # create two classes based on the single class\n",
    "    one_hot_encoded_data = pd.get_dummies(dataset['class'])\n",
    "    #print(one_hot_encoded_data)\n",
    "    dataset['awake'] = one_hot_encoded_data[0]\n",
    "    dataset['asleep'] = one_hot_encoded_data[1]\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# transform output to one_hot_encoding for the testing dataset\n",
    "X_test = transform_output_nominal_class_into_one_hot_encoding(X_test)\n",
    "\n",
    "# transform output to one_hot_encoding for the input dataset\n",
    "for i in range(0,len(clientList)):\n",
    "    clientList[i] = transform_output_nominal_class_into_one_hot_encoding(clientList[i])\n",
    "    #print (clientList[i])\n",
    "    \n",
    "\n",
    "X_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a39a58ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 134888 entries, 0 to 23751\n",
      "Data columns (total 14 columns):\n",
      " #   Column              Non-Null Count   Dtype  \n",
      "---  ------              --------------   -----  \n",
      " 0   activity            134888 non-null  float32\n",
      " 1   location            134888 non-null  float32\n",
      " 2   timestamp           134888 non-null  float64\n",
      " 3   time_to_next_alarm  134888 non-null  float32\n",
      " 4   sound               134888 non-null  float32\n",
      " 5   proximity           134888 non-null  float32\n",
      " 6   phone_lock          134888 non-null  float32\n",
      " 7   light               134888 non-null  float32\n",
      " 8   day_of_week         134888 non-null  float32\n",
      " 9   minutes_day         134888 non-null  float32\n",
      " 10  timestamp_text      134888 non-null  object \n",
      " 11  class               134888 non-null  object \n",
      " 12  awake               134888 non-null  float32\n",
      " 13  asleep              134888 non-null  float32\n",
      "dtypes: float32(11), float64(1), object(2)\n",
      "memory usage: 9.8+ MB\n"
     ]
    }
   ],
   "source": [
    "def transform_data_type(dataframe):\n",
    "    \n",
    "    # transform inputs\n",
    "    for column in inputFeatures:\n",
    "        dataframe[column] = dataframe[column].astype('float32')\n",
    "    \n",
    "    # transform outputs\n",
    "    for column in outputClasses:\n",
    "        dataframe[column] = dataframe[column].astype('float32')\n",
    "    \n",
    "    return dataframe\n",
    "\n",
    "# transforms the data\n",
    "X_test = transform_data_type(X_test)\n",
    "\n",
    "X_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ec93564f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepering the test dataset\n"
     ]
    }
   ],
   "source": [
    "print(\"Prepering the test dataset\")\n",
    "# selects the data to train and test\n",
    "X_test_data = X_test[inputFeatures]\n",
    "y_test_label = X_test[outputClasses]\n",
    "\n",
    "# transtorm data to tensor slices\n",
    "#client_test_dataset = tf.data.Dataset.from_tensor_slices((X_test_data.values, y_test_label.values))\n",
    "\n",
    "#client_test_dataset = client_test_dataset.repeat(NUM_EPOCHS).batch(BATCH_SIZE, drop_remainder=True)\n",
    "#client_test_dataset = client_test_dataset.repeat(NUM_EPOCHS).batch(BATCH_SIZE)\n",
    "\n",
    "#print(client_test_dataset.element_spec)\n",
    "#client_test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2da70f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"preparing the training datasets\")\n",
    "#federated_training_data = []\n",
    "# transform the data\n",
    "#for i in range(0,len(clientList)):\n",
    "#    # selects the data to train and test\n",
    "#    data   = clientList[i][inputFeatures]\n",
    "#    labels = clientList[i][outputClasses]\n",
    "#    # transform the data to tensor slices\n",
    "#    client_train_dataset = tf.data.Dataset.from_tensor_slices((data.values, labels.values))\n",
    "    # apply the configs\n",
    "#    client_train_dataset = client_train_dataset.repeat(NUM_EPOCHS).batch(BATCH_SIZE)\n",
    "    # transform the data to\n",
    " #   federated_training_data.append(client_train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2a9b73de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating model\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 16)                160       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 8)                 136       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 2)                 18        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 314\n",
      "Trainable params: 314\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-15 18:03:42.780401: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-15 18:03:42.814383: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2024-02-15 18:03:42.814405: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2024-02-15 18:03:42.815568: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "print(\"creating model\")\n",
    "\n",
    "def create_keras_model():\n",
    "    return tf.keras.models.Sequential([\n",
    "      tf.keras.layers.InputLayer(input_shape=(9,)),\n",
    "      #tf.keras.layers.Dense(9, activation=tf.keras.activations.relu), \n",
    "      tf.keras.layers.Dense(16, activation=tf.keras.activations.relu),\n",
    "      tf.keras.layers.Dense(8, activation=tf.keras.activations.relu),\n",
    "      tf.keras.layers.Dense(2, activation=tf.keras.activations.softmax)\n",
    "      #tf.keras.layers.Dense(2, activation=tf.nn.sigmoid)\n",
    "    ])\n",
    "\n",
    "keras_model = create_keras_model()\n",
    "#keras_model.summary()\n",
    "keras_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "35447b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and data (MobileNetV2, CIFAR-10)\n",
    "#model = keras_model\n",
    "#model.compile(\"adam\", \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "#(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "362b0f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_and_save_results(keras_model,X_test_data, y_test_label, current_round_index, \n",
    "                              clientId, prefix_string = \"Results\", lossValue = -1):\n",
    "     # predict values\n",
    "    yhat_probs = keras_model.predict(X_test_data,verbose=VERBOSE)\n",
    "    \n",
    "    # as we deal with a classification problem with one hot encoding, we must round the values to 0 and 1.\n",
    "    yhat_probs_rounded = yhat_probs.round()\n",
    "    \n",
    "    # create a dataframe with the predicted data\n",
    "    y_predicted_df = pd.DataFrame(data=yhat_probs_rounded,columns=['awake','asleep']) \n",
    "    #y_test_label_label = pd.DataFrame(data=y_test_label,columns=['awake','asleep']) \n",
    "    \n",
    "    roundData = []\n",
    "\n",
    "    columns = ['client','round','loss','class','accuracy','precision','recall', \n",
    "               'f1_score','cohen_kappa_score','roc_auc_score','confusion_matrix',\n",
    "               'TP','FP','FN','TN']\n",
    "    \n",
    "    # Instantiate the list that will contain the results\n",
    "    listOfMetrics = list()\n",
    "    \n",
    "    #print('awake')    \n",
    "    #res,resA = printMetrics(y_test_label['awake'],y_predicted_df['awake'])\n",
    "    res,resA = generateMetrics(y_test_label['awake'],y_predicted_df['awake'])\n",
    "    listOfMetrics.append(res)\n",
    "    \n",
    "    classData = np.concatenate(([clientId,current_round_index,lossValue,'awake'], resA))\n",
    "    roundData.append(classData)\n",
    "    \n",
    "    #print('')\n",
    "    #print('asleep')\n",
    "    #res,resA = printMetrics(y_test_label['asleep'],y_predicted_df['asleep'])\n",
    "    res,resA = generateMetrics(y_test_label['asleep'],y_predicted_df['asleep'])\n",
    "    listOfMetrics.append(res)\n",
    "    # new data\n",
    "    classData = np.concatenate(([clientId,current_round_index,lossValue,'asleep'], resA))\n",
    "    roundData.append(classData)\n",
    "    \n",
    "    #print('Global')\n",
    "    #resA = showGlobalMetrics(listOfMetrics) #return [accuracy,precision,recall,f1_score,cohen_kappa_score,roc_auc_score\n",
    "    resA = generateGlobalMetrics(listOfMetrics) #return [accuracy,precision,recall,f1_score,cohen_kappa_score,roc_auc_score\n",
    "    # new data\n",
    "    classData = np.concatenate(([clientId,current_round_index,lossValue,'avg'], resA))\n",
    "    roundData.append(classData)\n",
    "    \n",
    "    dataMetrics = pd.DataFrame(data=roundData,columns=columns) \n",
    "    # write file\n",
    "    if(clientId >= 0):\n",
    "        outputMetricFile = outputFolder+\"/\"+prefix_string+\"_MLP_client_\" + str(clientId) + \"_round_\" + str(current_round_index) + \".csv\"\n",
    "    else:\n",
    "        outputMetricFile = outputFolder+\"/global_model_MLP_metrics.csv\"\n",
    "        outputMetricFile = outputFolder+\"/\"+prefix_string+\".csv\"\n",
    "        # print global model results\n",
    "        if(os.path.isfile(outputMetricFile)):\n",
    "            dataset = pd.read_csv(outputMetricFile)\n",
    "            dataMetrics = pd.concat([dataset, dataMetrics], axis=0)\n",
    "        # Perform garbage collection\n",
    "        gc.collect()\n",
    "        \n",
    "    dataMetrics.to_csv(outputMetricFile, sep=',', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ab1e2479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint model result_unbalanced_epoch_1_rounds_5_cycle_1/checkpoints/round-*\n",
      "No checkpoint found\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading checkpoint model\",checkPointFolder+\"/round-*\")\n",
    "list_of_files = [fname for fname in glob.glob(checkPointFolder+\"/round-*\")]\n",
    "last_round_checkpoint = -1\n",
    "latest_round_file = None\n",
    "model_check_point = None\n",
    "filename_np = None\n",
    "filename_h5 = None\n",
    "\n",
    "if len(list_of_files) > 0:\n",
    "    latest_round_file = max(list_of_files, key=os.path.getctime)\n",
    "    print(\"Loading pre-trained model from: \", latest_round_file)\n",
    "    if(len(latest_round_file) > 0):\n",
    "        # load the name\n",
    "        last_round = latest_round_file.replace(checkPointFolder+\"/round-\",\"\")\n",
    "        last_round = last_round.replace(\"-weights.npz\",\"\")\n",
    "        last_round = last_round.replace(\"-weights.h5\",\"\")\n",
    "        print(\"Last round: \",last_round)\n",
    "    \n",
    "        last_round_checkpoint = int(last_round)\n",
    "        filename_h5 = checkPointFolder+\"/round-\"+last_round+\"-weights.h5\"\n",
    "        filename_np = checkPointFolder+\"/round-\"+last_round+\"-weights.npz\"\n",
    "else:\n",
    "    print(\"No checkpoint found\")\n",
    "\n",
    "    #check_point_model = tf.keras.models.load_model(latest_round_file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7f8aeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "722dabf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_round_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bf9bb93d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#if latest_round_file is not None:\n",
    "#    keras_model.load_weights(latest_round_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5161a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "35c16138",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUMBER_OF_ITERATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "10217727",
   "metadata": {},
   "outputs": [],
   "source": [
    "if(last_round_checkpoint > -1):\n",
    "    NUMBER_OF_ITERATIONS = NUMBER_OF_ITERATIONS_FINAL - (last_round_checkpoint)\n",
    "\n",
    "    print(\"Number of iteractions after the checkpoint: \",NUMBER_OF_ITERATIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4dc894c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveModelStrategy(fl.server.strategy.FedAvg):\n",
    "    def aggregate_fit(\n",
    "        self,\n",
    "        server_round: int,\n",
    "        results: List[Tuple[fl.server.client_proxy.ClientProxy, fl.common.FitRes]],\n",
    "        failures: List[Union[Tuple[ClientProxy, FitRes], BaseException]],\n",
    "    ) -> Tuple[Optional[Parameters], Dict[str, Scalar]]:\n",
    "\n",
    "        # Call aggregate_fit from base class (FedAvg) to aggregate parameters and metrics\n",
    "        aggregated_parameters, aggregated_metrics = super().aggregate_fit(server_round, results, failures)\n",
    "        \n",
    "        #print(\"TEsteeee\", aggregated_parameters)\n",
    "        if aggregated_parameters is not None:\n",
    "            # Convert `Parameters` to `List[np.ndarray]`\n",
    "            aggregated_ndarrays: List[np.ndarray] = fl.common.parameters_to_ndarrays(aggregated_parameters)\n",
    "\n",
    "            # Save aggregated_ndarrays\n",
    "            print(f\"Saving round {server_round} aggregated_ndarrays...\")\n",
    "            fileName = f\"{checkPointFolder}/round-{server_round}-weights.npz\"\n",
    "            print(fileName)\n",
    "            #print(aggregated_parameters)\n",
    "            print()\n",
    "            np.savez(fileName, *aggregated_ndarrays)\n",
    "            #np.savez(fileName+\"2\", *aggregated_parameters)\n",
    "            #keras_model = create_keras_model()\n",
    "            #keras_model.set_weights(aggregated_parameters)\n",
    "            #keras_model.save_weights(fileName)\n",
    "\n",
    "        return aggregated_parameters, aggregated_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c71c6242",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Declarating client function\n"
     ]
    }
   ],
   "source": [
    "print(\"Declarating client function\")\n",
    "\n",
    "# Define a Flower client\n",
    "class FlowerISABELASleepClient(fl.client.NumPyClient):\n",
    "\n",
    "    def __init__(self, clientId, model, X_train_data, y_train_label,round_index=0):\n",
    "        self.round_index = round_index\n",
    "        self.clientId = clientId\n",
    "        self.model = model\n",
    "        self.X_train_data = X_train_data\n",
    "        self.y_train_label = y_train_label\n",
    "\n",
    "\n",
    "    def get_parameters(self, config):\n",
    "        \"\"\"Return current weights.\"\"\"\n",
    "        return self.model.get_weights()\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "               \n",
    "        \"\"\"Fit model and return new weights as well as number of training examples.\"\"\"\n",
    "        self.model.set_weights(parameters)\n",
    "        \n",
    "        \n",
    "        # Evaluate local model parameters on the local test data\n",
    "        loss, accuracy = self.model.evaluate(self.X_train_data, self.y_train_label,verbose=VERBOSE)       \n",
    "        # print model results (verify the quality from the proxy data)\n",
    "        evaluate_and_save_results(self.model,self.X_train_data, self.y_train_label, self.round_index, self.clientId,\"local_data_before_fit\",loss)\n",
    " \n",
    "        # use the checkpoint if it is not -1\n",
    "        #if(last_round_checkpoint == self.round_index and last_round_checkpoint != -1):\n",
    "        #    print(\"loading checkpoint: \",filename_h5, \" to client \",self.clientId)\n",
    "        #    print(\"loading\", latest_round_file)\n",
    "        #    self.model = tf.keras.models.load_weights(filename_h5)\n",
    "            \n",
    "        self.model.fit(self.X_train_data, self.y_train_label, epochs=NUM_EPOCHS, batch_size=BATCH_SIZE,verbose=VERBOSE)\n",
    "\n",
    "        # Evaluate local model parameters on the local test data\n",
    "        loss, accuracy = self.model.evaluate(X_test_data, y_test_label,verbose=VERBOSE)\n",
    "\n",
    "        # print model results\n",
    "        evaluate_and_save_results(self.model,X_test_data, y_test_label, self.round_index, self.clientId,\"proxy_data_after_fit\",loss)\n",
    "        return self.model.get_weights(), len(self.X_train_data), {}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c336f022",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_evaluate_fn( model):\n",
    "    \n",
    "    def evaluate(\n",
    "        server_round: int, parameters: NDArrays, config: Dict[str, Scalar]\n",
    "    ) -> Optional[Tuple[float, Dict[str, Scalar]]]:\n",
    "        \n",
    "        current_round = server_round\n",
    "        \n",
    "        if(last_round_checkpoint > -1):\n",
    "            current_round = server_round + last_round_checkpoint\n",
    "            \n",
    "        print(\"Evaluating global model round\",current_round)\n",
    "        \n",
    "        model.set_weights(parameters)\n",
    "        \n",
    "        # Evaluate local model parameters on the local test data\n",
    "        loss, accuracy = model.evaluate(X_test_data, y_test_label,verbose=VERBOSE)\n",
    "\n",
    "        # only saves if the server_round + last_round_checkpoint != last_round_checkpoint to avaid duble metrics\n",
    "        if(current_round > last_round_checkpoint):\n",
    "            # print model results\n",
    "            evaluate_and_save_results(model,X_test_data, y_test_label, current_round, -1,\"global_model_metrics_after_agregation\",loss)\n",
    "\n",
    "            #checkpoint\n",
    "            fileName = f\"{checkPointFolder}/round-{current_round}-weights.h5\"\n",
    "            model.save_weights(fileName)\n",
    "        else:\n",
    "            print(\"Round already evaluated\")\n",
    "        \n",
    "        return loss, { 'accuracy': accuracy }\n",
    "    return evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f6e8c638",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO flwr 2024-02-15 18:03:49,890 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=5, round_timeout=None)\n",
      "2024-02-15 18:03:51,824\tINFO worker.py:1621 -- Started a local Ray instance.\n",
      "INFO flwr 2024-02-15 18:03:52,946 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 16.0, 'node:__internal_head__': 1.0, 'node:172.30.126.159': 1.0, 'object_store_memory': 2084240179.0, 'memory': 4168480359.0}\n",
      "INFO flwr 2024-02-15 18:03:52,947 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html\n",
      "INFO flwr 2024-02-15 18:03:52,948 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 1}\n",
      "INFO flwr 2024-02-15 18:03:52,963 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 16 actors\n",
      "INFO flwr 2024-02-15 18:03:52,965 | server.py:89 | Initializing global parameters\n",
      "INFO flwr 2024-02-15 18:03:52,966 | server.py:276 | Requesting initial parameters from one random client\n",
      "INFO flwr 2024-02-15 18:03:58,550 | server.py:280 | Received initial parameters from one random client\n",
      "INFO flwr 2024-02-15 18:03:58,552 | server.py:91 | Evaluating initial parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(DefaultActor pid=24189)\u001b[0m starting client: 4 <class 'str'>\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24189)\u001b[0m starting client:  <class 'int'>\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24189)\u001b[0m Creating client model to client: 4\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24189)\u001b[0m Data X: 2753\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24189)\u001b[0m Data Y: 2753\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24189)\u001b[0m Creating client model to client: 4 round 0\n",
      "Evaluating global model round 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(DefaultActor pid=24189)\u001b[0m /home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py:72: DeprecationWarning:  Ensure your client is of type `flwr.client.Client`. Please convert it using the `.to_client()` method before returning it in the `client_fn` you pass to `start_simulation`. We have applied this conversion on your behalf. Not returning a `Client` might trigger an error in future versions of Flower.\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24189)\u001b[0m   client = check_clientfn_returns_client(client_fn(cid))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO flwr 2024-02-15 18:04:05,787 | server.py:94 | initial parameters (loss, other metrics): 0.6753955483436584, {'accuracy': 0.6329399347305298}\n",
      "INFO flwr 2024-02-15 18:04:05,788 | server.py:104 | FL starting\n",
      "DEBUG flwr 2024-02-15 18:04:05,789 | server.py:222 | fit_round 1: strategy sampled 19 clients (out of 19)\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24189)\u001b[0m /home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py:72: DeprecationWarning:  Ensure your client is of type `flwr.client.Client`. Please convert it using the `.to_client()` method before returning it in the `client_fn` you pass to `start_simulation`. We have applied this conversion on your behalf. Not returning a `Client` might trigger an error in future versions of Flower.\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24189)\u001b[0m   client = check_clientfn_returns_client(client_fn(cid))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(DefaultActor pid=24189)\u001b[0m starting client: 15 <class 'str'>\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24189)\u001b[0m starting client:  <class 'int'>\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24189)\u001b[0m Creating client model to client: 15\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24189)\u001b[0m Data X: 21669\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24189)\u001b[0m Data Y: 21669\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24189)\u001b[0m Creating client model to client: 15 round 0\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24187)\u001b[0m starting client: 10 <class 'str'>\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24187)\u001b[0m starting client:  <class 'int'>\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24187)\u001b[0m Creating client model to client: 10\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24187)\u001b[0m Data X: 26020\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24187)\u001b[0m Data Y: 26020\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24187)\u001b[0m Creating client model to client: 10 round 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(DefaultActor pid=24187)\u001b[0m /home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py:72: DeprecationWarning:  Ensure your client is of type `flwr.client.Client`. Please convert it using the `.to_client()` method before returning it in the `client_fn` you pass to `start_simulation`. We have applied this conversion on your behalf. Not returning a `Client` might trigger an error in future versions of Flower.\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24187)\u001b[0m   client = check_clientfn_returns_client(client_fn(cid))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(DefaultActor pid=24189)\u001b[0m 2\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24171)\u001b[0m starting client: 13 <class 'str'>\u001b[32m [repeated 14x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24171)\u001b[0m starting client:  <class 'int'>\u001b[32m [repeated 14x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24171)\u001b[0m Creating client model to client: 13\u001b[32m [repeated 14x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24171)\u001b[0m Data X: 23244\u001b[32m [repeated 14x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24171)\u001b[0m Data Y: 23244\u001b[32m [repeated 14x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24171)\u001b[0m Creating client model to client: 13 round 0\u001b[32m [repeated 14x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-02-15 18:04:13,877 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: d581abe9f194281cae040ffe01000000, name=DefaultActor.__init__, pid=24187, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950222), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ca86cd785378531d2b883fd9a151213ec5b7e9009ad88559c186049f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ca86cd785378531d2b883fd9a151213ec5b7e9009ad88559c186049f*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24189\t0.31\tray::DefaultActor.run\n",
      "24182\t0.31\tray::DefaultActor.run\n",
      "24186\t0.31\tray::DefaultActor.run\n",
      "24180\t0.31\tray::DefaultActor.run\n",
      "24178\t0.30\tray::DefaultActor.run\n",
      "24184\t0.30\tray::DefaultActor.run\n",
      "24179\t0.30\tray::DefaultActor.run\n",
      "24174\t0.30\tray::DefaultActor.run\n",
      "24185\t0.30\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-02-15 18:04:13,879 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: d581abe9f194281cae040ffe01000000, name=DefaultActor.__init__, pid=24187, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950222), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ca86cd785378531d2b883fd9a151213ec5b7e9009ad88559c186049f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ca86cd785378531d2b883fd9a151213ec5b7e9009ad88559c186049f*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24189\t0.31\tray::DefaultActor.run\n",
      "24182\t0.31\tray::DefaultActor.run\n",
      "24186\t0.31\tray::DefaultActor.run\n",
      "24180\t0.31\tray::DefaultActor.run\n",
      "24178\t0.30\tray::DefaultActor.run\n",
      "24184\t0.30\tray::DefaultActor.run\n",
      "24179\t0.30\tray::DefaultActor.run\n",
      "24174\t0.30\tray::DefaultActor.run\n",
      "24185\t0.30\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-02-15 18:04:13,884 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: d581abe9f194281cae040ffe01000000, name=DefaultActor.__init__, pid=24187, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950222), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ca86cd785378531d2b883fd9a151213ec5b7e9009ad88559c186049f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ca86cd785378531d2b883fd9a151213ec5b7e9009ad88559c186049f*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24189\t0.31\tray::DefaultActor.run\n",
      "24182\t0.31\tray::DefaultActor.run\n",
      "24186\t0.31\tray::DefaultActor.run\n",
      "24180\t0.31\tray::DefaultActor.run\n",
      "24178\t0.30\tray::DefaultActor.run\n",
      "24184\t0.30\tray::DefaultActor.run\n",
      "24179\t0.30\tray::DefaultActor.run\n",
      "24174\t0.30\tray::DefaultActor.run\n",
      "24185\t0.30\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-02-15 18:04:13,881 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: d581abe9f194281cae040ffe01000000, name=DefaultActor.__init__, pid=24187, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950222), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ca86cd785378531d2b883fd9a151213ec5b7e9009ad88559c186049f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ca86cd785378531d2b883fd9a151213ec5b7e9009ad88559c186049f*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24189\t0.31\tray::DefaultActor.run\n",
      "24182\t0.31\tray::DefaultActor.run\n",
      "24186\t0.31\tray::DefaultActor.run\n",
      "24180\t0.31\tray::DefaultActor.run\n",
      "24178\t0.30\tray::DefaultActor.run\n",
      "24184\t0.30\tray::DefaultActor.run\n",
      "24179\t0.30\tray::DefaultActor.run\n",
      "24174\t0.30\tray::DefaultActor.run\n",
      "24185\t0.30\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-02-15 18:04:13,882 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: d581abe9f194281cae040ffe01000000, name=DefaultActor.__init__, pid=24187, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950222), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ca86cd785378531d2b883fd9a151213ec5b7e9009ad88559c186049f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ca86cd785378531d2b883fd9a151213ec5b7e9009ad88559c186049f*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24189\t0.31\tray::DefaultActor.run\n",
      "24182\t0.31\tray::DefaultActor.run\n",
      "24186\t0.31\tray::DefaultActor.run\n",
      "24180\t0.31\tray::DefaultActor.run\n",
      "24178\t0.30\tray::DefaultActor.run\n",
      "24184\t0.30\tray::DefaultActor.run\n",
      "24179\t0.30\tray::DefaultActor.run\n",
      "24174\t0.30\tray::DefaultActor.run\n",
      "24185\t0.30\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-02-15 18:04:13,890 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: d581abe9f194281cae040ffe01000000, name=DefaultActor.__init__, pid=24187, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950222), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ca86cd785378531d2b883fd9a151213ec5b7e9009ad88559c186049f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ca86cd785378531d2b883fd9a151213ec5b7e9009ad88559c186049f*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24189\t0.31\tray::DefaultActor.run\n",
      "24182\t0.31\tray::DefaultActor.run\n",
      "24186\t0.31\tray::DefaultActor.run\n",
      "24180\t0.31\tray::DefaultActor.run\n",
      "24178\t0.30\tray::DefaultActor.run\n",
      "24184\t0.30\tray::DefaultActor.run\n",
      "24179\t0.30\tray::DefaultActor.run\n",
      "24174\t0.30\tray::DefaultActor.run\n",
      "24185\t0.30\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-02-15 18:04:13,881 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: d581abe9f194281cae040ffe01000000, name=DefaultActor.__init__, pid=24187, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950222), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ca86cd785378531d2b883fd9a151213ec5b7e9009ad88559c186049f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ca86cd785378531d2b883fd9a151213ec5b7e9009ad88559c186049f*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24189\t0.31\tray::DefaultActor.run\n",
      "24182\t0.31\tray::DefaultActor.run\n",
      "24186\t0.31\tray::DefaultActor.run\n",
      "24180\t0.31\tray::DefaultActor.run\n",
      "24178\t0.30\tray::DefaultActor.run\n",
      "24184\t0.30\tray::DefaultActor.run\n",
      "24179\t0.30\tray::DefaultActor.run\n",
      "24174\t0.30\tray::DefaultActor.run\n",
      "24185\t0.30\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-02-15 18:04:13,896 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: d581abe9f194281cae040ffe01000000, name=DefaultActor.__init__, pid=24187, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950222), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ca86cd785378531d2b883fd9a151213ec5b7e9009ad88559c186049f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ca86cd785378531d2b883fd9a151213ec5b7e9009ad88559c186049f*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24189\t0.31\tray::DefaultActor.run\n",
      "24182\t0.31\tray::DefaultActor.run\n",
      "24186\t0.31\tray::DefaultActor.run\n",
      "24180\t0.31\tray::DefaultActor.run\n",
      "24178\t0.30\tray::DefaultActor.run\n",
      "24184\t0.30\tray::DefaultActor.run\n",
      "24179\t0.30\tray::DefaultActor.run\n",
      "24174\t0.30\tray::DefaultActor.run\n",
      "24185\t0.30\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-02-15 18:04:21,984 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: 388fabd5122dfa924f937a4401000000, name=DefaultActor.__init__, pid=24184, memory used=0.33GB) was running was 7.07GB / 7.44GB (0.950416), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7183a21e8c6dc6d32f920bd6e9b8a0be3c757fc658e03caf33a1bb11) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7183a21e8c6dc6d32f920bd6e9b8a0be3c757fc658e03caf33a1bb11*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.12\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24184\t0.33\tray::DefaultActor.run\n",
      "24186\t0.32\tray::DefaultActor.run\n",
      "24175\t0.32\tray::DefaultActor.run\n",
      "24189\t0.32\tray::DefaultActor.run\n",
      "24181\t0.32\tray::DefaultActor.run\n",
      "24172\t0.32\tray::DefaultActor.run\n",
      "24179\t0.32\tray::DefaultActor.run\n",
      "24185\t0.32\tray::DefaultActor.run\n",
      "24182\t0.32\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-02-15 18:04:21,986 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: 388fabd5122dfa924f937a4401000000, name=DefaultActor.__init__, pid=24184, memory used=0.33GB) was running was 7.07GB / 7.44GB (0.950416), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7183a21e8c6dc6d32f920bd6e9b8a0be3c757fc658e03caf33a1bb11) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7183a21e8c6dc6d32f920bd6e9b8a0be3c757fc658e03caf33a1bb11*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.12\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24184\t0.33\tray::DefaultActor.run\n",
      "24186\t0.32\tray::DefaultActor.run\n",
      "24175\t0.32\tray::DefaultActor.run\n",
      "24189\t0.32\tray::DefaultActor.run\n",
      "24181\t0.32\tray::DefaultActor.run\n",
      "24172\t0.32\tray::DefaultActor.run\n",
      "24179\t0.32\tray::DefaultActor.run\n",
      "24185\t0.32\tray::DefaultActor.run\n",
      "24182\t0.32\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24174)\u001b[0m /home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24174)\u001b[0m   _warn_prf(average, modifier, msg_start, len(result))\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24171)\u001b[0m /home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py:72: DeprecationWarning:  Ensure your client is of type `flwr.client.Client`. Please convert it using the `.to_client()` method before returning it in the `client_fn` you pass to `start_simulation`. We have applied this conversion on your behalf. Not returning a `Client` might trigger an error in future versions of Flower.\u001b[32m [repeated 14x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24171)\u001b[0m   client = check_clientfn_returns_client(client_fn(cid))\u001b[32m [repeated 14x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(DefaultActor pid=24174)\u001b[0m 2\u001b[32m [repeated 16x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-02-15 18:04:28,147 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: 1aabde1e63fd801f3a80b1b001000000, name=DefaultActor.__init__, pid=24178, memory used=0.33GB) was running was 7.07GB / 7.44GB (0.950049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5520f083efbdea86985d6fe1a9f8ec4a8d92792d52c2c3639df180d8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-5520f083efbdea86985d6fe1a9f8ec4a8d92792d52c2c3639df180d8*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.13\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24189\t0.37\tray::DefaultActor.run\n",
      "24182\t0.37\tray::DefaultActor.run\n",
      "24180\t0.36\tray::DefaultActor.run\n",
      "24174\t0.36\tray::DefaultActor\n",
      "24181\t0.35\tray::DefaultActor.run\n",
      "24175\t0.35\tray::DefaultActor.run\n",
      "24176\t0.34\tray::DefaultActor.run\n",
      "24185\t0.34\tray::DefaultActor.run\n",
      "24186\t0.33\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-02-15 18:04:28,150 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: 1aabde1e63fd801f3a80b1b001000000, name=DefaultActor.__init__, pid=24178, memory used=0.33GB) was running was 7.07GB / 7.44GB (0.950049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5520f083efbdea86985d6fe1a9f8ec4a8d92792d52c2c3639df180d8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-5520f083efbdea86985d6fe1a9f8ec4a8d92792d52c2c3639df180d8*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.13\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24189\t0.37\tray::DefaultActor.run\n",
      "24182\t0.37\tray::DefaultActor.run\n",
      "24180\t0.36\tray::DefaultActor.run\n",
      "24174\t0.36\tray::DefaultActor\n",
      "24181\t0.35\tray::DefaultActor.run\n",
      "24175\t0.35\tray::DefaultActor.run\n",
      "24176\t0.34\tray::DefaultActor.run\n",
      "24185\t0.34\tray::DefaultActor.run\n",
      "24186\t0.33\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-02-15 18:04:29,289 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: b69812a30d49bad7a23056ae01000000, name=DefaultActor.__init__, pid=24177, memory used=0.36GB) was running was 7.07GB / 7.44GB (0.950478), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: eb87cbf60b1e6dc3a911ba590051b73508b005f44b4692d9a1345e71) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-eb87cbf60b1e6dc3a911ba590051b73508b005f44b4692d9a1345e71*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.13\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24189\t0.37\tray::DefaultActor\n",
      "24182\t0.37\tray::DefaultActor\n",
      "24186\t0.37\tray::DefaultActor.run\n",
      "24185\t0.37\tray::DefaultActor.run\n",
      "24175\t0.37\tray::DefaultActor.run\n",
      "24181\t0.37\tray::DefaultActor\n",
      "24179\t0.37\tray::DefaultActor.run\n",
      "24180\t0.37\tray::DefaultActor\n",
      "24172\t0.36\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-02-15 18:04:29,291 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: b69812a30d49bad7a23056ae01000000, name=DefaultActor.__init__, pid=24177, memory used=0.36GB) was running was 7.07GB / 7.44GB (0.950478), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: eb87cbf60b1e6dc3a911ba590051b73508b005f44b4692d9a1345e71) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-eb87cbf60b1e6dc3a911ba590051b73508b005f44b4692d9a1345e71*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.13\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24189\t0.37\tray::DefaultActor\n",
      "24182\t0.37\tray::DefaultActor\n",
      "24186\t0.37\tray::DefaultActor.run\n",
      "24185\t0.37\tray::DefaultActor.run\n",
      "24175\t0.37\tray::DefaultActor.run\n",
      "24181\t0.37\tray::DefaultActor\n",
      "24179\t0.37\tray::DefaultActor.run\n",
      "24180\t0.37\tray::DefaultActor\n",
      "24172\t0.36\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2024-02-15 18:04:29,913 | server.py:236 | fit_round 1 received 12 results and 7 failures\n",
      "WARNING flwr 2024-02-15 18:04:29,919 | fedavg.py:250 | No fit_metrics_aggregation_fn provided\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving round 1 aggregated_ndarrays...\n",
      "result_unbalanced_epoch_1_rounds_5_cycle_1/checkpoints/round-1-weights.npz\n",
      "\n",
      "Evaluating global model round 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO flwr 2024-02-15 18:04:36,627 | server.py:125 | fit progress: (1, 0.7319059371948242, {'accuracy': 0.7041471600532532}, 30.83866578800007)\n",
      "DEBUG flwr 2024-02-15 18:04:36,629 | server.py:173 | evaluate_round 1: strategy sampled 19 clients (out of 19)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(DefaultActor pid=24171)\u001b[0m /home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24171)\u001b[0m   _warn_prf(average, modifier, msg_start, len(result))\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24186)\u001b[0m /home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py:72: DeprecationWarning:  Ensure your client is of type `flwr.client.Client`. Please convert it using the `.to_client()` method before returning it in the `client_fn` you pass to `start_simulation`. We have applied this conversion on your behalf. Not returning a `Client` might trigger an error in future versions of Flower.\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24186)\u001b[0m   client = check_clientfn_returns_client(client_fn(cid))\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24171)\u001b[0m /home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py:72: DeprecationWarning:  Ensure your client is of type `flwr.client.Client`. Please convert it using the `.to_client()` method before returning it in the `client_fn` you pass to `start_simulation`. We have applied this conversion on your behalf. Not returning a `Client` might trigger an error in future versions of Flower.\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24171)\u001b[0m   client = check_clientfn_returns_client(client_fn(cid))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(DefaultActor pid=24171)\u001b[0m starting client: 12 <class 'str'>\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24171)\u001b[0m starting client:  <class 'int'>\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24171)\u001b[0m Creating client model to client: 12\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24171)\u001b[0m Data X: 31873\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24171)\u001b[0m Data Y: 31873\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24171)\u001b[0m Creating client model to client: 12 round 0\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24186)\u001b[0m 2\u001b[32m [repeated 11x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-02-15 18:04:37,233 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: b69812a30d49bad7a23056ae01000000, name=DefaultActor.__init__, pid=24177, memory used=0.36GB) was running was 7.07GB / 7.44GB (0.950478), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: eb87cbf60b1e6dc3a911ba590051b73508b005f44b4692d9a1345e71) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-eb87cbf60b1e6dc3a911ba590051b73508b005f44b4692d9a1345e71*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.13\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24189\t0.37\tray::DefaultActor\n",
      "24182\t0.37\tray::DefaultActor\n",
      "24186\t0.37\tray::DefaultActor.run\n",
      "24185\t0.37\tray::DefaultActor.run\n",
      "24175\t0.37\tray::DefaultActor.run\n",
      "24181\t0.37\tray::DefaultActor\n",
      "24179\t0.37\tray::DefaultActor.run\n",
      "24180\t0.37\tray::DefaultActor\n",
      "24172\t0.36\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-02-15 18:04:37,234 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: b69812a30d49bad7a23056ae01000000, name=DefaultActor.__init__, pid=24177, memory used=0.36GB) was running was 7.07GB / 7.44GB (0.950478), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: eb87cbf60b1e6dc3a911ba590051b73508b005f44b4692d9a1345e71) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-eb87cbf60b1e6dc3a911ba590051b73508b005f44b4692d9a1345e71*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.13\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24189\t0.37\tray::DefaultActor\n",
      "24182\t0.37\tray::DefaultActor\n",
      "24186\t0.37\tray::DefaultActor.run\n",
      "24185\t0.37\tray::DefaultActor.run\n",
      "24175\t0.37\tray::DefaultActor.run\n",
      "24181\t0.37\tray::DefaultActor\n",
      "24179\t0.37\tray::DefaultActor.run\n",
      "24180\t0.37\tray::DefaultActor\n",
      "24172\t0.36\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-02-15 18:04:37,237 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: 388fabd5122dfa924f937a4401000000, name=DefaultActor.__init__, pid=24184, memory used=0.33GB) was running was 7.07GB / 7.44GB (0.950416), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7183a21e8c6dc6d32f920bd6e9b8a0be3c757fc658e03caf33a1bb11) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7183a21e8c6dc6d32f920bd6e9b8a0be3c757fc658e03caf33a1bb11*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.12\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24184\t0.33\tray::DefaultActor.run\n",
      "24186\t0.32\tray::DefaultActor.run\n",
      "24175\t0.32\tray::DefaultActor.run\n",
      "24189\t0.32\tray::DefaultActor.run\n",
      "24181\t0.32\tray::DefaultActor.run\n",
      "24172\t0.32\tray::DefaultActor.run\n",
      "24179\t0.32\tray::DefaultActor.run\n",
      "24185\t0.32\tray::DefaultActor.run\n",
      "24182\t0.32\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-02-15 18:04:37,238 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: d581abe9f194281cae040ffe01000000, name=DefaultActor.__init__, pid=24187, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950222), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ca86cd785378531d2b883fd9a151213ec5b7e9009ad88559c186049f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ca86cd785378531d2b883fd9a151213ec5b7e9009ad88559c186049f*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24189\t0.31\tray::DefaultActor.run\n",
      "24182\t0.31\tray::DefaultActor.run\n",
      "24186\t0.31\tray::DefaultActor.run\n",
      "24180\t0.31\tray::DefaultActor.run\n",
      "24178\t0.30\tray::DefaultActor.run\n",
      "24184\t0.30\tray::DefaultActor.run\n",
      "24179\t0.30\tray::DefaultActor.run\n",
      "24174\t0.30\tray::DefaultActor.run\n",
      "24185\t0.30\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-02-15 18:04:37,239 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: 388fabd5122dfa924f937a4401000000, name=DefaultActor.__init__, pid=24184, memory used=0.33GB) was running was 7.07GB / 7.44GB (0.950416), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7183a21e8c6dc6d32f920bd6e9b8a0be3c757fc658e03caf33a1bb11) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7183a21e8c6dc6d32f920bd6e9b8a0be3c757fc658e03caf33a1bb11*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.12\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24184\t0.33\tray::DefaultActor.run\n",
      "24186\t0.32\tray::DefaultActor.run\n",
      "24175\t0.32\tray::DefaultActor.run\n",
      "24189\t0.32\tray::DefaultActor.run\n",
      "24181\t0.32\tray::DefaultActor.run\n",
      "24172\t0.32\tray::DefaultActor.run\n",
      "24179\t0.32\tray::DefaultActor.run\n",
      "24185\t0.32\tray::DefaultActor.run\n",
      "24182\t0.32\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-02-15 18:04:37,240 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: d581abe9f194281cae040ffe01000000, name=DefaultActor.__init__, pid=24187, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950222), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ca86cd785378531d2b883fd9a151213ec5b7e9009ad88559c186049f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ca86cd785378531d2b883fd9a151213ec5b7e9009ad88559c186049f*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24189\t0.31\tray::DefaultActor.run\n",
      "24182\t0.31\tray::DefaultActor.run\n",
      "24186\t0.31\tray::DefaultActor.run\n",
      "24180\t0.31\tray::DefaultActor.run\n",
      "24178\t0.30\tray::DefaultActor.run\n",
      "24184\t0.30\tray::DefaultActor.run\n",
      "24179\t0.30\tray::DefaultActor.run\n",
      "24174\t0.30\tray::DefaultActor.run\n",
      "24185\t0.30\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-02-15 18:04:37,246 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: 1aabde1e63fd801f3a80b1b001000000, name=DefaultActor.__init__, pid=24178, memory used=0.33GB) was running was 7.07GB / 7.44GB (0.950049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5520f083efbdea86985d6fe1a9f8ec4a8d92792d52c2c3639df180d8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-5520f083efbdea86985d6fe1a9f8ec4a8d92792d52c2c3639df180d8*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.13\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24189\t0.37\tray::DefaultActor.run\n",
      "24182\t0.37\tray::DefaultActor.run\n",
      "24180\t0.36\tray::DefaultActor.run\n",
      "24174\t0.36\tray::DefaultActor\n",
      "24181\t0.35\tray::DefaultActor.run\n",
      "24175\t0.35\tray::DefaultActor.run\n",
      "24176\t0.34\tray::DefaultActor.run\n",
      "24185\t0.34\tray::DefaultActor.run\n",
      "24186\t0.33\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-02-15 18:04:37,247 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: 1aabde1e63fd801f3a80b1b001000000, name=DefaultActor.__init__, pid=24178, memory used=0.33GB) was running was 7.07GB / 7.44GB (0.950049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5520f083efbdea86985d6fe1a9f8ec4a8d92792d52c2c3639df180d8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-5520f083efbdea86985d6fe1a9f8ec4a8d92792d52c2c3639df180d8*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.13\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24189\t0.37\tray::DefaultActor.run\n",
      "24182\t0.37\tray::DefaultActor.run\n",
      "24180\t0.36\tray::DefaultActor.run\n",
      "24174\t0.36\tray::DefaultActor\n",
      "24181\t0.35\tray::DefaultActor.run\n",
      "24175\t0.35\tray::DefaultActor.run\n",
      "24176\t0.34\tray::DefaultActor.run\n",
      "24185\t0.34\tray::DefaultActor.run\n",
      "24186\t0.33\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2024-02-15 18:04:37,267 | server.py:187 | evaluate_round 1 received 0 results and 19 failures\n",
      "DEBUG flwr 2024-02-15 18:04:37,268 | server.py:222 | fit_round 2: strategy sampled 19 clients (out of 19)\n",
      "ERROR flwr 2024-02-15 18:04:38,160 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: 1aabde1e63fd801f3a80b1b001000000, name=DefaultActor.__init__, pid=24178, memory used=0.33GB) was running was 7.07GB / 7.44GB (0.950049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5520f083efbdea86985d6fe1a9f8ec4a8d92792d52c2c3639df180d8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-5520f083efbdea86985d6fe1a9f8ec4a8d92792d52c2c3639df180d8*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.13\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24189\t0.37\tray::DefaultActor.run\n",
      "24182\t0.37\tray::DefaultActor.run\n",
      "24180\t0.36\tray::DefaultActor.run\n",
      "24174\t0.36\tray::DefaultActor\n",
      "24181\t0.35\tray::DefaultActor.run\n",
      "24175\t0.35\tray::DefaultActor.run\n",
      "24176\t0.34\tray::DefaultActor.run\n",
      "24185\t0.34\tray::DefaultActor.run\n",
      "24186\t0.33\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-02-15 18:04:38,162 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: 1aabde1e63fd801f3a80b1b001000000, name=DefaultActor.__init__, pid=24178, memory used=0.33GB) was running was 7.07GB / 7.44GB (0.950049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5520f083efbdea86985d6fe1a9f8ec4a8d92792d52c2c3639df180d8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-5520f083efbdea86985d6fe1a9f8ec4a8d92792d52c2c3639df180d8*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.13\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24189\t0.37\tray::DefaultActor.run\n",
      "24182\t0.37\tray::DefaultActor.run\n",
      "24180\t0.36\tray::DefaultActor.run\n",
      "24174\t0.36\tray::DefaultActor\n",
      "24181\t0.35\tray::DefaultActor.run\n",
      "24175\t0.35\tray::DefaultActor.run\n",
      "24176\t0.34\tray::DefaultActor.run\n",
      "24185\t0.34\tray::DefaultActor.run\n",
      "24186\t0.33\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-02-15 18:04:38,230 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: d581abe9f194281cae040ffe01000000, name=DefaultActor.__init__, pid=24187, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950222), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ca86cd785378531d2b883fd9a151213ec5b7e9009ad88559c186049f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ca86cd785378531d2b883fd9a151213ec5b7e9009ad88559c186049f*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24189\t0.31\tray::DefaultActor.run\n",
      "24182\t0.31\tray::DefaultActor.run\n",
      "24186\t0.31\tray::DefaultActor.run\n",
      "24180\t0.31\tray::DefaultActor.run\n",
      "24178\t0.30\tray::DefaultActor.run\n",
      "24184\t0.30\tray::DefaultActor.run\n",
      "24179\t0.30\tray::DefaultActor.run\n",
      "24174\t0.30\tray::DefaultActor.run\n",
      "24185\t0.30\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-02-15 18:04:38,237 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: d581abe9f194281cae040ffe01000000, name=DefaultActor.__init__, pid=24187, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950222), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ca86cd785378531d2b883fd9a151213ec5b7e9009ad88559c186049f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ca86cd785378531d2b883fd9a151213ec5b7e9009ad88559c186049f*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24189\t0.31\tray::DefaultActor.run\n",
      "24182\t0.31\tray::DefaultActor.run\n",
      "24186\t0.31\tray::DefaultActor.run\n",
      "24180\t0.31\tray::DefaultActor.run\n",
      "24178\t0.30\tray::DefaultActor.run\n",
      "24184\t0.30\tray::DefaultActor.run\n",
      "24179\t0.30\tray::DefaultActor.run\n",
      "24174\t0.30\tray::DefaultActor.run\n",
      "24185\t0.30\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-02-15 18:04:38,276 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: 6edfb7c0691ff0c70305db8d01000000, name=DefaultActor.__init__, pid=24189, memory used=0.37GB) was running was 7.22GB / 7.44GB (0.97048), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7d24c495b5bb39a4c54813e900f4c2f98e31d8b9f36138518c6bfa2b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7d24c495b5bb39a4c54813e900f4c2f98e31d8b9f36138518c6bfa2b*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.59\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24186\t0.38\tray::DefaultActor\n",
      "24189\t0.37\tray::DefaultActor\n",
      "24182\t0.37\tray::DefaultActor\n",
      "24179\t0.37\tray::DefaultActor\n",
      "24185\t0.37\tray::DefaultActor\n",
      "24175\t0.37\tray::DefaultActor\n",
      "24181\t0.37\tray::DefaultActor\n",
      "24172\t0.37\tray::DefaultActor\n",
      "24171\t0.37\tray::DefaultActor\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-02-15 18:04:38,278 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: 6edfb7c0691ff0c70305db8d01000000, name=DefaultActor.__init__, pid=24189, memory used=0.37GB) was running was 7.22GB / 7.44GB (0.97048), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7d24c495b5bb39a4c54813e900f4c2f98e31d8b9f36138518c6bfa2b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7d24c495b5bb39a4c54813e900f4c2f98e31d8b9f36138518c6bfa2b*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.59\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24186\t0.38\tray::DefaultActor\n",
      "24189\t0.37\tray::DefaultActor\n",
      "24182\t0.37\tray::DefaultActor\n",
      "24179\t0.37\tray::DefaultActor\n",
      "24185\t0.37\tray::DefaultActor\n",
      "24175\t0.37\tray::DefaultActor\n",
      "24181\t0.37\tray::DefaultActor\n",
      "24172\t0.37\tray::DefaultActor\n",
      "24171\t0.37\tray::DefaultActor\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-02-15 18:04:38,284 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: 388fabd5122dfa924f937a4401000000, name=DefaultActor.__init__, pid=24184, memory used=0.33GB) was running was 7.07GB / 7.44GB (0.950416), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7183a21e8c6dc6d32f920bd6e9b8a0be3c757fc658e03caf33a1bb11) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7183a21e8c6dc6d32f920bd6e9b8a0be3c757fc658e03caf33a1bb11*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.12\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24184\t0.33\tray::DefaultActor.run\n",
      "24186\t0.32\tray::DefaultActor.run\n",
      "24175\t0.32\tray::DefaultActor.run\n",
      "24189\t0.32\tray::DefaultActor.run\n",
      "24181\t0.32\tray::DefaultActor.run\n",
      "24172\t0.32\tray::DefaultActor.run\n",
      "24179\t0.32\tray::DefaultActor.run\n",
      "24185\t0.32\tray::DefaultActor.run\n",
      "24182\t0.32\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-02-15 18:04:38,286 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: b69812a30d49bad7a23056ae01000000, name=DefaultActor.__init__, pid=24177, memory used=0.36GB) was running was 7.07GB / 7.44GB (0.950478), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: eb87cbf60b1e6dc3a911ba590051b73508b005f44b4692d9a1345e71) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-eb87cbf60b1e6dc3a911ba590051b73508b005f44b4692d9a1345e71*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.13\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24189\t0.37\tray::DefaultActor\n",
      "24182\t0.37\tray::DefaultActor\n",
      "24186\t0.37\tray::DefaultActor.run\n",
      "24185\t0.37\tray::DefaultActor.run\n",
      "24175\t0.37\tray::DefaultActor.run\n",
      "24181\t0.37\tray::DefaultActor\n",
      "24179\t0.37\tray::DefaultActor.run\n",
      "24180\t0.37\tray::DefaultActor\n",
      "24172\t0.36\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-02-15 18:04:38,286 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: 388fabd5122dfa924f937a4401000000, name=DefaultActor.__init__, pid=24184, memory used=0.33GB) was running was 7.07GB / 7.44GB (0.950416), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7183a21e8c6dc6d32f920bd6e9b8a0be3c757fc658e03caf33a1bb11) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7183a21e8c6dc6d32f920bd6e9b8a0be3c757fc658e03caf33a1bb11*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.12\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24184\t0.33\tray::DefaultActor.run\n",
      "24186\t0.32\tray::DefaultActor.run\n",
      "24175\t0.32\tray::DefaultActor.run\n",
      "24189\t0.32\tray::DefaultActor.run\n",
      "24181\t0.32\tray::DefaultActor.run\n",
      "24172\t0.32\tray::DefaultActor.run\n",
      "24179\t0.32\tray::DefaultActor.run\n",
      "24185\t0.32\tray::DefaultActor.run\n",
      "24182\t0.32\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-02-15 18:04:38,287 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: d581abe9f194281cae040ffe01000000, name=DefaultActor.__init__, pid=24187, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950222), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ca86cd785378531d2b883fd9a151213ec5b7e9009ad88559c186049f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ca86cd785378531d2b883fd9a151213ec5b7e9009ad88559c186049f*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24189\t0.31\tray::DefaultActor.run\n",
      "24182\t0.31\tray::DefaultActor.run\n",
      "24186\t0.31\tray::DefaultActor.run\n",
      "24180\t0.31\tray::DefaultActor.run\n",
      "24178\t0.30\tray::DefaultActor.run\n",
      "24184\t0.30\tray::DefaultActor.run\n",
      "24179\t0.30\tray::DefaultActor.run\n",
      "24174\t0.30\tray::DefaultActor.run\n",
      "24185\t0.30\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-02-15 18:04:38,289 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: b69812a30d49bad7a23056ae01000000, name=DefaultActor.__init__, pid=24177, memory used=0.36GB) was running was 7.07GB / 7.44GB (0.950478), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: eb87cbf60b1e6dc3a911ba590051b73508b005f44b4692d9a1345e71) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-eb87cbf60b1e6dc3a911ba590051b73508b005f44b4692d9a1345e71*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.13\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24189\t0.37\tray::DefaultActor\n",
      "24182\t0.37\tray::DefaultActor\n",
      "24186\t0.37\tray::DefaultActor.run\n",
      "24185\t0.37\tray::DefaultActor.run\n",
      "24175\t0.37\tray::DefaultActor.run\n",
      "24181\t0.37\tray::DefaultActor\n",
      "24179\t0.37\tray::DefaultActor.run\n",
      "24180\t0.37\tray::DefaultActor\n",
      "24172\t0.36\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-02-15 18:04:38,292 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: c15383f49961ecb4a37d265501000000, name=DefaultActor.__init__, pid=24180, memory used=0.37GB) was running was 7.12GB / 7.44GB (0.95787), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: b5d7f31e8e1c5600a4998934a7df6c8f6b9f159ae5138dfa91eb3fcf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-b5d7f31e8e1c5600a4998934a7df6c8f6b9f159ae5138dfa91eb3fcf*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.83\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24186\t0.38\tray::DefaultActor.run\n",
      "24171\t0.38\tray::DefaultActor.run\n",
      "24172\t0.37\tray::DefaultActor.run\n",
      "24175\t0.37\tray::DefaultActor.run\n",
      "24182\t0.37\tray::DefaultActor\n",
      "24181\t0.37\tray::DefaultActor.run\n",
      "24179\t0.37\tray::DefaultActor\n",
      "24180\t0.37\tray::DefaultActor.run\n",
      "24185\t0.37\tray::DefaultActor\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-02-15 18:04:38,294 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: d581abe9f194281cae040ffe01000000, name=DefaultActor.__init__, pid=24187, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950222), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ca86cd785378531d2b883fd9a151213ec5b7e9009ad88559c186049f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ca86cd785378531d2b883fd9a151213ec5b7e9009ad88559c186049f*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24189\t0.31\tray::DefaultActor.run\n",
      "24182\t0.31\tray::DefaultActor.run\n",
      "24186\t0.31\tray::DefaultActor.run\n",
      "24180\t0.31\tray::DefaultActor.run\n",
      "24178\t0.30\tray::DefaultActor.run\n",
      "24184\t0.30\tray::DefaultActor.run\n",
      "24179\t0.30\tray::DefaultActor.run\n",
      "24174\t0.30\tray::DefaultActor.run\n",
      "24185\t0.30\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-02-15 18:04:38,298 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: c15383f49961ecb4a37d265501000000, name=DefaultActor.__init__, pid=24180, memory used=0.37GB) was running was 7.12GB / 7.44GB (0.95787), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: b5d7f31e8e1c5600a4998934a7df6c8f6b9f159ae5138dfa91eb3fcf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-b5d7f31e8e1c5600a4998934a7df6c8f6b9f159ae5138dfa91eb3fcf*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.83\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24186\t0.38\tray::DefaultActor.run\n",
      "24171\t0.38\tray::DefaultActor.run\n",
      "24172\t0.37\tray::DefaultActor.run\n",
      "24175\t0.37\tray::DefaultActor.run\n",
      "24182\t0.37\tray::DefaultActor\n",
      "24181\t0.37\tray::DefaultActor.run\n",
      "24179\t0.37\tray::DefaultActor\n",
      "24180\t0.37\tray::DefaultActor.run\n",
      "24185\t0.37\tray::DefaultActor\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-02-15 18:04:38,299 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: 388fabd5122dfa924f937a4401000000, name=DefaultActor.__init__, pid=24184, memory used=0.33GB) was running was 7.07GB / 7.44GB (0.950416), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7183a21e8c6dc6d32f920bd6e9b8a0be3c757fc658e03caf33a1bb11) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7183a21e8c6dc6d32f920bd6e9b8a0be3c757fc658e03caf33a1bb11*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.12\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24184\t0.33\tray::DefaultActor.run\n",
      "24186\t0.32\tray::DefaultActor.run\n",
      "24175\t0.32\tray::DefaultActor.run\n",
      "24189\t0.32\tray::DefaultActor.run\n",
      "24181\t0.32\tray::DefaultActor.run\n",
      "24172\t0.32\tray::DefaultActor.run\n",
      "24179\t0.32\tray::DefaultActor.run\n",
      "24185\t0.32\tray::DefaultActor.run\n",
      "24182\t0.32\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-02-15 18:04:38,299 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: 1aabde1e63fd801f3a80b1b001000000, name=DefaultActor.__init__, pid=24178, memory used=0.33GB) was running was 7.07GB / 7.44GB (0.950049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5520f083efbdea86985d6fe1a9f8ec4a8d92792d52c2c3639df180d8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-5520f083efbdea86985d6fe1a9f8ec4a8d92792d52c2c3639df180d8*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.13\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24189\t0.37\tray::DefaultActor.run\n",
      "24182\t0.37\tray::DefaultActor.run\n",
      "24180\t0.36\tray::DefaultActor.run\n",
      "24174\t0.36\tray::DefaultActor\n",
      "24181\t0.35\tray::DefaultActor.run\n",
      "24175\t0.35\tray::DefaultActor.run\n",
      "24176\t0.34\tray::DefaultActor.run\n",
      "24185\t0.34\tray::DefaultActor.run\n",
      "24186\t0.33\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-02-15 18:04:38,306 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: 388fabd5122dfa924f937a4401000000, name=DefaultActor.__init__, pid=24184, memory used=0.33GB) was running was 7.07GB / 7.44GB (0.950416), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7183a21e8c6dc6d32f920bd6e9b8a0be3c757fc658e03caf33a1bb11) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7183a21e8c6dc6d32f920bd6e9b8a0be3c757fc658e03caf33a1bb11*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.12\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24184\t0.33\tray::DefaultActor.run\n",
      "24186\t0.32\tray::DefaultActor.run\n",
      "24175\t0.32\tray::DefaultActor.run\n",
      "24189\t0.32\tray::DefaultActor.run\n",
      "24181\t0.32\tray::DefaultActor.run\n",
      "24172\t0.32\tray::DefaultActor.run\n",
      "24179\t0.32\tray::DefaultActor.run\n",
      "24185\t0.32\tray::DefaultActor.run\n",
      "24182\t0.32\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-02-15 18:04:38,306 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: 1aabde1e63fd801f3a80b1b001000000, name=DefaultActor.__init__, pid=24178, memory used=0.33GB) was running was 7.07GB / 7.44GB (0.950049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5520f083efbdea86985d6fe1a9f8ec4a8d92792d52c2c3639df180d8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-5520f083efbdea86985d6fe1a9f8ec4a8d92792d52c2c3639df180d8*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.13\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24189\t0.37\tray::DefaultActor.run\n",
      "24182\t0.37\tray::DefaultActor.run\n",
      "24180\t0.36\tray::DefaultActor.run\n",
      "24174\t0.36\tray::DefaultActor\n",
      "24181\t0.35\tray::DefaultActor.run\n",
      "24175\t0.35\tray::DefaultActor.run\n",
      "24176\t0.34\tray::DefaultActor.run\n",
      "24185\t0.34\tray::DefaultActor.run\n",
      "24186\t0.33\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24175)\u001b[0m /home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24175)\u001b[0m   _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(DefaultActor pid=24185)\u001b[0m starting client: 12 <class 'str'>\u001b[32m [repeated 25x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24185)\u001b[0m starting client:  <class 'int'>\u001b[32m [repeated 25x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24185)\u001b[0m Creating client model to client: 12\u001b[32m [repeated 25x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24185)\u001b[0m Data X: 31873\u001b[32m [repeated 25x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24185)\u001b[0m Data Y: 31873\u001b[32m [repeated 25x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24185)\u001b[0m Creating client model to client: 12 round 0\u001b[32m [repeated 25x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24185)\u001b[0m 2\u001b[32m [repeated 10x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2024-02-15 18:04:51,751 E 23772 23772] (raylet) node_manager.cc:3084: 6 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67, IP: 172.30.126.159) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.30.126.159`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(DefaultActor pid=24186)\u001b[0m 2\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24181)\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2024-02-15 18:04:56,508 | server.py:236 | fit_round 2 received 10 results and 9 failures\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving round 2 aggregated_ndarrays...\n",
      "result_unbalanced_epoch_1_rounds_5_cycle_1/checkpoints/round-2-weights.npz\n",
      "\n",
      "Evaluating global model round 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO flwr 2024-02-15 18:05:03,523 | server.py:125 | fit progress: (2, 0.8276991248130798, {'accuracy': 0.7227329611778259}, 57.73447790199998)\n",
      "DEBUG flwr 2024-02-15 18:05:03,525 | server.py:173 | evaluate_round 2: strategy sampled 19 clients (out of 19)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(DefaultActor pid=24182)\u001b[0m /home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py:72: DeprecationWarning:  Ensure your client is of type `flwr.client.Client`. Please convert it using the `.to_client()` method before returning it in the `client_fn` you pass to `start_simulation`. We have applied this conversion on your behalf. Not returning a `Client` might trigger an error in future versions of Flower.\u001b[32m [repeated 25x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24182)\u001b[0m   client = check_clientfn_returns_client(client_fn(cid))\u001b[32m [repeated 25x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24179)\u001b[0m /home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24179)\u001b[0m   _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(DefaultActor pid=24171)\u001b[0m starting client: 5 <class 'str'>\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24171)\u001b[0m starting client:  <class 'int'>\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24171)\u001b[0m Creating client model to client: 5\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24171)\u001b[0m Data X: 26567\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24171)\u001b[0m Data Y: 26567\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24171)\u001b[0m Creating client model to client: 5 round 0\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24185)\u001b[0m 2\u001b[32m [repeated 8x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-02-15 18:05:03,936 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: b69812a30d49bad7a23056ae01000000, name=DefaultActor.__init__, pid=24177, memory used=0.36GB) was running was 7.07GB / 7.44GB (0.950478), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: eb87cbf60b1e6dc3a911ba590051b73508b005f44b4692d9a1345e71) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-eb87cbf60b1e6dc3a911ba590051b73508b005f44b4692d9a1345e71*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.13\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24189\t0.37\tray::DefaultActor\n",
      "24182\t0.37\tray::DefaultActor\n",
      "24186\t0.37\tray::DefaultActor.run\n",
      "24185\t0.37\tray::DefaultActor.run\n",
      "24175\t0.37\tray::DefaultActor.run\n",
      "24181\t0.37\tray::DefaultActor\n",
      "24179\t0.37\tray::DefaultActor.run\n",
      "24180\t0.37\tray::DefaultActor\n",
      "24172\t0.36\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-02-15 18:05:03,938 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: b69812a30d49bad7a23056ae01000000, name=DefaultActor.__init__, pid=24177, memory used=0.36GB) was running was 7.07GB / 7.44GB (0.950478), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: eb87cbf60b1e6dc3a911ba590051b73508b005f44b4692d9a1345e71) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-eb87cbf60b1e6dc3a911ba590051b73508b005f44b4692d9a1345e71*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.13\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24189\t0.37\tray::DefaultActor\n",
      "24182\t0.37\tray::DefaultActor\n",
      "24186\t0.37\tray::DefaultActor.run\n",
      "24185\t0.37\tray::DefaultActor.run\n",
      "24175\t0.37\tray::DefaultActor.run\n",
      "24181\t0.37\tray::DefaultActor\n",
      "24179\t0.37\tray::DefaultActor.run\n",
      "24180\t0.37\tray::DefaultActor\n",
      "24172\t0.36\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-02-15 18:05:03,944 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: 1aabde1e63fd801f3a80b1b001000000, name=DefaultActor.__init__, pid=24178, memory used=0.33GB) was running was 7.07GB / 7.44GB (0.950049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5520f083efbdea86985d6fe1a9f8ec4a8d92792d52c2c3639df180d8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-5520f083efbdea86985d6fe1a9f8ec4a8d92792d52c2c3639df180d8*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.13\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24189\t0.37\tray::DefaultActor.run\n",
      "24182\t0.37\tray::DefaultActor.run\n",
      "24180\t0.36\tray::DefaultActor.run\n",
      "24174\t0.36\tray::DefaultActor\n",
      "24181\t0.35\tray::DefaultActor.run\n",
      "24175\t0.35\tray::DefaultActor.run\n",
      "24176\t0.34\tray::DefaultActor.run\n",
      "24185\t0.34\tray::DefaultActor.run\n",
      "24186\t0.33\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-02-15 18:05:03,946 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: 1aabde1e63fd801f3a80b1b001000000, name=DefaultActor.__init__, pid=24178, memory used=0.33GB) was running was 7.07GB / 7.44GB (0.950049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5520f083efbdea86985d6fe1a9f8ec4a8d92792d52c2c3639df180d8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-5520f083efbdea86985d6fe1a9f8ec4a8d92792d52c2c3639df180d8*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.13\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24189\t0.37\tray::DefaultActor.run\n",
      "24182\t0.37\tray::DefaultActor.run\n",
      "24180\t0.36\tray::DefaultActor.run\n",
      "24174\t0.36\tray::DefaultActor\n",
      "24181\t0.35\tray::DefaultActor.run\n",
      "24175\t0.35\tray::DefaultActor.run\n",
      "24176\t0.34\tray::DefaultActor.run\n",
      "24185\t0.34\tray::DefaultActor.run\n",
      "24186\t0.33\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-02-15 18:05:03,948 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: 6edfb7c0691ff0c70305db8d01000000, name=DefaultActor.__init__, pid=24189, memory used=0.37GB) was running was 7.22GB / 7.44GB (0.97048), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7d24c495b5bb39a4c54813e900f4c2f98e31d8b9f36138518c6bfa2b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7d24c495b5bb39a4c54813e900f4c2f98e31d8b9f36138518c6bfa2b*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.59\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24186\t0.38\tray::DefaultActor\n",
      "24189\t0.37\tray::DefaultActor\n",
      "24182\t0.37\tray::DefaultActor\n",
      "24179\t0.37\tray::DefaultActor\n",
      "24185\t0.37\tray::DefaultActor\n",
      "24175\t0.37\tray::DefaultActor\n",
      "24181\t0.37\tray::DefaultActor\n",
      "24172\t0.37\tray::DefaultActor\n",
      "24171\t0.37\tray::DefaultActor\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-02-15 18:05:03,949 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: 6edfb7c0691ff0c70305db8d01000000, name=DefaultActor.__init__, pid=24189, memory used=0.37GB) was running was 7.22GB / 7.44GB (0.97048), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7d24c495b5bb39a4c54813e900f4c2f98e31d8b9f36138518c6bfa2b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7d24c495b5bb39a4c54813e900f4c2f98e31d8b9f36138518c6bfa2b*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.59\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24186\t0.38\tray::DefaultActor\n",
      "24189\t0.37\tray::DefaultActor\n",
      "24182\t0.37\tray::DefaultActor\n",
      "24179\t0.37\tray::DefaultActor\n",
      "24185\t0.37\tray::DefaultActor\n",
      "24175\t0.37\tray::DefaultActor\n",
      "24181\t0.37\tray::DefaultActor\n",
      "24172\t0.37\tray::DefaultActor\n",
      "24171\t0.37\tray::DefaultActor\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-02-15 18:05:03,952 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: 388fabd5122dfa924f937a4401000000, name=DefaultActor.__init__, pid=24184, memory used=0.33GB) was running was 7.07GB / 7.44GB (0.950416), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7183a21e8c6dc6d32f920bd6e9b8a0be3c757fc658e03caf33a1bb11) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7183a21e8c6dc6d32f920bd6e9b8a0be3c757fc658e03caf33a1bb11*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.12\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24184\t0.33\tray::DefaultActor.run\n",
      "24186\t0.32\tray::DefaultActor.run\n",
      "24175\t0.32\tray::DefaultActor.run\n",
      "24189\t0.32\tray::DefaultActor.run\n",
      "24181\t0.32\tray::DefaultActor.run\n",
      "24172\t0.32\tray::DefaultActor.run\n",
      "24179\t0.32\tray::DefaultActor.run\n",
      "24185\t0.32\tray::DefaultActor.run\n",
      "24182\t0.32\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-02-15 18:05:03,954 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: 388fabd5122dfa924f937a4401000000, name=DefaultActor.__init__, pid=24184, memory used=0.33GB) was running was 7.07GB / 7.44GB (0.950416), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7183a21e8c6dc6d32f920bd6e9b8a0be3c757fc658e03caf33a1bb11) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7183a21e8c6dc6d32f920bd6e9b8a0be3c757fc658e03caf33a1bb11*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.12\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24184\t0.33\tray::DefaultActor.run\n",
      "24186\t0.32\tray::DefaultActor.run\n",
      "24175\t0.32\tray::DefaultActor.run\n",
      "24189\t0.32\tray::DefaultActor.run\n",
      "24181\t0.32\tray::DefaultActor.run\n",
      "24172\t0.32\tray::DefaultActor.run\n",
      "24179\t0.32\tray::DefaultActor.run\n",
      "24185\t0.32\tray::DefaultActor.run\n",
      "24182\t0.32\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-02-15 18:05:03,954 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: c15383f49961ecb4a37d265501000000, name=DefaultActor.__init__, pid=24180, memory used=0.37GB) was running was 7.12GB / 7.44GB (0.95787), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: b5d7f31e8e1c5600a4998934a7df6c8f6b9f159ae5138dfa91eb3fcf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-b5d7f31e8e1c5600a4998934a7df6c8f6b9f159ae5138dfa91eb3fcf*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.83\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24186\t0.38\tray::DefaultActor.run\n",
      "24171\t0.38\tray::DefaultActor.run\n",
      "24172\t0.37\tray::DefaultActor.run\n",
      "24175\t0.37\tray::DefaultActor.run\n",
      "24182\t0.37\tray::DefaultActor\n",
      "24181\t0.37\tray::DefaultActor.run\n",
      "24179\t0.37\tray::DefaultActor\n",
      "24180\t0.37\tray::DefaultActor.run\n",
      "24185\t0.37\tray::DefaultActor\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-02-15 18:05:03,958 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: c15383f49961ecb4a37d265501000000, name=DefaultActor.__init__, pid=24180, memory used=0.37GB) was running was 7.12GB / 7.44GB (0.95787), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: b5d7f31e8e1c5600a4998934a7df6c8f6b9f159ae5138dfa91eb3fcf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-b5d7f31e8e1c5600a4998934a7df6c8f6b9f159ae5138dfa91eb3fcf*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.83\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24186\t0.38\tray::DefaultActor.run\n",
      "24171\t0.38\tray::DefaultActor.run\n",
      "24172\t0.37\tray::DefaultActor.run\n",
      "24175\t0.37\tray::DefaultActor.run\n",
      "24182\t0.37\tray::DefaultActor\n",
      "24181\t0.37\tray::DefaultActor.run\n",
      "24179\t0.37\tray::DefaultActor\n",
      "24180\t0.37\tray::DefaultActor.run\n",
      "24185\t0.37\tray::DefaultActor\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-02-15 18:05:03,959 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: d581abe9f194281cae040ffe01000000, name=DefaultActor.__init__, pid=24187, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950222), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ca86cd785378531d2b883fd9a151213ec5b7e9009ad88559c186049f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ca86cd785378531d2b883fd9a151213ec5b7e9009ad88559c186049f*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24189\t0.31\tray::DefaultActor.run\n",
      "24182\t0.31\tray::DefaultActor.run\n",
      "24186\t0.31\tray::DefaultActor.run\n",
      "24180\t0.31\tray::DefaultActor.run\n",
      "24178\t0.30\tray::DefaultActor.run\n",
      "24184\t0.30\tray::DefaultActor.run\n",
      "24179\t0.30\tray::DefaultActor.run\n",
      "24174\t0.30\tray::DefaultActor.run\n",
      "24185\t0.30\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-02-15 18:05:03,960 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: d581abe9f194281cae040ffe01000000, name=DefaultActor.__init__, pid=24187, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950222), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ca86cd785378531d2b883fd9a151213ec5b7e9009ad88559c186049f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ca86cd785378531d2b883fd9a151213ec5b7e9009ad88559c186049f*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24189\t0.31\tray::DefaultActor.run\n",
      "24182\t0.31\tray::DefaultActor.run\n",
      "24186\t0.31\tray::DefaultActor.run\n",
      "24180\t0.31\tray::DefaultActor.run\n",
      "24178\t0.30\tray::DefaultActor.run\n",
      "24184\t0.30\tray::DefaultActor.run\n",
      "24179\t0.30\tray::DefaultActor.run\n",
      "24174\t0.30\tray::DefaultActor.run\n",
      "24185\t0.30\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2024-02-15 18:05:03,982 | server.py:187 | evaluate_round 2 received 0 results and 19 failures\n",
      "DEBUG flwr 2024-02-15 18:05:03,983 | server.py:222 | fit_round 3: strategy sampled 19 clients (out of 19)\n",
      "ERROR flwr 2024-02-15 18:05:04,517 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: 388fabd5122dfa924f937a4401000000, name=DefaultActor.__init__, pid=24184, memory used=0.33GB) was running was 7.07GB / 7.44GB (0.950416), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7183a21e8c6dc6d32f920bd6e9b8a0be3c757fc658e03caf33a1bb11) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7183a21e8c6dc6d32f920bd6e9b8a0be3c757fc658e03caf33a1bb11*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.12\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24184\t0.33\tray::DefaultActor.run\n",
      "24186\t0.32\tray::DefaultActor.run\n",
      "24175\t0.32\tray::DefaultActor.run\n",
      "24189\t0.32\tray::DefaultActor.run\n",
      "24181\t0.32\tray::DefaultActor.run\n",
      "24172\t0.32\tray::DefaultActor.run\n",
      "24179\t0.32\tray::DefaultActor.run\n",
      "24185\t0.32\tray::DefaultActor.run\n",
      "24182\t0.32\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-02-15 18:05:04,519 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: 1aabde1e63fd801f3a80b1b001000000, name=DefaultActor.__init__, pid=24178, memory used=0.33GB) was running was 7.07GB / 7.44GB (0.950049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5520f083efbdea86985d6fe1a9f8ec4a8d92792d52c2c3639df180d8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-5520f083efbdea86985d6fe1a9f8ec4a8d92792d52c2c3639df180d8*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.13\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24189\t0.37\tray::DefaultActor.run\n",
      "24182\t0.37\tray::DefaultActor.run\n",
      "24180\t0.36\tray::DefaultActor.run\n",
      "24174\t0.36\tray::DefaultActor\n",
      "24181\t0.35\tray::DefaultActor.run\n",
      "24175\t0.35\tray::DefaultActor.run\n",
      "24176\t0.34\tray::DefaultActor.run\n",
      "24185\t0.34\tray::DefaultActor.run\n",
      "24186\t0.33\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-02-15 18:05:04,519 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: 388fabd5122dfa924f937a4401000000, name=DefaultActor.__init__, pid=24184, memory used=0.33GB) was running was 7.07GB / 7.44GB (0.950416), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7183a21e8c6dc6d32f920bd6e9b8a0be3c757fc658e03caf33a1bb11) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7183a21e8c6dc6d32f920bd6e9b8a0be3c757fc658e03caf33a1bb11*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.12\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24184\t0.33\tray::DefaultActor.run\n",
      "24186\t0.32\tray::DefaultActor.run\n",
      "24175\t0.32\tray::DefaultActor.run\n",
      "24189\t0.32\tray::DefaultActor.run\n",
      "24181\t0.32\tray::DefaultActor.run\n",
      "24172\t0.32\tray::DefaultActor.run\n",
      "24179\t0.32\tray::DefaultActor.run\n",
      "24185\t0.32\tray::DefaultActor.run\n",
      "24182\t0.32\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-02-15 18:05:04,522 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: 1aabde1e63fd801f3a80b1b001000000, name=DefaultActor.__init__, pid=24178, memory used=0.33GB) was running was 7.07GB / 7.44GB (0.950049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5520f083efbdea86985d6fe1a9f8ec4a8d92792d52c2c3639df180d8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-5520f083efbdea86985d6fe1a9f8ec4a8d92792d52c2c3639df180d8*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.13\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24189\t0.37\tray::DefaultActor.run\n",
      "24182\t0.37\tray::DefaultActor.run\n",
      "24180\t0.36\tray::DefaultActor.run\n",
      "24174\t0.36\tray::DefaultActor\n",
      "24181\t0.35\tray::DefaultActor.run\n",
      "24175\t0.35\tray::DefaultActor.run\n",
      "24176\t0.34\tray::DefaultActor.run\n",
      "24185\t0.34\tray::DefaultActor.run\n",
      "24186\t0.33\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-02-15 18:05:04,524 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: d581abe9f194281cae040ffe01000000, name=DefaultActor.__init__, pid=24187, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950222), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ca86cd785378531d2b883fd9a151213ec5b7e9009ad88559c186049f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ca86cd785378531d2b883fd9a151213ec5b7e9009ad88559c186049f*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24189\t0.31\tray::DefaultActor.run\n",
      "24182\t0.31\tray::DefaultActor.run\n",
      "24186\t0.31\tray::DefaultActor.run\n",
      "24180\t0.31\tray::DefaultActor.run\n",
      "24178\t0.30\tray::DefaultActor.run\n",
      "24184\t0.30\tray::DefaultActor.run\n",
      "24179\t0.30\tray::DefaultActor.run\n",
      "24174\t0.30\tray::DefaultActor.run\n",
      "24185\t0.30\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-02-15 18:05:04,527 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: c15383f49961ecb4a37d265501000000, name=DefaultActor.__init__, pid=24180, memory used=0.37GB) was running was 7.12GB / 7.44GB (0.95787), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: b5d7f31e8e1c5600a4998934a7df6c8f6b9f159ae5138dfa91eb3fcf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-b5d7f31e8e1c5600a4998934a7df6c8f6b9f159ae5138dfa91eb3fcf*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.83\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24186\t0.38\tray::DefaultActor.run\n",
      "24171\t0.38\tray::DefaultActor.run\n",
      "24172\t0.37\tray::DefaultActor.run\n",
      "24175\t0.37\tray::DefaultActor.run\n",
      "24182\t0.37\tray::DefaultActor\n",
      "24181\t0.37\tray::DefaultActor.run\n",
      "24179\t0.37\tray::DefaultActor\n",
      "24180\t0.37\tray::DefaultActor.run\n",
      "24185\t0.37\tray::DefaultActor\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-02-15 18:05:04,527 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: 6edfb7c0691ff0c70305db8d01000000, name=DefaultActor.__init__, pid=24189, memory used=0.37GB) was running was 7.22GB / 7.44GB (0.97048), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7d24c495b5bb39a4c54813e900f4c2f98e31d8b9f36138518c6bfa2b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7d24c495b5bb39a4c54813e900f4c2f98e31d8b9f36138518c6bfa2b*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.59\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24186\t0.38\tray::DefaultActor\n",
      "24189\t0.37\tray::DefaultActor\n",
      "24182\t0.37\tray::DefaultActor\n",
      "24179\t0.37\tray::DefaultActor\n",
      "24185\t0.37\tray::DefaultActor\n",
      "24175\t0.37\tray::DefaultActor\n",
      "24181\t0.37\tray::DefaultActor\n",
      "24172\t0.37\tray::DefaultActor\n",
      "24171\t0.37\tray::DefaultActor\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-02-15 18:05:04,528 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: d581abe9f194281cae040ffe01000000, name=DefaultActor.__init__, pid=24187, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950222), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ca86cd785378531d2b883fd9a151213ec5b7e9009ad88559c186049f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ca86cd785378531d2b883fd9a151213ec5b7e9009ad88559c186049f*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24189\t0.31\tray::DefaultActor.run\n",
      "24182\t0.31\tray::DefaultActor.run\n",
      "24186\t0.31\tray::DefaultActor.run\n",
      "24180\t0.31\tray::DefaultActor.run\n",
      "24178\t0.30\tray::DefaultActor.run\n",
      "24184\t0.30\tray::DefaultActor.run\n",
      "24179\t0.30\tray::DefaultActor.run\n",
      "24174\t0.30\tray::DefaultActor.run\n",
      "24185\t0.30\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-02-15 18:05:04,529 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: c15383f49961ecb4a37d265501000000, name=DefaultActor.__init__, pid=24180, memory used=0.37GB) was running was 7.12GB / 7.44GB (0.95787), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: b5d7f31e8e1c5600a4998934a7df6c8f6b9f159ae5138dfa91eb3fcf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-b5d7f31e8e1c5600a4998934a7df6c8f6b9f159ae5138dfa91eb3fcf*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.83\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24186\t0.38\tray::DefaultActor.run\n",
      "24171\t0.38\tray::DefaultActor.run\n",
      "24172\t0.37\tray::DefaultActor.run\n",
      "24175\t0.37\tray::DefaultActor.run\n",
      "24182\t0.37\tray::DefaultActor\n",
      "24181\t0.37\tray::DefaultActor.run\n",
      "24179\t0.37\tray::DefaultActor\n",
      "24180\t0.37\tray::DefaultActor.run\n",
      "24185\t0.37\tray::DefaultActor\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-02-15 18:05:04,533 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: d581abe9f194281cae040ffe01000000, name=DefaultActor.__init__, pid=24187, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950222), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ca86cd785378531d2b883fd9a151213ec5b7e9009ad88559c186049f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ca86cd785378531d2b883fd9a151213ec5b7e9009ad88559c186049f*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24189\t0.31\tray::DefaultActor.run\n",
      "24182\t0.31\tray::DefaultActor.run\n",
      "24186\t0.31\tray::DefaultActor.run\n",
      "24180\t0.31\tray::DefaultActor.run\n",
      "24178\t0.30\tray::DefaultActor.run\n",
      "24184\t0.30\tray::DefaultActor.run\n",
      "24179\t0.30\tray::DefaultActor.run\n",
      "24174\t0.30\tray::DefaultActor.run\n",
      "24185\t0.30\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-02-15 18:05:04,534 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: 6edfb7c0691ff0c70305db8d01000000, name=DefaultActor.__init__, pid=24189, memory used=0.37GB) was running was 7.22GB / 7.44GB (0.97048), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7d24c495b5bb39a4c54813e900f4c2f98e31d8b9f36138518c6bfa2b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7d24c495b5bb39a4c54813e900f4c2f98e31d8b9f36138518c6bfa2b*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.59\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24186\t0.38\tray::DefaultActor\n",
      "24189\t0.37\tray::DefaultActor\n",
      "24182\t0.37\tray::DefaultActor\n",
      "24179\t0.37\tray::DefaultActor\n",
      "24185\t0.37\tray::DefaultActor\n",
      "24175\t0.37\tray::DefaultActor\n",
      "24181\t0.37\tray::DefaultActor\n",
      "24172\t0.37\tray::DefaultActor\n",
      "24171\t0.37\tray::DefaultActor\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-02-15 18:05:04,535 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: b69812a30d49bad7a23056ae01000000, name=DefaultActor.__init__, pid=24177, memory used=0.36GB) was running was 7.07GB / 7.44GB (0.950478), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: eb87cbf60b1e6dc3a911ba590051b73508b005f44b4692d9a1345e71) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-eb87cbf60b1e6dc3a911ba590051b73508b005f44b4692d9a1345e71*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.13\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24189\t0.37\tray::DefaultActor\n",
      "24182\t0.37\tray::DefaultActor\n",
      "24186\t0.37\tray::DefaultActor.run\n",
      "24185\t0.37\tray::DefaultActor.run\n",
      "24175\t0.37\tray::DefaultActor.run\n",
      "24181\t0.37\tray::DefaultActor\n",
      "24179\t0.37\tray::DefaultActor.run\n",
      "24180\t0.37\tray::DefaultActor\n",
      "24172\t0.36\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-02-15 18:05:04,539 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: c15383f49961ecb4a37d265501000000, name=DefaultActor.__init__, pid=24180, memory used=0.37GB) was running was 7.12GB / 7.44GB (0.95787), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: b5d7f31e8e1c5600a4998934a7df6c8f6b9f159ae5138dfa91eb3fcf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-b5d7f31e8e1c5600a4998934a7df6c8f6b9f159ae5138dfa91eb3fcf*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.83\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24186\t0.38\tray::DefaultActor.run\n",
      "24171\t0.38\tray::DefaultActor.run\n",
      "24172\t0.37\tray::DefaultActor.run\n",
      "24175\t0.37\tray::DefaultActor.run\n",
      "24182\t0.37\tray::DefaultActor\n",
      "24181\t0.37\tray::DefaultActor.run\n",
      "24179\t0.37\tray::DefaultActor\n",
      "24180\t0.37\tray::DefaultActor.run\n",
      "24185\t0.37\tray::DefaultActor\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-02-15 18:05:04,541 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: 1aabde1e63fd801f3a80b1b001000000, name=DefaultActor.__init__, pid=24178, memory used=0.33GB) was running was 7.07GB / 7.44GB (0.950049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5520f083efbdea86985d6fe1a9f8ec4a8d92792d52c2c3639df180d8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-5520f083efbdea86985d6fe1a9f8ec4a8d92792d52c2c3639df180d8*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.13\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24189\t0.37\tray::DefaultActor.run\n",
      "24182\t0.37\tray::DefaultActor.run\n",
      "24180\t0.36\tray::DefaultActor.run\n",
      "24174\t0.36\tray::DefaultActor\n",
      "24181\t0.35\tray::DefaultActor.run\n",
      "24175\t0.35\tray::DefaultActor.run\n",
      "24176\t0.34\tray::DefaultActor.run\n",
      "24185\t0.34\tray::DefaultActor.run\n",
      "24186\t0.33\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-02-15 18:05:04,542 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: d581abe9f194281cae040ffe01000000, name=DefaultActor.__init__, pid=24187, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950222), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ca86cd785378531d2b883fd9a151213ec5b7e9009ad88559c186049f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ca86cd785378531d2b883fd9a151213ec5b7e9009ad88559c186049f*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24189\t0.31\tray::DefaultActor.run\n",
      "24182\t0.31\tray::DefaultActor.run\n",
      "24186\t0.31\tray::DefaultActor.run\n",
      "24180\t0.31\tray::DefaultActor.run\n",
      "24178\t0.30\tray::DefaultActor.run\n",
      "24184\t0.30\tray::DefaultActor.run\n",
      "24179\t0.30\tray::DefaultActor.run\n",
      "24174\t0.30\tray::DefaultActor.run\n",
      "24185\t0.30\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-02-15 18:05:04,546 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: b69812a30d49bad7a23056ae01000000, name=DefaultActor.__init__, pid=24177, memory used=0.36GB) was running was 7.07GB / 7.44GB (0.950478), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: eb87cbf60b1e6dc3a911ba590051b73508b005f44b4692d9a1345e71) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-eb87cbf60b1e6dc3a911ba590051b73508b005f44b4692d9a1345e71*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.13\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24189\t0.37\tray::DefaultActor\n",
      "24182\t0.37\tray::DefaultActor\n",
      "24186\t0.37\tray::DefaultActor.run\n",
      "24185\t0.37\tray::DefaultActor.run\n",
      "24175\t0.37\tray::DefaultActor.run\n",
      "24181\t0.37\tray::DefaultActor\n",
      "24179\t0.37\tray::DefaultActor.run\n",
      "24180\t0.37\tray::DefaultActor\n",
      "24172\t0.36\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-02-15 18:05:04,548 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: c15383f49961ecb4a37d265501000000, name=DefaultActor.__init__, pid=24180, memory used=0.37GB) was running was 7.12GB / 7.44GB (0.95787), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: b5d7f31e8e1c5600a4998934a7df6c8f6b9f159ae5138dfa91eb3fcf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-b5d7f31e8e1c5600a4998934a7df6c8f6b9f159ae5138dfa91eb3fcf*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.83\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24186\t0.38\tray::DefaultActor.run\n",
      "24171\t0.38\tray::DefaultActor.run\n",
      "24172\t0.37\tray::DefaultActor.run\n",
      "24175\t0.37\tray::DefaultActor.run\n",
      "24182\t0.37\tray::DefaultActor\n",
      "24181\t0.37\tray::DefaultActor.run\n",
      "24179\t0.37\tray::DefaultActor\n",
      "24180\t0.37\tray::DefaultActor.run\n",
      "24185\t0.37\tray::DefaultActor\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-02-15 18:05:04,549 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: 1aabde1e63fd801f3a80b1b001000000, name=DefaultActor.__init__, pid=24178, memory used=0.33GB) was running was 7.07GB / 7.44GB (0.950049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5520f083efbdea86985d6fe1a9f8ec4a8d92792d52c2c3639df180d8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-5520f083efbdea86985d6fe1a9f8ec4a8d92792d52c2c3639df180d8*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.13\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24189\t0.37\tray::DefaultActor.run\n",
      "24182\t0.37\tray::DefaultActor.run\n",
      "24180\t0.36\tray::DefaultActor.run\n",
      "24174\t0.36\tray::DefaultActor\n",
      "24181\t0.35\tray::DefaultActor.run\n",
      "24175\t0.35\tray::DefaultActor.run\n",
      "24176\t0.34\tray::DefaultActor.run\n",
      "24185\t0.34\tray::DefaultActor.run\n",
      "24186\t0.33\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-02-15 18:05:06,866 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: 8080131699fdec87227ce5a101000000, name=DefaultActor.__init__, pid=24181, memory used=0.39GB) was running was 7.07GB / 7.44GB (0.950481), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8f9f4202b7c08e54248e55344f63a4dff0d1bcebccfc900a981c5fe4) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-8f9f4202b7c08e54248e55344f63a4dff0d1bcebccfc900a981c5fe4*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.84\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24171\t0.40\tray::DefaultActor.run\n",
      "24182\t0.40\tray::DefaultActor.run\n",
      "24185\t0.39\tray::DefaultActor.run\n",
      "24176\t0.39\tray::DefaultActor.run\n",
      "24175\t0.39\tray::DefaultActor.run\n",
      "24186\t0.39\tray::DefaultActor.run\n",
      "24181\t0.39\tray::DefaultActor.run\n",
      "24172\t0.39\tray::DefaultActor.run\n",
      "24179\t0.39\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-02-15 18:05:06,868 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: 8080131699fdec87227ce5a101000000, name=DefaultActor.__init__, pid=24181, memory used=0.39GB) was running was 7.07GB / 7.44GB (0.950481), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8f9f4202b7c08e54248e55344f63a4dff0d1bcebccfc900a981c5fe4) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-8f9f4202b7c08e54248e55344f63a4dff0d1bcebccfc900a981c5fe4*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.84\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24171\t0.40\tray::DefaultActor.run\n",
      "24182\t0.40\tray::DefaultActor.run\n",
      "24185\t0.39\tray::DefaultActor.run\n",
      "24176\t0.39\tray::DefaultActor.run\n",
      "24175\t0.39\tray::DefaultActor.run\n",
      "24186\t0.39\tray::DefaultActor.run\n",
      "24181\t0.39\tray::DefaultActor.run\n",
      "24172\t0.39\tray::DefaultActor.run\n",
      "24179\t0.39\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(DefaultActor pid=24176)\u001b[0m starting client: 2 <class 'str'>\u001b[32m [repeated 22x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24176)\u001b[0m starting client:  <class 'int'>\u001b[32m [repeated 22x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24176)\u001b[0m Creating client model to client: 2\u001b[32m [repeated 22x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24176)\u001b[0m Data X: 3383\u001b[32m [repeated 22x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24176)\u001b[0m Data Y: 3383\u001b[32m [repeated 22x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24176)\u001b[0m Creating client model to client: 2 round 0\u001b[32m [repeated 22x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24176)\u001b[0m 2\u001b[32m [repeated 10x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2024-02-15 18:05:24,128 | server.py:236 | fit_round 3 received 9 results and 10 failures\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving round 3 aggregated_ndarrays...\n",
      "result_unbalanced_epoch_1_rounds_5_cycle_1/checkpoints/round-3-weights.npz\n",
      "\n",
      "Evaluating global model round 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO flwr 2024-02-15 18:05:30,994 | server.py:125 | fit progress: (3, 0.8657560348510742, {'accuracy': 0.7227032780647278}, 85.20560923000016)\n",
      "DEBUG flwr 2024-02-15 18:05:30,996 | server.py:173 | evaluate_round 3: strategy sampled 19 clients (out of 19)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(DefaultActor pid=24179)\u001b[0m /home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py:72: DeprecationWarning:  Ensure your client is of type `flwr.client.Client`. Please convert it using the `.to_client()` method before returning it in the `client_fn` you pass to `start_simulation`. We have applied this conversion on your behalf. Not returning a `Client` might trigger an error in future versions of Flower.\u001b[32m [repeated 23x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24179)\u001b[0m   client = check_clientfn_returns_client(client_fn(cid))\u001b[32m [repeated 23x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(DefaultActor pid=24186)\u001b[0m starting client: 5 <class 'str'>\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24186)\u001b[0m starting client:  <class 'int'>\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24186)\u001b[0m Creating client model to client: 5\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24186)\u001b[0m Data X: 26567\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24186)\u001b[0m Data Y: 26567\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24186)\u001b[0m Creating client model to client: 5 round 0\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24186)\u001b[0m 2\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24171)\u001b[0m starting client: 17 <class 'str'>\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24171)\u001b[0m starting client:  <class 'int'>\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24171)\u001b[0m Creating client model to client: 17\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24171)\u001b[0m Data X: 22059\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24171)\u001b[0m Data Y: 22059\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24171)\u001b[0m Creating client model to client: 17 round 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-02-15 18:05:31,416 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: c15383f49961ecb4a37d265501000000, name=DefaultActor.__init__, pid=24180, memory used=0.37GB) was running was 7.12GB / 7.44GB (0.95787), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: b5d7f31e8e1c5600a4998934a7df6c8f6b9f159ae5138dfa91eb3fcf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-b5d7f31e8e1c5600a4998934a7df6c8f6b9f159ae5138dfa91eb3fcf*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.83\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24186\t0.38\tray::DefaultActor.run\n",
      "24171\t0.38\tray::DefaultActor.run\n",
      "24172\t0.37\tray::DefaultActor.run\n",
      "24175\t0.37\tray::DefaultActor.run\n",
      "24182\t0.37\tray::DefaultActor\n",
      "24181\t0.37\tray::DefaultActor.run\n",
      "24179\t0.37\tray::DefaultActor\n",
      "24180\t0.37\tray::DefaultActor.run\n",
      "24185\t0.37\tray::DefaultActor\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-02-15 18:05:31,416 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: 8080131699fdec87227ce5a101000000, name=DefaultActor.__init__, pid=24181, memory used=0.39GB) was running was 7.07GB / 7.44GB (0.950481), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8f9f4202b7c08e54248e55344f63a4dff0d1bcebccfc900a981c5fe4) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-8f9f4202b7c08e54248e55344f63a4dff0d1bcebccfc900a981c5fe4*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.84\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24171\t0.40\tray::DefaultActor.run\n",
      "24182\t0.40\tray::DefaultActor.run\n",
      "24185\t0.39\tray::DefaultActor.run\n",
      "24176\t0.39\tray::DefaultActor.run\n",
      "24175\t0.39\tray::DefaultActor.run\n",
      "24186\t0.39\tray::DefaultActor.run\n",
      "24181\t0.39\tray::DefaultActor.run\n",
      "24172\t0.39\tray::DefaultActor.run\n",
      "24179\t0.39\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-02-15 18:05:31,418 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: b69812a30d49bad7a23056ae01000000, name=DefaultActor.__init__, pid=24177, memory used=0.36GB) was running was 7.07GB / 7.44GB (0.950478), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: eb87cbf60b1e6dc3a911ba590051b73508b005f44b4692d9a1345e71) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-eb87cbf60b1e6dc3a911ba590051b73508b005f44b4692d9a1345e71*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.13\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24189\t0.37\tray::DefaultActor\n",
      "24182\t0.37\tray::DefaultActor\n",
      "24186\t0.37\tray::DefaultActor.run\n",
      "24185\t0.37\tray::DefaultActor.run\n",
      "24175\t0.37\tray::DefaultActor.run\n",
      "24181\t0.37\tray::DefaultActor\n",
      "24179\t0.37\tray::DefaultActor.run\n",
      "24180\t0.37\tray::DefaultActor\n",
      "24172\t0.36\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-02-15 18:05:31,418 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: c15383f49961ecb4a37d265501000000, name=DefaultActor.__init__, pid=24180, memory used=0.37GB) was running was 7.12GB / 7.44GB (0.95787), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: b5d7f31e8e1c5600a4998934a7df6c8f6b9f159ae5138dfa91eb3fcf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-b5d7f31e8e1c5600a4998934a7df6c8f6b9f159ae5138dfa91eb3fcf*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.83\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24186\t0.38\tray::DefaultActor.run\n",
      "24171\t0.38\tray::DefaultActor.run\n",
      "24172\t0.37\tray::DefaultActor.run\n",
      "24175\t0.37\tray::DefaultActor.run\n",
      "24182\t0.37\tray::DefaultActor\n",
      "24181\t0.37\tray::DefaultActor.run\n",
      "24179\t0.37\tray::DefaultActor\n",
      "24180\t0.37\tray::DefaultActor.run\n",
      "24185\t0.37\tray::DefaultActor\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-02-15 18:05:31,418 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: d581abe9f194281cae040ffe01000000, name=DefaultActor.__init__, pid=24187, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950222), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ca86cd785378531d2b883fd9a151213ec5b7e9009ad88559c186049f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ca86cd785378531d2b883fd9a151213ec5b7e9009ad88559c186049f*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24189\t0.31\tray::DefaultActor.run\n",
      "24182\t0.31\tray::DefaultActor.run\n",
      "24186\t0.31\tray::DefaultActor.run\n",
      "24180\t0.31\tray::DefaultActor.run\n",
      "24178\t0.30\tray::DefaultActor.run\n",
      "24184\t0.30\tray::DefaultActor.run\n",
      "24179\t0.30\tray::DefaultActor.run\n",
      "24174\t0.30\tray::DefaultActor.run\n",
      "24185\t0.30\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-02-15 18:05:31,420 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: 8080131699fdec87227ce5a101000000, name=DefaultActor.__init__, pid=24181, memory used=0.39GB) was running was 7.07GB / 7.44GB (0.950481), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8f9f4202b7c08e54248e55344f63a4dff0d1bcebccfc900a981c5fe4) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-8f9f4202b7c08e54248e55344f63a4dff0d1bcebccfc900a981c5fe4*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.84\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24171\t0.40\tray::DefaultActor.run\n",
      "24182\t0.40\tray::DefaultActor.run\n",
      "24185\t0.39\tray::DefaultActor.run\n",
      "24176\t0.39\tray::DefaultActor.run\n",
      "24175\t0.39\tray::DefaultActor.run\n",
      "24186\t0.39\tray::DefaultActor.run\n",
      "24181\t0.39\tray::DefaultActor.run\n",
      "24172\t0.39\tray::DefaultActor.run\n",
      "24179\t0.39\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-02-15 18:05:31,420 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: 388fabd5122dfa924f937a4401000000, name=DefaultActor.__init__, pid=24184, memory used=0.33GB) was running was 7.07GB / 7.44GB (0.950416), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7183a21e8c6dc6d32f920bd6e9b8a0be3c757fc658e03caf33a1bb11) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7183a21e8c6dc6d32f920bd6e9b8a0be3c757fc658e03caf33a1bb11*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.12\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24184\t0.33\tray::DefaultActor.run\n",
      "24186\t0.32\tray::DefaultActor.run\n",
      "24175\t0.32\tray::DefaultActor.run\n",
      "24189\t0.32\tray::DefaultActor.run\n",
      "24181\t0.32\tray::DefaultActor.run\n",
      "24172\t0.32\tray::DefaultActor.run\n",
      "24179\t0.32\tray::DefaultActor.run\n",
      "24185\t0.32\tray::DefaultActor.run\n",
      "24182\t0.32\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-02-15 18:05:31,421 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: b69812a30d49bad7a23056ae01000000, name=DefaultActor.__init__, pid=24177, memory used=0.36GB) was running was 7.07GB / 7.44GB (0.950478), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: eb87cbf60b1e6dc3a911ba590051b73508b005f44b4692d9a1345e71) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-eb87cbf60b1e6dc3a911ba590051b73508b005f44b4692d9a1345e71*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.13\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24189\t0.37\tray::DefaultActor\n",
      "24182\t0.37\tray::DefaultActor\n",
      "24186\t0.37\tray::DefaultActor.run\n",
      "24185\t0.37\tray::DefaultActor.run\n",
      "24175\t0.37\tray::DefaultActor.run\n",
      "24181\t0.37\tray::DefaultActor\n",
      "24179\t0.37\tray::DefaultActor.run\n",
      "24180\t0.37\tray::DefaultActor\n",
      "24172\t0.36\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-02-15 18:05:31,423 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: 6edfb7c0691ff0c70305db8d01000000, name=DefaultActor.__init__, pid=24189, memory used=0.37GB) was running was 7.22GB / 7.44GB (0.97048), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7d24c495b5bb39a4c54813e900f4c2f98e31d8b9f36138518c6bfa2b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7d24c495b5bb39a4c54813e900f4c2f98e31d8b9f36138518c6bfa2b*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.59\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24186\t0.38\tray::DefaultActor\n",
      "24189\t0.37\tray::DefaultActor\n",
      "24182\t0.37\tray::DefaultActor\n",
      "24179\t0.37\tray::DefaultActor\n",
      "24185\t0.37\tray::DefaultActor\n",
      "24175\t0.37\tray::DefaultActor\n",
      "24181\t0.37\tray::DefaultActor\n",
      "24172\t0.37\tray::DefaultActor\n",
      "24171\t0.37\tray::DefaultActor\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-02-15 18:05:31,425 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: d581abe9f194281cae040ffe01000000, name=DefaultActor.__init__, pid=24187, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950222), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ca86cd785378531d2b883fd9a151213ec5b7e9009ad88559c186049f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ca86cd785378531d2b883fd9a151213ec5b7e9009ad88559c186049f*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24189\t0.31\tray::DefaultActor.run\n",
      "24182\t0.31\tray::DefaultActor.run\n",
      "24186\t0.31\tray::DefaultActor.run\n",
      "24180\t0.31\tray::DefaultActor.run\n",
      "24178\t0.30\tray::DefaultActor.run\n",
      "24184\t0.30\tray::DefaultActor.run\n",
      "24179\t0.30\tray::DefaultActor.run\n",
      "24174\t0.30\tray::DefaultActor.run\n",
      "24185\t0.30\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-02-15 18:05:31,427 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: 388fabd5122dfa924f937a4401000000, name=DefaultActor.__init__, pid=24184, memory used=0.33GB) was running was 7.07GB / 7.44GB (0.950416), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7183a21e8c6dc6d32f920bd6e9b8a0be3c757fc658e03caf33a1bb11) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7183a21e8c6dc6d32f920bd6e9b8a0be3c757fc658e03caf33a1bb11*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.12\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24184\t0.33\tray::DefaultActor.run\n",
      "24186\t0.32\tray::DefaultActor.run\n",
      "24175\t0.32\tray::DefaultActor.run\n",
      "24189\t0.32\tray::DefaultActor.run\n",
      "24181\t0.32\tray::DefaultActor.run\n",
      "24172\t0.32\tray::DefaultActor.run\n",
      "24179\t0.32\tray::DefaultActor.run\n",
      "24185\t0.32\tray::DefaultActor.run\n",
      "24182\t0.32\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-02-15 18:05:31,428 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: 1aabde1e63fd801f3a80b1b001000000, name=DefaultActor.__init__, pid=24178, memory used=0.33GB) was running was 7.07GB / 7.44GB (0.950049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5520f083efbdea86985d6fe1a9f8ec4a8d92792d52c2c3639df180d8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-5520f083efbdea86985d6fe1a9f8ec4a8d92792d52c2c3639df180d8*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.13\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24189\t0.37\tray::DefaultActor.run\n",
      "24182\t0.37\tray::DefaultActor.run\n",
      "24180\t0.36\tray::DefaultActor.run\n",
      "24174\t0.36\tray::DefaultActor\n",
      "24181\t0.35\tray::DefaultActor.run\n",
      "24175\t0.35\tray::DefaultActor.run\n",
      "24176\t0.34\tray::DefaultActor.run\n",
      "24185\t0.34\tray::DefaultActor.run\n",
      "24186\t0.33\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-02-15 18:05:31,430 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: 6edfb7c0691ff0c70305db8d01000000, name=DefaultActor.__init__, pid=24189, memory used=0.37GB) was running was 7.22GB / 7.44GB (0.97048), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7d24c495b5bb39a4c54813e900f4c2f98e31d8b9f36138518c6bfa2b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7d24c495b5bb39a4c54813e900f4c2f98e31d8b9f36138518c6bfa2b*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.59\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24186\t0.38\tray::DefaultActor\n",
      "24189\t0.37\tray::DefaultActor\n",
      "24182\t0.37\tray::DefaultActor\n",
      "24179\t0.37\tray::DefaultActor\n",
      "24185\t0.37\tray::DefaultActor\n",
      "24175\t0.37\tray::DefaultActor\n",
      "24181\t0.37\tray::DefaultActor\n",
      "24172\t0.37\tray::DefaultActor\n",
      "24171\t0.37\tray::DefaultActor\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-02-15 18:05:31,434 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: 1aabde1e63fd801f3a80b1b001000000, name=DefaultActor.__init__, pid=24178, memory used=0.33GB) was running was 7.07GB / 7.44GB (0.950049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5520f083efbdea86985d6fe1a9f8ec4a8d92792d52c2c3639df180d8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-5520f083efbdea86985d6fe1a9f8ec4a8d92792d52c2c3639df180d8*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.13\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24189\t0.37\tray::DefaultActor.run\n",
      "24182\t0.37\tray::DefaultActor.run\n",
      "24180\t0.36\tray::DefaultActor.run\n",
      "24174\t0.36\tray::DefaultActor\n",
      "24181\t0.35\tray::DefaultActor.run\n",
      "24175\t0.35\tray::DefaultActor.run\n",
      "24176\t0.34\tray::DefaultActor.run\n",
      "24185\t0.34\tray::DefaultActor.run\n",
      "24186\t0.33\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "DEBUG flwr 2024-02-15 18:05:31,458 | server.py:187 | evaluate_round 3 received 0 results and 19 failures\n",
      "DEBUG flwr 2024-02-15 18:05:31,459 | server.py:222 | fit_round 4: strategy sampled 19 clients (out of 19)\n",
      "ERROR flwr 2024-02-15 18:05:32,225 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: 1aabde1e63fd801f3a80b1b001000000, name=DefaultActor.__init__, pid=24178, memory used=0.33GB) was running was 7.07GB / 7.44GB (0.950049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5520f083efbdea86985d6fe1a9f8ec4a8d92792d52c2c3639df180d8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-5520f083efbdea86985d6fe1a9f8ec4a8d92792d52c2c3639df180d8*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.13\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24189\t0.37\tray::DefaultActor.run\n",
      "24182\t0.37\tray::DefaultActor.run\n",
      "24180\t0.36\tray::DefaultActor.run\n",
      "24174\t0.36\tray::DefaultActor\n",
      "24181\t0.35\tray::DefaultActor.run\n",
      "24175\t0.35\tray::DefaultActor.run\n",
      "24176\t0.34\tray::DefaultActor.run\n",
      "24185\t0.34\tray::DefaultActor.run\n",
      "24186\t0.33\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-02-15 18:05:32,227 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: 1aabde1e63fd801f3a80b1b001000000, name=DefaultActor.__init__, pid=24178, memory used=0.33GB) was running was 7.07GB / 7.44GB (0.950049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5520f083efbdea86985d6fe1a9f8ec4a8d92792d52c2c3639df180d8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-5520f083efbdea86985d6fe1a9f8ec4a8d92792d52c2c3639df180d8*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.13\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24189\t0.37\tray::DefaultActor.run\n",
      "24182\t0.37\tray::DefaultActor.run\n",
      "24180\t0.36\tray::DefaultActor.run\n",
      "24174\t0.36\tray::DefaultActor\n",
      "24181\t0.35\tray::DefaultActor.run\n",
      "24175\t0.35\tray::DefaultActor.run\n",
      "24176\t0.34\tray::DefaultActor.run\n",
      "24185\t0.34\tray::DefaultActor.run\n",
      "24186\t0.33\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-02-15 18:05:32,231 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: 6edfb7c0691ff0c70305db8d01000000, name=DefaultActor.__init__, pid=24189, memory used=0.37GB) was running was 7.22GB / 7.44GB (0.97048), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7d24c495b5bb39a4c54813e900f4c2f98e31d8b9f36138518c6bfa2b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7d24c495b5bb39a4c54813e900f4c2f98e31d8b9f36138518c6bfa2b*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.59\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24186\t0.38\tray::DefaultActor\n",
      "24189\t0.37\tray::DefaultActor\n",
      "24182\t0.37\tray::DefaultActor\n",
      "24179\t0.37\tray::DefaultActor\n",
      "24185\t0.37\tray::DefaultActor\n",
      "24175\t0.37\tray::DefaultActor\n",
      "24181\t0.37\tray::DefaultActor\n",
      "24172\t0.37\tray::DefaultActor\n",
      "24171\t0.37\tray::DefaultActor\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-02-15 18:05:32,232 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: b69812a30d49bad7a23056ae01000000, name=DefaultActor.__init__, pid=24177, memory used=0.36GB) was running was 7.07GB / 7.44GB (0.950478), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: eb87cbf60b1e6dc3a911ba590051b73508b005f44b4692d9a1345e71) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-eb87cbf60b1e6dc3a911ba590051b73508b005f44b4692d9a1345e71*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.13\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24189\t0.37\tray::DefaultActor\n",
      "24182\t0.37\tray::DefaultActor\n",
      "24186\t0.37\tray::DefaultActor.run\n",
      "24185\t0.37\tray::DefaultActor.run\n",
      "24175\t0.37\tray::DefaultActor.run\n",
      "24181\t0.37\tray::DefaultActor\n",
      "24179\t0.37\tray::DefaultActor.run\n",
      "24180\t0.37\tray::DefaultActor\n",
      "24172\t0.36\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-02-15 18:05:32,233 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: 6edfb7c0691ff0c70305db8d01000000, name=DefaultActor.__init__, pid=24189, memory used=0.37GB) was running was 7.22GB / 7.44GB (0.97048), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7d24c495b5bb39a4c54813e900f4c2f98e31d8b9f36138518c6bfa2b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7d24c495b5bb39a4c54813e900f4c2f98e31d8b9f36138518c6bfa2b*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.59\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24186\t0.38\tray::DefaultActor\n",
      "24189\t0.37\tray::DefaultActor\n",
      "24182\t0.37\tray::DefaultActor\n",
      "24179\t0.37\tray::DefaultActor\n",
      "24185\t0.37\tray::DefaultActor\n",
      "24175\t0.37\tray::DefaultActor\n",
      "24181\t0.37\tray::DefaultActor\n",
      "24172\t0.37\tray::DefaultActor\n",
      "24171\t0.37\tray::DefaultActor\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-02-15 18:05:32,235 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: b69812a30d49bad7a23056ae01000000, name=DefaultActor.__init__, pid=24177, memory used=0.36GB) was running was 7.07GB / 7.44GB (0.950478), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: eb87cbf60b1e6dc3a911ba590051b73508b005f44b4692d9a1345e71) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-eb87cbf60b1e6dc3a911ba590051b73508b005f44b4692d9a1345e71*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.13\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24189\t0.37\tray::DefaultActor\n",
      "24182\t0.37\tray::DefaultActor\n",
      "24186\t0.37\tray::DefaultActor.run\n",
      "24185\t0.37\tray::DefaultActor.run\n",
      "24175\t0.37\tray::DefaultActor.run\n",
      "24181\t0.37\tray::DefaultActor\n",
      "24179\t0.37\tray::DefaultActor.run\n",
      "24180\t0.37\tray::DefaultActor\n",
      "24172\t0.36\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-02-15 18:05:32,283 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: 388fabd5122dfa924f937a4401000000, name=DefaultActor.__init__, pid=24184, memory used=0.33GB) was running was 7.07GB / 7.44GB (0.950416), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7183a21e8c6dc6d32f920bd6e9b8a0be3c757fc658e03caf33a1bb11) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7183a21e8c6dc6d32f920bd6e9b8a0be3c757fc658e03caf33a1bb11*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.12\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24184\t0.33\tray::DefaultActor.run\n",
      "24186\t0.32\tray::DefaultActor.run\n",
      "24175\t0.32\tray::DefaultActor.run\n",
      "24189\t0.32\tray::DefaultActor.run\n",
      "24181\t0.32\tray::DefaultActor.run\n",
      "24172\t0.32\tray::DefaultActor.run\n",
      "24179\t0.32\tray::DefaultActor.run\n",
      "24185\t0.32\tray::DefaultActor.run\n",
      "24182\t0.32\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-02-15 18:05:32,284 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: d581abe9f194281cae040ffe01000000, name=DefaultActor.__init__, pid=24187, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950222), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ca86cd785378531d2b883fd9a151213ec5b7e9009ad88559c186049f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ca86cd785378531d2b883fd9a151213ec5b7e9009ad88559c186049f*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24189\t0.31\tray::DefaultActor.run\n",
      "24182\t0.31\tray::DefaultActor.run\n",
      "24186\t0.31\tray::DefaultActor.run\n",
      "24180\t0.31\tray::DefaultActor.run\n",
      "24178\t0.30\tray::DefaultActor.run\n",
      "24184\t0.30\tray::DefaultActor.run\n",
      "24179\t0.30\tray::DefaultActor.run\n",
      "24174\t0.30\tray::DefaultActor.run\n",
      "24185\t0.30\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-02-15 18:05:32,284 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: 8080131699fdec87227ce5a101000000, name=DefaultActor.__init__, pid=24181, memory used=0.39GB) was running was 7.07GB / 7.44GB (0.950481), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8f9f4202b7c08e54248e55344f63a4dff0d1bcebccfc900a981c5fe4) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-8f9f4202b7c08e54248e55344f63a4dff0d1bcebccfc900a981c5fe4*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.84\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24171\t0.40\tray::DefaultActor.run\n",
      "24182\t0.40\tray::DefaultActor.run\n",
      "24185\t0.39\tray::DefaultActor.run\n",
      "24176\t0.39\tray::DefaultActor.run\n",
      "24175\t0.39\tray::DefaultActor.run\n",
      "24186\t0.39\tray::DefaultActor.run\n",
      "24181\t0.39\tray::DefaultActor.run\n",
      "24172\t0.39\tray::DefaultActor.run\n",
      "24179\t0.39\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-02-15 18:05:32,287 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: 388fabd5122dfa924f937a4401000000, name=DefaultActor.__init__, pid=24184, memory used=0.33GB) was running was 7.07GB / 7.44GB (0.950416), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7183a21e8c6dc6d32f920bd6e9b8a0be3c757fc658e03caf33a1bb11) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7183a21e8c6dc6d32f920bd6e9b8a0be3c757fc658e03caf33a1bb11*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.12\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24184\t0.33\tray::DefaultActor.run\n",
      "24186\t0.32\tray::DefaultActor.run\n",
      "24175\t0.32\tray::DefaultActor.run\n",
      "24189\t0.32\tray::DefaultActor.run\n",
      "24181\t0.32\tray::DefaultActor.run\n",
      "24172\t0.32\tray::DefaultActor.run\n",
      "24179\t0.32\tray::DefaultActor.run\n",
      "24185\t0.32\tray::DefaultActor.run\n",
      "24182\t0.32\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-02-15 18:05:32,288 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: c15383f49961ecb4a37d265501000000, name=DefaultActor.__init__, pid=24180, memory used=0.37GB) was running was 7.12GB / 7.44GB (0.95787), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: b5d7f31e8e1c5600a4998934a7df6c8f6b9f159ae5138dfa91eb3fcf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-b5d7f31e8e1c5600a4998934a7df6c8f6b9f159ae5138dfa91eb3fcf*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.83\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24186\t0.38\tray::DefaultActor.run\n",
      "24171\t0.38\tray::DefaultActor.run\n",
      "24172\t0.37\tray::DefaultActor.run\n",
      "24175\t0.37\tray::DefaultActor.run\n",
      "24182\t0.37\tray::DefaultActor\n",
      "24181\t0.37\tray::DefaultActor.run\n",
      "24179\t0.37\tray::DefaultActor\n",
      "24180\t0.37\tray::DefaultActor.run\n",
      "24185\t0.37\tray::DefaultActor\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-02-15 18:05:32,290 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: d581abe9f194281cae040ffe01000000, name=DefaultActor.__init__, pid=24187, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950222), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ca86cd785378531d2b883fd9a151213ec5b7e9009ad88559c186049f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ca86cd785378531d2b883fd9a151213ec5b7e9009ad88559c186049f*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24189\t0.31\tray::DefaultActor.run\n",
      "24182\t0.31\tray::DefaultActor.run\n",
      "24186\t0.31\tray::DefaultActor.run\n",
      "24180\t0.31\tray::DefaultActor.run\n",
      "24178\t0.30\tray::DefaultActor.run\n",
      "24184\t0.30\tray::DefaultActor.run\n",
      "24179\t0.30\tray::DefaultActor.run\n",
      "24174\t0.30\tray::DefaultActor.run\n",
      "24185\t0.30\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-02-15 18:05:32,294 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: 8080131699fdec87227ce5a101000000, name=DefaultActor.__init__, pid=24181, memory used=0.39GB) was running was 7.07GB / 7.44GB (0.950481), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8f9f4202b7c08e54248e55344f63a4dff0d1bcebccfc900a981c5fe4) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-8f9f4202b7c08e54248e55344f63a4dff0d1bcebccfc900a981c5fe4*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.84\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24171\t0.40\tray::DefaultActor.run\n",
      "24182\t0.40\tray::DefaultActor.run\n",
      "24185\t0.39\tray::DefaultActor.run\n",
      "24176\t0.39\tray::DefaultActor.run\n",
      "24175\t0.39\tray::DefaultActor.run\n",
      "24186\t0.39\tray::DefaultActor.run\n",
      "24181\t0.39\tray::DefaultActor.run\n",
      "24172\t0.39\tray::DefaultActor.run\n",
      "24179\t0.39\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-02-15 18:05:32,295 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: b69812a30d49bad7a23056ae01000000, name=DefaultActor.__init__, pid=24177, memory used=0.36GB) was running was 7.07GB / 7.44GB (0.950478), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: eb87cbf60b1e6dc3a911ba590051b73508b005f44b4692d9a1345e71) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-eb87cbf60b1e6dc3a911ba590051b73508b005f44b4692d9a1345e71*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.13\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24189\t0.37\tray::DefaultActor\n",
      "24182\t0.37\tray::DefaultActor\n",
      "24186\t0.37\tray::DefaultActor.run\n",
      "24185\t0.37\tray::DefaultActor.run\n",
      "24175\t0.37\tray::DefaultActor.run\n",
      "24181\t0.37\tray::DefaultActor\n",
      "24179\t0.37\tray::DefaultActor.run\n",
      "24180\t0.37\tray::DefaultActor\n",
      "24172\t0.36\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-02-15 18:05:32,300 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: c15383f49961ecb4a37d265501000000, name=DefaultActor.__init__, pid=24180, memory used=0.37GB) was running was 7.12GB / 7.44GB (0.95787), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: b5d7f31e8e1c5600a4998934a7df6c8f6b9f159ae5138dfa91eb3fcf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-b5d7f31e8e1c5600a4998934a7df6c8f6b9f159ae5138dfa91eb3fcf*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.83\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24186\t0.38\tray::DefaultActor.run\n",
      "24171\t0.38\tray::DefaultActor.run\n",
      "24172\t0.37\tray::DefaultActor.run\n",
      "24175\t0.37\tray::DefaultActor.run\n",
      "24182\t0.37\tray::DefaultActor\n",
      "24181\t0.37\tray::DefaultActor.run\n",
      "24179\t0.37\tray::DefaultActor\n",
      "24180\t0.37\tray::DefaultActor.run\n",
      "24185\t0.37\tray::DefaultActor\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-02-15 18:05:32,300 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: 1aabde1e63fd801f3a80b1b001000000, name=DefaultActor.__init__, pid=24178, memory used=0.33GB) was running was 7.07GB / 7.44GB (0.950049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5520f083efbdea86985d6fe1a9f8ec4a8d92792d52c2c3639df180d8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-5520f083efbdea86985d6fe1a9f8ec4a8d92792d52c2c3639df180d8*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.13\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24189\t0.37\tray::DefaultActor.run\n",
      "24182\t0.37\tray::DefaultActor.run\n",
      "24180\t0.36\tray::DefaultActor.run\n",
      "24174\t0.36\tray::DefaultActor\n",
      "24181\t0.35\tray::DefaultActor.run\n",
      "24175\t0.35\tray::DefaultActor.run\n",
      "24176\t0.34\tray::DefaultActor.run\n",
      "24185\t0.34\tray::DefaultActor.run\n",
      "24186\t0.33\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-02-15 18:05:32,303 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: 8080131699fdec87227ce5a101000000, name=DefaultActor.__init__, pid=24181, memory used=0.39GB) was running was 7.07GB / 7.44GB (0.950481), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8f9f4202b7c08e54248e55344f63a4dff0d1bcebccfc900a981c5fe4) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-8f9f4202b7c08e54248e55344f63a4dff0d1bcebccfc900a981c5fe4*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.84\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24171\t0.40\tray::DefaultActor.run\n",
      "24182\t0.40\tray::DefaultActor.run\n",
      "24185\t0.39\tray::DefaultActor.run\n",
      "24176\t0.39\tray::DefaultActor.run\n",
      "24175\t0.39\tray::DefaultActor.run\n",
      "24186\t0.39\tray::DefaultActor.run\n",
      "24181\t0.39\tray::DefaultActor.run\n",
      "24172\t0.39\tray::DefaultActor.run\n",
      "24179\t0.39\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-02-15 18:05:32,306 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: b69812a30d49bad7a23056ae01000000, name=DefaultActor.__init__, pid=24177, memory used=0.36GB) was running was 7.07GB / 7.44GB (0.950478), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: eb87cbf60b1e6dc3a911ba590051b73508b005f44b4692d9a1345e71) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-eb87cbf60b1e6dc3a911ba590051b73508b005f44b4692d9a1345e71*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.13\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24189\t0.37\tray::DefaultActor\n",
      "24182\t0.37\tray::DefaultActor\n",
      "24186\t0.37\tray::DefaultActor.run\n",
      "24185\t0.37\tray::DefaultActor.run\n",
      "24175\t0.37\tray::DefaultActor.run\n",
      "24181\t0.37\tray::DefaultActor\n",
      "24179\t0.37\tray::DefaultActor.run\n",
      "24180\t0.37\tray::DefaultActor\n",
      "24172\t0.36\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-02-15 18:05:32,309 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: 1aabde1e63fd801f3a80b1b001000000, name=DefaultActor.__init__, pid=24178, memory used=0.33GB) was running was 7.07GB / 7.44GB (0.950049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5520f083efbdea86985d6fe1a9f8ec4a8d92792d52c2c3639df180d8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-5520f083efbdea86985d6fe1a9f8ec4a8d92792d52c2c3639df180d8*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.13\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24189\t0.37\tray::DefaultActor.run\n",
      "24182\t0.37\tray::DefaultActor.run\n",
      "24180\t0.36\tray::DefaultActor.run\n",
      "24174\t0.36\tray::DefaultActor\n",
      "24181\t0.35\tray::DefaultActor.run\n",
      "24175\t0.35\tray::DefaultActor.run\n",
      "24176\t0.34\tray::DefaultActor.run\n",
      "24185\t0.34\tray::DefaultActor.run\n",
      "24186\t0.33\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-02-15 18:05:32,311 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: 8080131699fdec87227ce5a101000000, name=DefaultActor.__init__, pid=24181, memory used=0.39GB) was running was 7.07GB / 7.44GB (0.950481), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8f9f4202b7c08e54248e55344f63a4dff0d1bcebccfc900a981c5fe4) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-8f9f4202b7c08e54248e55344f63a4dff0d1bcebccfc900a981c5fe4*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.84\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24171\t0.40\tray::DefaultActor.run\n",
      "24182\t0.40\tray::DefaultActor.run\n",
      "24185\t0.39\tray::DefaultActor.run\n",
      "24176\t0.39\tray::DefaultActor.run\n",
      "24175\t0.39\tray::DefaultActor.run\n",
      "24186\t0.39\tray::DefaultActor.run\n",
      "24181\t0.39\tray::DefaultActor.run\n",
      "24172\t0.39\tray::DefaultActor.run\n",
      "24179\t0.39\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(DefaultActor pid=24182)\u001b[0m 2\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24185)\u001b[0m starting client: 17 <class 'str'>\u001b[32m [repeated 19x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24185)\u001b[0m starting client:  <class 'int'>\u001b[32m [repeated 19x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24185)\u001b[0m Creating client model to client: 17\u001b[32m [repeated 19x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24185)\u001b[0m Data X: 22059\u001b[32m [repeated 19x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24185)\u001b[0m Data Y: 22059\u001b[32m [repeated 19x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24185)\u001b[0m Creating client model to client: 17 round 0\u001b[32m [repeated 19x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24171)\u001b[0m 2\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24179)\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2024-02-15 18:05:50,569 | server.py:236 | fit_round 4 received 9 results and 10 failures\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving round 4 aggregated_ndarrays...\n",
      "result_unbalanced_epoch_1_rounds_5_cycle_1/checkpoints/round-4-weights.npz\n",
      "\n",
      "Evaluating global model round 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2024-02-15 18:05:51,752 E 23772 23772] (raylet) node_manager.cc:3084: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67, IP: 172.30.126.159) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.30.126.159`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "INFO flwr 2024-02-15 18:05:57,475 | server.py:125 | fit progress: (4, 0.9044705033302307, {'accuracy': 0.7454184293746948}, 111.68621444200016)\n",
      "DEBUG flwr 2024-02-15 18:05:57,476 | server.py:173 | evaluate_round 4: strategy sampled 19 clients (out of 19)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(DefaultActor pid=24182)\u001b[0m /home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py:72: DeprecationWarning:  Ensure your client is of type `flwr.client.Client`. Please convert it using the `.to_client()` method before returning it in the `client_fn` you pass to `start_simulation`. We have applied this conversion on your behalf. Not returning a `Client` might trigger an error in future versions of Flower.\u001b[32m [repeated 21x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24182)\u001b[0m   client = check_clientfn_returns_client(client_fn(cid))\u001b[32m [repeated 21x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(DefaultActor pid=24182)\u001b[0m starting client: 7 <class 'str'>\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24182)\u001b[0m starting client:  <class 'int'>\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24182)\u001b[0m Creating client model to client: 7\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24182)\u001b[0m Data X: 33903\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24182)\u001b[0m Data Y: 33903\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24182)\u001b[0m Creating client model to client: 7 round 0\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24172)\u001b[0m 2\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24172)\u001b[0m starting client: 2 <class 'str'>\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24172)\u001b[0m starting client:  <class 'int'>\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24172)\u001b[0m Creating client model to client: 2\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24172)\u001b[0m Data X: 3383\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24172)\u001b[0m Data Y: 3383\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24172)\u001b[0m Creating client model to client: 2 round 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-02-15 18:05:57,891 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: 8080131699fdec87227ce5a101000000, name=DefaultActor.__init__, pid=24181, memory used=0.39GB) was running was 7.07GB / 7.44GB (0.950481), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8f9f4202b7c08e54248e55344f63a4dff0d1bcebccfc900a981c5fe4) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-8f9f4202b7c08e54248e55344f63a4dff0d1bcebccfc900a981c5fe4*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.84\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24171\t0.40\tray::DefaultActor.run\n",
      "24182\t0.40\tray::DefaultActor.run\n",
      "24185\t0.39\tray::DefaultActor.run\n",
      "24176\t0.39\tray::DefaultActor.run\n",
      "24175\t0.39\tray::DefaultActor.run\n",
      "24186\t0.39\tray::DefaultActor.run\n",
      "24181\t0.39\tray::DefaultActor.run\n",
      "24172\t0.39\tray::DefaultActor.run\n",
      "24179\t0.39\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-02-15 18:05:57,893 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: 8080131699fdec87227ce5a101000000, name=DefaultActor.__init__, pid=24181, memory used=0.39GB) was running was 7.07GB / 7.44GB (0.950481), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8f9f4202b7c08e54248e55344f63a4dff0d1bcebccfc900a981c5fe4) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-8f9f4202b7c08e54248e55344f63a4dff0d1bcebccfc900a981c5fe4*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.84\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24171\t0.40\tray::DefaultActor.run\n",
      "24182\t0.40\tray::DefaultActor.run\n",
      "24185\t0.39\tray::DefaultActor.run\n",
      "24176\t0.39\tray::DefaultActor.run\n",
      "24175\t0.39\tray::DefaultActor.run\n",
      "24186\t0.39\tray::DefaultActor.run\n",
      "24181\t0.39\tray::DefaultActor.run\n",
      "24172\t0.39\tray::DefaultActor.run\n",
      "24179\t0.39\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-02-15 18:05:57,893 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: b69812a30d49bad7a23056ae01000000, name=DefaultActor.__init__, pid=24177, memory used=0.36GB) was running was 7.07GB / 7.44GB (0.950478), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: eb87cbf60b1e6dc3a911ba590051b73508b005f44b4692d9a1345e71) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-eb87cbf60b1e6dc3a911ba590051b73508b005f44b4692d9a1345e71*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.13\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24189\t0.37\tray::DefaultActor\n",
      "24182\t0.37\tray::DefaultActor\n",
      "24186\t0.37\tray::DefaultActor.run\n",
      "24185\t0.37\tray::DefaultActor.run\n",
      "24175\t0.37\tray::DefaultActor.run\n",
      "24181\t0.37\tray::DefaultActor\n",
      "24179\t0.37\tray::DefaultActor.run\n",
      "24180\t0.37\tray::DefaultActor\n",
      "24172\t0.36\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-02-15 18:05:57,896 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: b69812a30d49bad7a23056ae01000000, name=DefaultActor.__init__, pid=24177, memory used=0.36GB) was running was 7.07GB / 7.44GB (0.950478), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: eb87cbf60b1e6dc3a911ba590051b73508b005f44b4692d9a1345e71) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-eb87cbf60b1e6dc3a911ba590051b73508b005f44b4692d9a1345e71*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.13\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24189\t0.37\tray::DefaultActor\n",
      "24182\t0.37\tray::DefaultActor\n",
      "24186\t0.37\tray::DefaultActor.run\n",
      "24185\t0.37\tray::DefaultActor.run\n",
      "24175\t0.37\tray::DefaultActor.run\n",
      "24181\t0.37\tray::DefaultActor\n",
      "24179\t0.37\tray::DefaultActor.run\n",
      "24180\t0.37\tray::DefaultActor\n",
      "24172\t0.36\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-02-15 18:05:57,896 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: 388fabd5122dfa924f937a4401000000, name=DefaultActor.__init__, pid=24184, memory used=0.33GB) was running was 7.07GB / 7.44GB (0.950416), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7183a21e8c6dc6d32f920bd6e9b8a0be3c757fc658e03caf33a1bb11) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7183a21e8c6dc6d32f920bd6e9b8a0be3c757fc658e03caf33a1bb11*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.12\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24184\t0.33\tray::DefaultActor.run\n",
      "24186\t0.32\tray::DefaultActor.run\n",
      "24175\t0.32\tray::DefaultActor.run\n",
      "24189\t0.32\tray::DefaultActor.run\n",
      "24181\t0.32\tray::DefaultActor.run\n",
      "24172\t0.32\tray::DefaultActor.run\n",
      "24179\t0.32\tray::DefaultActor.run\n",
      "24185\t0.32\tray::DefaultActor.run\n",
      "24182\t0.32\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-02-15 18:05:57,899 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: c15383f49961ecb4a37d265501000000, name=DefaultActor.__init__, pid=24180, memory used=0.37GB) was running was 7.12GB / 7.44GB (0.95787), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: b5d7f31e8e1c5600a4998934a7df6c8f6b9f159ae5138dfa91eb3fcf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-b5d7f31e8e1c5600a4998934a7df6c8f6b9f159ae5138dfa91eb3fcf*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.83\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24186\t0.38\tray::DefaultActor.run\n",
      "24171\t0.38\tray::DefaultActor.run\n",
      "24172\t0.37\tray::DefaultActor.run\n",
      "24175\t0.37\tray::DefaultActor.run\n",
      "24182\t0.37\tray::DefaultActor\n",
      "24181\t0.37\tray::DefaultActor.run\n",
      "24179\t0.37\tray::DefaultActor\n",
      "24180\t0.37\tray::DefaultActor.run\n",
      "24185\t0.37\tray::DefaultActor\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-02-15 18:05:57,899 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: 388fabd5122dfa924f937a4401000000, name=DefaultActor.__init__, pid=24184, memory used=0.33GB) was running was 7.07GB / 7.44GB (0.950416), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7183a21e8c6dc6d32f920bd6e9b8a0be3c757fc658e03caf33a1bb11) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7183a21e8c6dc6d32f920bd6e9b8a0be3c757fc658e03caf33a1bb11*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.12\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24184\t0.33\tray::DefaultActor.run\n",
      "24186\t0.32\tray::DefaultActor.run\n",
      "24175\t0.32\tray::DefaultActor.run\n",
      "24189\t0.32\tray::DefaultActor.run\n",
      "24181\t0.32\tray::DefaultActor.run\n",
      "24172\t0.32\tray::DefaultActor.run\n",
      "24179\t0.32\tray::DefaultActor.run\n",
      "24185\t0.32\tray::DefaultActor.run\n",
      "24182\t0.32\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-02-15 18:05:57,901 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: c15383f49961ecb4a37d265501000000, name=DefaultActor.__init__, pid=24180, memory used=0.37GB) was running was 7.12GB / 7.44GB (0.95787), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: b5d7f31e8e1c5600a4998934a7df6c8f6b9f159ae5138dfa91eb3fcf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-b5d7f31e8e1c5600a4998934a7df6c8f6b9f159ae5138dfa91eb3fcf*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.83\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24186\t0.38\tray::DefaultActor.run\n",
      "24171\t0.38\tray::DefaultActor.run\n",
      "24172\t0.37\tray::DefaultActor.run\n",
      "24175\t0.37\tray::DefaultActor.run\n",
      "24182\t0.37\tray::DefaultActor\n",
      "24181\t0.37\tray::DefaultActor.run\n",
      "24179\t0.37\tray::DefaultActor\n",
      "24180\t0.37\tray::DefaultActor.run\n",
      "24185\t0.37\tray::DefaultActor\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-02-15 18:05:57,906 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: 6edfb7c0691ff0c70305db8d01000000, name=DefaultActor.__init__, pid=24189, memory used=0.37GB) was running was 7.22GB / 7.44GB (0.97048), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7d24c495b5bb39a4c54813e900f4c2f98e31d8b9f36138518c6bfa2b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7d24c495b5bb39a4c54813e900f4c2f98e31d8b9f36138518c6bfa2b*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.59\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24186\t0.38\tray::DefaultActor\n",
      "24189\t0.37\tray::DefaultActor\n",
      "24182\t0.37\tray::DefaultActor\n",
      "24179\t0.37\tray::DefaultActor\n",
      "24185\t0.37\tray::DefaultActor\n",
      "24175\t0.37\tray::DefaultActor\n",
      "24181\t0.37\tray::DefaultActor\n",
      "24172\t0.37\tray::DefaultActor\n",
      "24171\t0.37\tray::DefaultActor\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-02-15 18:05:57,908 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: 6edfb7c0691ff0c70305db8d01000000, name=DefaultActor.__init__, pid=24189, memory used=0.37GB) was running was 7.22GB / 7.44GB (0.97048), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7d24c495b5bb39a4c54813e900f4c2f98e31d8b9f36138518c6bfa2b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7d24c495b5bb39a4c54813e900f4c2f98e31d8b9f36138518c6bfa2b*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.59\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24186\t0.38\tray::DefaultActor\n",
      "24189\t0.37\tray::DefaultActor\n",
      "24182\t0.37\tray::DefaultActor\n",
      "24179\t0.37\tray::DefaultActor\n",
      "24185\t0.37\tray::DefaultActor\n",
      "24175\t0.37\tray::DefaultActor\n",
      "24181\t0.37\tray::DefaultActor\n",
      "24172\t0.37\tray::DefaultActor\n",
      "24171\t0.37\tray::DefaultActor\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-02-15 18:05:57,909 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: d581abe9f194281cae040ffe01000000, name=DefaultActor.__init__, pid=24187, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950222), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ca86cd785378531d2b883fd9a151213ec5b7e9009ad88559c186049f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ca86cd785378531d2b883fd9a151213ec5b7e9009ad88559c186049f*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24189\t0.31\tray::DefaultActor.run\n",
      "24182\t0.31\tray::DefaultActor.run\n",
      "24186\t0.31\tray::DefaultActor.run\n",
      "24180\t0.31\tray::DefaultActor.run\n",
      "24178\t0.30\tray::DefaultActor.run\n",
      "24184\t0.30\tray::DefaultActor.run\n",
      "24179\t0.30\tray::DefaultActor.run\n",
      "24174\t0.30\tray::DefaultActor.run\n",
      "24185\t0.30\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-02-15 18:05:57,910 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: 1aabde1e63fd801f3a80b1b001000000, name=DefaultActor.__init__, pid=24178, memory used=0.33GB) was running was 7.07GB / 7.44GB (0.950049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5520f083efbdea86985d6fe1a9f8ec4a8d92792d52c2c3639df180d8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-5520f083efbdea86985d6fe1a9f8ec4a8d92792d52c2c3639df180d8*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.13\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24189\t0.37\tray::DefaultActor.run\n",
      "24182\t0.37\tray::DefaultActor.run\n",
      "24180\t0.36\tray::DefaultActor.run\n",
      "24174\t0.36\tray::DefaultActor\n",
      "24181\t0.35\tray::DefaultActor.run\n",
      "24175\t0.35\tray::DefaultActor.run\n",
      "24176\t0.34\tray::DefaultActor.run\n",
      "24185\t0.34\tray::DefaultActor.run\n",
      "24186\t0.33\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-02-15 18:05:57,911 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: d581abe9f194281cae040ffe01000000, name=DefaultActor.__init__, pid=24187, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950222), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ca86cd785378531d2b883fd9a151213ec5b7e9009ad88559c186049f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ca86cd785378531d2b883fd9a151213ec5b7e9009ad88559c186049f*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24189\t0.31\tray::DefaultActor.run\n",
      "24182\t0.31\tray::DefaultActor.run\n",
      "24186\t0.31\tray::DefaultActor.run\n",
      "24180\t0.31\tray::DefaultActor.run\n",
      "24178\t0.30\tray::DefaultActor.run\n",
      "24184\t0.30\tray::DefaultActor.run\n",
      "24179\t0.30\tray::DefaultActor.run\n",
      "24174\t0.30\tray::DefaultActor.run\n",
      "24185\t0.30\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-02-15 18:05:57,913 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: 1aabde1e63fd801f3a80b1b001000000, name=DefaultActor.__init__, pid=24178, memory used=0.33GB) was running was 7.07GB / 7.44GB (0.950049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5520f083efbdea86985d6fe1a9f8ec4a8d92792d52c2c3639df180d8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-5520f083efbdea86985d6fe1a9f8ec4a8d92792d52c2c3639df180d8*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.13\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24189\t0.37\tray::DefaultActor.run\n",
      "24182\t0.37\tray::DefaultActor.run\n",
      "24180\t0.36\tray::DefaultActor.run\n",
      "24174\t0.36\tray::DefaultActor\n",
      "24181\t0.35\tray::DefaultActor.run\n",
      "24175\t0.35\tray::DefaultActor.run\n",
      "24176\t0.34\tray::DefaultActor.run\n",
      "24185\t0.34\tray::DefaultActor.run\n",
      "24186\t0.33\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "DEBUG flwr 2024-02-15 18:05:57,933 | server.py:187 | evaluate_round 4 received 0 results and 19 failures\n",
      "DEBUG flwr 2024-02-15 18:05:57,934 | server.py:222 | fit_round 5: strategy sampled 19 clients (out of 19)\n",
      "ERROR flwr 2024-02-15 18:05:58,415 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: d581abe9f194281cae040ffe01000000, name=DefaultActor.__init__, pid=24187, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950222), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ca86cd785378531d2b883fd9a151213ec5b7e9009ad88559c186049f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ca86cd785378531d2b883fd9a151213ec5b7e9009ad88559c186049f*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24189\t0.31\tray::DefaultActor.run\n",
      "24182\t0.31\tray::DefaultActor.run\n",
      "24186\t0.31\tray::DefaultActor.run\n",
      "24180\t0.31\tray::DefaultActor.run\n",
      "24178\t0.30\tray::DefaultActor.run\n",
      "24184\t0.30\tray::DefaultActor.run\n",
      "24179\t0.30\tray::DefaultActor.run\n",
      "24174\t0.30\tray::DefaultActor.run\n",
      "24185\t0.30\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-02-15 18:05:58,432 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: d581abe9f194281cae040ffe01000000, name=DefaultActor.__init__, pid=24187, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950222), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ca86cd785378531d2b883fd9a151213ec5b7e9009ad88559c186049f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ca86cd785378531d2b883fd9a151213ec5b7e9009ad88559c186049f*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24189\t0.31\tray::DefaultActor.run\n",
      "24182\t0.31\tray::DefaultActor.run\n",
      "24186\t0.31\tray::DefaultActor.run\n",
      "24180\t0.31\tray::DefaultActor.run\n",
      "24178\t0.30\tray::DefaultActor.run\n",
      "24184\t0.30\tray::DefaultActor.run\n",
      "24179\t0.30\tray::DefaultActor.run\n",
      "24174\t0.30\tray::DefaultActor.run\n",
      "24185\t0.30\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-02-15 18:05:58,466 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: 1aabde1e63fd801f3a80b1b001000000, name=DefaultActor.__init__, pid=24178, memory used=0.33GB) was running was 7.07GB / 7.44GB (0.950049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5520f083efbdea86985d6fe1a9f8ec4a8d92792d52c2c3639df180d8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-5520f083efbdea86985d6fe1a9f8ec4a8d92792d52c2c3639df180d8*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.13\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24189\t0.37\tray::DefaultActor.run\n",
      "24182\t0.37\tray::DefaultActor.run\n",
      "24180\t0.36\tray::DefaultActor.run\n",
      "24174\t0.36\tray::DefaultActor\n",
      "24181\t0.35\tray::DefaultActor.run\n",
      "24175\t0.35\tray::DefaultActor.run\n",
      "24176\t0.34\tray::DefaultActor.run\n",
      "24185\t0.34\tray::DefaultActor.run\n",
      "24186\t0.33\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-02-15 18:05:58,489 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: 1aabde1e63fd801f3a80b1b001000000, name=DefaultActor.__init__, pid=24178, memory used=0.33GB) was running was 7.07GB / 7.44GB (0.950049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5520f083efbdea86985d6fe1a9f8ec4a8d92792d52c2c3639df180d8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-5520f083efbdea86985d6fe1a9f8ec4a8d92792d52c2c3639df180d8*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.13\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24189\t0.37\tray::DefaultActor.run\n",
      "24182\t0.37\tray::DefaultActor.run\n",
      "24180\t0.36\tray::DefaultActor.run\n",
      "24174\t0.36\tray::DefaultActor\n",
      "24181\t0.35\tray::DefaultActor.run\n",
      "24175\t0.35\tray::DefaultActor.run\n",
      "24176\t0.34\tray::DefaultActor.run\n",
      "24185\t0.34\tray::DefaultActor.run\n",
      "24186\t0.33\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-02-15 18:05:58,517 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: b69812a30d49bad7a23056ae01000000, name=DefaultActor.__init__, pid=24177, memory used=0.36GB) was running was 7.07GB / 7.44GB (0.950478), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: eb87cbf60b1e6dc3a911ba590051b73508b005f44b4692d9a1345e71) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-eb87cbf60b1e6dc3a911ba590051b73508b005f44b4692d9a1345e71*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.13\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24189\t0.37\tray::DefaultActor\n",
      "24182\t0.37\tray::DefaultActor\n",
      "24186\t0.37\tray::DefaultActor.run\n",
      "24185\t0.37\tray::DefaultActor.run\n",
      "24175\t0.37\tray::DefaultActor.run\n",
      "24181\t0.37\tray::DefaultActor\n",
      "24179\t0.37\tray::DefaultActor.run\n",
      "24180\t0.37\tray::DefaultActor\n",
      "24172\t0.36\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-02-15 18:05:58,521 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: d581abe9f194281cae040ffe01000000, name=DefaultActor.__init__, pid=24187, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950222), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ca86cd785378531d2b883fd9a151213ec5b7e9009ad88559c186049f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ca86cd785378531d2b883fd9a151213ec5b7e9009ad88559c186049f*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24189\t0.31\tray::DefaultActor.run\n",
      "24182\t0.31\tray::DefaultActor.run\n",
      "24186\t0.31\tray::DefaultActor.run\n",
      "24180\t0.31\tray::DefaultActor.run\n",
      "24178\t0.30\tray::DefaultActor.run\n",
      "24184\t0.30\tray::DefaultActor.run\n",
      "24179\t0.30\tray::DefaultActor.run\n",
      "24174\t0.30\tray::DefaultActor.run\n",
      "24185\t0.30\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-02-15 18:05:58,521 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: b69812a30d49bad7a23056ae01000000, name=DefaultActor.__init__, pid=24177, memory used=0.36GB) was running was 7.07GB / 7.44GB (0.950478), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: eb87cbf60b1e6dc3a911ba590051b73508b005f44b4692d9a1345e71) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-eb87cbf60b1e6dc3a911ba590051b73508b005f44b4692d9a1345e71*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.13\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24189\t0.37\tray::DefaultActor\n",
      "24182\t0.37\tray::DefaultActor\n",
      "24186\t0.37\tray::DefaultActor.run\n",
      "24185\t0.37\tray::DefaultActor.run\n",
      "24175\t0.37\tray::DefaultActor.run\n",
      "24181\t0.37\tray::DefaultActor\n",
      "24179\t0.37\tray::DefaultActor.run\n",
      "24180\t0.37\tray::DefaultActor\n",
      "24172\t0.36\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-02-15 18:05:58,524 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: d581abe9f194281cae040ffe01000000, name=DefaultActor.__init__, pid=24187, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950222), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ca86cd785378531d2b883fd9a151213ec5b7e9009ad88559c186049f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ca86cd785378531d2b883fd9a151213ec5b7e9009ad88559c186049f*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24189\t0.31\tray::DefaultActor.run\n",
      "24182\t0.31\tray::DefaultActor.run\n",
      "24186\t0.31\tray::DefaultActor.run\n",
      "24180\t0.31\tray::DefaultActor.run\n",
      "24178\t0.30\tray::DefaultActor.run\n",
      "24184\t0.30\tray::DefaultActor.run\n",
      "24179\t0.30\tray::DefaultActor.run\n",
      "24174\t0.30\tray::DefaultActor.run\n",
      "24185\t0.30\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-02-15 18:05:58,525 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: 6edfb7c0691ff0c70305db8d01000000, name=DefaultActor.__init__, pid=24189, memory used=0.37GB) was running was 7.22GB / 7.44GB (0.97048), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7d24c495b5bb39a4c54813e900f4c2f98e31d8b9f36138518c6bfa2b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7d24c495b5bb39a4c54813e900f4c2f98e31d8b9f36138518c6bfa2b*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.59\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24186\t0.38\tray::DefaultActor\n",
      "24189\t0.37\tray::DefaultActor\n",
      "24182\t0.37\tray::DefaultActor\n",
      "24179\t0.37\tray::DefaultActor\n",
      "24185\t0.37\tray::DefaultActor\n",
      "24175\t0.37\tray::DefaultActor\n",
      "24181\t0.37\tray::DefaultActor\n",
      "24172\t0.37\tray::DefaultActor\n",
      "24171\t0.37\tray::DefaultActor\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-02-15 18:05:58,534 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: 6edfb7c0691ff0c70305db8d01000000, name=DefaultActor.__init__, pid=24189, memory used=0.37GB) was running was 7.22GB / 7.44GB (0.97048), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7d24c495b5bb39a4c54813e900f4c2f98e31d8b9f36138518c6bfa2b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7d24c495b5bb39a4c54813e900f4c2f98e31d8b9f36138518c6bfa2b*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.59\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24186\t0.38\tray::DefaultActor\n",
      "24189\t0.37\tray::DefaultActor\n",
      "24182\t0.37\tray::DefaultActor\n",
      "24179\t0.37\tray::DefaultActor\n",
      "24185\t0.37\tray::DefaultActor\n",
      "24175\t0.37\tray::DefaultActor\n",
      "24181\t0.37\tray::DefaultActor\n",
      "24172\t0.37\tray::DefaultActor\n",
      "24171\t0.37\tray::DefaultActor\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-02-15 18:05:58,534 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: 1aabde1e63fd801f3a80b1b001000000, name=DefaultActor.__init__, pid=24178, memory used=0.33GB) was running was 7.07GB / 7.44GB (0.950049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5520f083efbdea86985d6fe1a9f8ec4a8d92792d52c2c3639df180d8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-5520f083efbdea86985d6fe1a9f8ec4a8d92792d52c2c3639df180d8*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.13\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24189\t0.37\tray::DefaultActor.run\n",
      "24182\t0.37\tray::DefaultActor.run\n",
      "24180\t0.36\tray::DefaultActor.run\n",
      "24174\t0.36\tray::DefaultActor\n",
      "24181\t0.35\tray::DefaultActor.run\n",
      "24175\t0.35\tray::DefaultActor.run\n",
      "24176\t0.34\tray::DefaultActor.run\n",
      "24185\t0.34\tray::DefaultActor.run\n",
      "24186\t0.33\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-02-15 18:05:58,534 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: 8080131699fdec87227ce5a101000000, name=DefaultActor.__init__, pid=24181, memory used=0.39GB) was running was 7.07GB / 7.44GB (0.950481), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8f9f4202b7c08e54248e55344f63a4dff0d1bcebccfc900a981c5fe4) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-8f9f4202b7c08e54248e55344f63a4dff0d1bcebccfc900a981c5fe4*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.84\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24171\t0.40\tray::DefaultActor.run\n",
      "24182\t0.40\tray::DefaultActor.run\n",
      "24185\t0.39\tray::DefaultActor.run\n",
      "24176\t0.39\tray::DefaultActor.run\n",
      "24175\t0.39\tray::DefaultActor.run\n",
      "24186\t0.39\tray::DefaultActor.run\n",
      "24181\t0.39\tray::DefaultActor.run\n",
      "24172\t0.39\tray::DefaultActor.run\n",
      "24179\t0.39\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-02-15 18:05:58,539 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: 1aabde1e63fd801f3a80b1b001000000, name=DefaultActor.__init__, pid=24178, memory used=0.33GB) was running was 7.07GB / 7.44GB (0.950049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5520f083efbdea86985d6fe1a9f8ec4a8d92792d52c2c3639df180d8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-5520f083efbdea86985d6fe1a9f8ec4a8d92792d52c2c3639df180d8*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.13\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24189\t0.37\tray::DefaultActor.run\n",
      "24182\t0.37\tray::DefaultActor.run\n",
      "24180\t0.36\tray::DefaultActor.run\n",
      "24174\t0.36\tray::DefaultActor\n",
      "24181\t0.35\tray::DefaultActor.run\n",
      "24175\t0.35\tray::DefaultActor.run\n",
      "24176\t0.34\tray::DefaultActor.run\n",
      "24185\t0.34\tray::DefaultActor.run\n",
      "24186\t0.33\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-02-15 18:05:58,539 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: c15383f49961ecb4a37d265501000000, name=DefaultActor.__init__, pid=24180, memory used=0.37GB) was running was 7.12GB / 7.44GB (0.95787), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: b5d7f31e8e1c5600a4998934a7df6c8f6b9f159ae5138dfa91eb3fcf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-b5d7f31e8e1c5600a4998934a7df6c8f6b9f159ae5138dfa91eb3fcf*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.83\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24186\t0.38\tray::DefaultActor.run\n",
      "24171\t0.38\tray::DefaultActor.run\n",
      "24172\t0.37\tray::DefaultActor.run\n",
      "24175\t0.37\tray::DefaultActor.run\n",
      "24182\t0.37\tray::DefaultActor\n",
      "24181\t0.37\tray::DefaultActor.run\n",
      "24179\t0.37\tray::DefaultActor\n",
      "24180\t0.37\tray::DefaultActor.run\n",
      "24185\t0.37\tray::DefaultActor\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-02-15 18:05:58,541 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: 8080131699fdec87227ce5a101000000, name=DefaultActor.__init__, pid=24181, memory used=0.39GB) was running was 7.07GB / 7.44GB (0.950481), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8f9f4202b7c08e54248e55344f63a4dff0d1bcebccfc900a981c5fe4) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-8f9f4202b7c08e54248e55344f63a4dff0d1bcebccfc900a981c5fe4*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.84\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24171\t0.40\tray::DefaultActor.run\n",
      "24182\t0.40\tray::DefaultActor.run\n",
      "24185\t0.39\tray::DefaultActor.run\n",
      "24176\t0.39\tray::DefaultActor.run\n",
      "24175\t0.39\tray::DefaultActor.run\n",
      "24186\t0.39\tray::DefaultActor.run\n",
      "24181\t0.39\tray::DefaultActor.run\n",
      "24172\t0.39\tray::DefaultActor.run\n",
      "24179\t0.39\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-02-15 18:05:58,549 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: 388fabd5122dfa924f937a4401000000, name=DefaultActor.__init__, pid=24184, memory used=0.33GB) was running was 7.07GB / 7.44GB (0.950416), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7183a21e8c6dc6d32f920bd6e9b8a0be3c757fc658e03caf33a1bb11) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7183a21e8c6dc6d32f920bd6e9b8a0be3c757fc658e03caf33a1bb11*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.12\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24184\t0.33\tray::DefaultActor.run\n",
      "24186\t0.32\tray::DefaultActor.run\n",
      "24175\t0.32\tray::DefaultActor.run\n",
      "24189\t0.32\tray::DefaultActor.run\n",
      "24181\t0.32\tray::DefaultActor.run\n",
      "24172\t0.32\tray::DefaultActor.run\n",
      "24179\t0.32\tray::DefaultActor.run\n",
      "24185\t0.32\tray::DefaultActor.run\n",
      "24182\t0.32\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-02-15 18:05:58,550 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: c15383f49961ecb4a37d265501000000, name=DefaultActor.__init__, pid=24180, memory used=0.37GB) was running was 7.12GB / 7.44GB (0.95787), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: b5d7f31e8e1c5600a4998934a7df6c8f6b9f159ae5138dfa91eb3fcf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-b5d7f31e8e1c5600a4998934a7df6c8f6b9f159ae5138dfa91eb3fcf*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.83\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24186\t0.38\tray::DefaultActor.run\n",
      "24171\t0.38\tray::DefaultActor.run\n",
      "24172\t0.37\tray::DefaultActor.run\n",
      "24175\t0.37\tray::DefaultActor.run\n",
      "24182\t0.37\tray::DefaultActor\n",
      "24181\t0.37\tray::DefaultActor.run\n",
      "24179\t0.37\tray::DefaultActor\n",
      "24180\t0.37\tray::DefaultActor.run\n",
      "24185\t0.37\tray::DefaultActor\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-02-15 18:05:58,550 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: 6edfb7c0691ff0c70305db8d01000000, name=DefaultActor.__init__, pid=24189, memory used=0.37GB) was running was 7.22GB / 7.44GB (0.97048), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7d24c495b5bb39a4c54813e900f4c2f98e31d8b9f36138518c6bfa2b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7d24c495b5bb39a4c54813e900f4c2f98e31d8b9f36138518c6bfa2b*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.59\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24186\t0.38\tray::DefaultActor\n",
      "24189\t0.37\tray::DefaultActor\n",
      "24182\t0.37\tray::DefaultActor\n",
      "24179\t0.37\tray::DefaultActor\n",
      "24185\t0.37\tray::DefaultActor\n",
      "24175\t0.37\tray::DefaultActor\n",
      "24181\t0.37\tray::DefaultActor\n",
      "24172\t0.37\tray::DefaultActor\n",
      "24171\t0.37\tray::DefaultActor\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-02-15 18:05:58,554 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: 388fabd5122dfa924f937a4401000000, name=DefaultActor.__init__, pid=24184, memory used=0.33GB) was running was 7.07GB / 7.44GB (0.950416), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7183a21e8c6dc6d32f920bd6e9b8a0be3c757fc658e03caf33a1bb11) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7183a21e8c6dc6d32f920bd6e9b8a0be3c757fc658e03caf33a1bb11*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.12\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24184\t0.33\tray::DefaultActor.run\n",
      "24186\t0.32\tray::DefaultActor.run\n",
      "24175\t0.32\tray::DefaultActor.run\n",
      "24189\t0.32\tray::DefaultActor.run\n",
      "24181\t0.32\tray::DefaultActor.run\n",
      "24172\t0.32\tray::DefaultActor.run\n",
      "24179\t0.32\tray::DefaultActor.run\n",
      "24185\t0.32\tray::DefaultActor.run\n",
      "24182\t0.32\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-02-15 18:05:58,556 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: 6edfb7c0691ff0c70305db8d01000000, name=DefaultActor.__init__, pid=24189, memory used=0.37GB) was running was 7.22GB / 7.44GB (0.97048), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7d24c495b5bb39a4c54813e900f4c2f98e31d8b9f36138518c6bfa2b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7d24c495b5bb39a4c54813e900f4c2f98e31d8b9f36138518c6bfa2b*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.59\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24186\t0.38\tray::DefaultActor\n",
      "24189\t0.37\tray::DefaultActor\n",
      "24182\t0.37\tray::DefaultActor\n",
      "24179\t0.37\tray::DefaultActor\n",
      "24185\t0.37\tray::DefaultActor\n",
      "24175\t0.37\tray::DefaultActor\n",
      "24181\t0.37\tray::DefaultActor\n",
      "24172\t0.37\tray::DefaultActor\n",
      "24171\t0.37\tray::DefaultActor\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(DefaultActor pid=24171)\u001b[0m 2\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24171)\u001b[0m starting client: 2 <class 'str'>\u001b[32m [repeated 19x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24171)\u001b[0m starting client:  <class 'int'>\u001b[32m [repeated 19x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24171)\u001b[0m Creating client model to client: 2\u001b[32m [repeated 19x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24171)\u001b[0m Data X: 3383\u001b[32m [repeated 19x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24171)\u001b[0m Data Y: 3383\u001b[32m [repeated 19x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24171)\u001b[0m Creating client model to client: 2 round 0\u001b[32m [repeated 19x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2024-02-15 18:06:16,313 | server.py:236 | fit_round 5 received 9 results and 10 failures\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving round 5 aggregated_ndarrays...\n",
      "result_unbalanced_epoch_1_rounds_5_cycle_1/checkpoints/round-5-weights.npz\n",
      "\n",
      "Evaluating global model round 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO flwr 2024-02-15 18:06:23,093 | server.py:125 | fit progress: (5, 0.8721485733985901, {'accuracy': 0.7343499660491943}, 137.3044667480001)\n",
      "DEBUG flwr 2024-02-15 18:06:23,095 | server.py:173 | evaluate_round 5: strategy sampled 19 clients (out of 19)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(DefaultActor pid=24182)\u001b[0m /home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py:72: DeprecationWarning:  Ensure your client is of type `flwr.client.Client`. Please convert it using the `.to_client()` method before returning it in the `client_fn` you pass to `start_simulation`. We have applied this conversion on your behalf. Not returning a `Client` might trigger an error in future versions of Flower.\u001b[32m [repeated 21x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24182)\u001b[0m   client = check_clientfn_returns_client(client_fn(cid))\u001b[32m [repeated 21x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(DefaultActor pid=24185)\u001b[0m 2\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24182)\u001b[0m starting client: 15 <class 'str'>\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24182)\u001b[0m starting client:  <class 'int'>\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24182)\u001b[0m Creating client model to client: 15\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24182)\u001b[0m Data X: 21669\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24182)\u001b[0m Data Y: 21669\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24182)\u001b[0m Creating client model to client: 15 round 0\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24179)\u001b[0m starting client: 5 <class 'str'>\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24179)\u001b[0m starting client:  <class 'int'>\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24179)\u001b[0m Creating client model to client: 5\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24179)\u001b[0m Data X: 26567\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24179)\u001b[0m Data Y: 26567\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24179)\u001b[0m Creating client model to client: 5 round 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-02-15 18:06:23,510 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: 1aabde1e63fd801f3a80b1b001000000, name=DefaultActor.__init__, pid=24178, memory used=0.33GB) was running was 7.07GB / 7.44GB (0.950049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5520f083efbdea86985d6fe1a9f8ec4a8d92792d52c2c3639df180d8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-5520f083efbdea86985d6fe1a9f8ec4a8d92792d52c2c3639df180d8*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.13\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24189\t0.37\tray::DefaultActor.run\n",
      "24182\t0.37\tray::DefaultActor.run\n",
      "24180\t0.36\tray::DefaultActor.run\n",
      "24174\t0.36\tray::DefaultActor\n",
      "24181\t0.35\tray::DefaultActor.run\n",
      "24175\t0.35\tray::DefaultActor.run\n",
      "24176\t0.34\tray::DefaultActor.run\n",
      "24185\t0.34\tray::DefaultActor.run\n",
      "24186\t0.33\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-02-15 18:06:23,511 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: 1aabde1e63fd801f3a80b1b001000000, name=DefaultActor.__init__, pid=24178, memory used=0.33GB) was running was 7.07GB / 7.44GB (0.950049), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5520f083efbdea86985d6fe1a9f8ec4a8d92792d52c2c3639df180d8) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-5520f083efbdea86985d6fe1a9f8ec4a8d92792d52c2c3639df180d8*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.13\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24189\t0.37\tray::DefaultActor.run\n",
      "24182\t0.37\tray::DefaultActor.run\n",
      "24180\t0.36\tray::DefaultActor.run\n",
      "24174\t0.36\tray::DefaultActor\n",
      "24181\t0.35\tray::DefaultActor.run\n",
      "24175\t0.35\tray::DefaultActor.run\n",
      "24176\t0.34\tray::DefaultActor.run\n",
      "24185\t0.34\tray::DefaultActor.run\n",
      "24186\t0.33\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-02-15 18:06:23,515 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: 6edfb7c0691ff0c70305db8d01000000, name=DefaultActor.__init__, pid=24189, memory used=0.37GB) was running was 7.22GB / 7.44GB (0.97048), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7d24c495b5bb39a4c54813e900f4c2f98e31d8b9f36138518c6bfa2b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7d24c495b5bb39a4c54813e900f4c2f98e31d8b9f36138518c6bfa2b*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.59\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24186\t0.38\tray::DefaultActor\n",
      "24189\t0.37\tray::DefaultActor\n",
      "24182\t0.37\tray::DefaultActor\n",
      "24179\t0.37\tray::DefaultActor\n",
      "24185\t0.37\tray::DefaultActor\n",
      "24175\t0.37\tray::DefaultActor\n",
      "24181\t0.37\tray::DefaultActor\n",
      "24172\t0.37\tray::DefaultActor\n",
      "24171\t0.37\tray::DefaultActor\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-02-15 18:06:23,516 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: 388fabd5122dfa924f937a4401000000, name=DefaultActor.__init__, pid=24184, memory used=0.33GB) was running was 7.07GB / 7.44GB (0.950416), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7183a21e8c6dc6d32f920bd6e9b8a0be3c757fc658e03caf33a1bb11) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7183a21e8c6dc6d32f920bd6e9b8a0be3c757fc658e03caf33a1bb11*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.12\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24184\t0.33\tray::DefaultActor.run\n",
      "24186\t0.32\tray::DefaultActor.run\n",
      "24175\t0.32\tray::DefaultActor.run\n",
      "24189\t0.32\tray::DefaultActor.run\n",
      "24181\t0.32\tray::DefaultActor.run\n",
      "24172\t0.32\tray::DefaultActor.run\n",
      "24179\t0.32\tray::DefaultActor.run\n",
      "24185\t0.32\tray::DefaultActor.run\n",
      "24182\t0.32\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-02-15 18:06:23,516 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: c15383f49961ecb4a37d265501000000, name=DefaultActor.__init__, pid=24180, memory used=0.37GB) was running was 7.12GB / 7.44GB (0.95787), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: b5d7f31e8e1c5600a4998934a7df6c8f6b9f159ae5138dfa91eb3fcf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-b5d7f31e8e1c5600a4998934a7df6c8f6b9f159ae5138dfa91eb3fcf*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.83\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24186\t0.38\tray::DefaultActor.run\n",
      "24171\t0.38\tray::DefaultActor.run\n",
      "24172\t0.37\tray::DefaultActor.run\n",
      "24175\t0.37\tray::DefaultActor.run\n",
      "24182\t0.37\tray::DefaultActor\n",
      "24181\t0.37\tray::DefaultActor.run\n",
      "24179\t0.37\tray::DefaultActor\n",
      "24180\t0.37\tray::DefaultActor.run\n",
      "24185\t0.37\tray::DefaultActor\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-02-15 18:06:23,517 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: 6edfb7c0691ff0c70305db8d01000000, name=DefaultActor.__init__, pid=24189, memory used=0.37GB) was running was 7.22GB / 7.44GB (0.97048), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7d24c495b5bb39a4c54813e900f4c2f98e31d8b9f36138518c6bfa2b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7d24c495b5bb39a4c54813e900f4c2f98e31d8b9f36138518c6bfa2b*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.59\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24186\t0.38\tray::DefaultActor\n",
      "24189\t0.37\tray::DefaultActor\n",
      "24182\t0.37\tray::DefaultActor\n",
      "24179\t0.37\tray::DefaultActor\n",
      "24185\t0.37\tray::DefaultActor\n",
      "24175\t0.37\tray::DefaultActor\n",
      "24181\t0.37\tray::DefaultActor\n",
      "24172\t0.37\tray::DefaultActor\n",
      "24171\t0.37\tray::DefaultActor\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-02-15 18:06:23,519 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: 388fabd5122dfa924f937a4401000000, name=DefaultActor.__init__, pid=24184, memory used=0.33GB) was running was 7.07GB / 7.44GB (0.950416), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 7183a21e8c6dc6d32f920bd6e9b8a0be3c757fc658e03caf33a1bb11) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-7183a21e8c6dc6d32f920bd6e9b8a0be3c757fc658e03caf33a1bb11*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.12\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24184\t0.33\tray::DefaultActor.run\n",
      "24186\t0.32\tray::DefaultActor.run\n",
      "24175\t0.32\tray::DefaultActor.run\n",
      "24189\t0.32\tray::DefaultActor.run\n",
      "24181\t0.32\tray::DefaultActor.run\n",
      "24172\t0.32\tray::DefaultActor.run\n",
      "24179\t0.32\tray::DefaultActor.run\n",
      "24185\t0.32\tray::DefaultActor.run\n",
      "24182\t0.32\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-02-15 18:06:23,521 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: c15383f49961ecb4a37d265501000000, name=DefaultActor.__init__, pid=24180, memory used=0.37GB) was running was 7.12GB / 7.44GB (0.95787), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: b5d7f31e8e1c5600a4998934a7df6c8f6b9f159ae5138dfa91eb3fcf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-b5d7f31e8e1c5600a4998934a7df6c8f6b9f159ae5138dfa91eb3fcf*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.83\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24186\t0.38\tray::DefaultActor.run\n",
      "24171\t0.38\tray::DefaultActor.run\n",
      "24172\t0.37\tray::DefaultActor.run\n",
      "24175\t0.37\tray::DefaultActor.run\n",
      "24182\t0.37\tray::DefaultActor\n",
      "24181\t0.37\tray::DefaultActor.run\n",
      "24179\t0.37\tray::DefaultActor\n",
      "24180\t0.37\tray::DefaultActor.run\n",
      "24185\t0.37\tray::DefaultActor\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-02-15 18:06:23,523 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: d581abe9f194281cae040ffe01000000, name=DefaultActor.__init__, pid=24187, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950222), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ca86cd785378531d2b883fd9a151213ec5b7e9009ad88559c186049f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ca86cd785378531d2b883fd9a151213ec5b7e9009ad88559c186049f*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24189\t0.31\tray::DefaultActor.run\n",
      "24182\t0.31\tray::DefaultActor.run\n",
      "24186\t0.31\tray::DefaultActor.run\n",
      "24180\t0.31\tray::DefaultActor.run\n",
      "24178\t0.30\tray::DefaultActor.run\n",
      "24184\t0.30\tray::DefaultActor.run\n",
      "24179\t0.30\tray::DefaultActor.run\n",
      "24174\t0.30\tray::DefaultActor.run\n",
      "24185\t0.30\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-02-15 18:06:23,526 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: 8080131699fdec87227ce5a101000000, name=DefaultActor.__init__, pid=24181, memory used=0.39GB) was running was 7.07GB / 7.44GB (0.950481), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8f9f4202b7c08e54248e55344f63a4dff0d1bcebccfc900a981c5fe4) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-8f9f4202b7c08e54248e55344f63a4dff0d1bcebccfc900a981c5fe4*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.84\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24171\t0.40\tray::DefaultActor.run\n",
      "24182\t0.40\tray::DefaultActor.run\n",
      "24185\t0.39\tray::DefaultActor.run\n",
      "24176\t0.39\tray::DefaultActor.run\n",
      "24175\t0.39\tray::DefaultActor.run\n",
      "24186\t0.39\tray::DefaultActor.run\n",
      "24181\t0.39\tray::DefaultActor.run\n",
      "24172\t0.39\tray::DefaultActor.run\n",
      "24179\t0.39\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-02-15 18:06:23,527 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: d581abe9f194281cae040ffe01000000, name=DefaultActor.__init__, pid=24187, memory used=0.30GB) was running was 7.07GB / 7.44GB (0.950222), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ca86cd785378531d2b883fd9a151213ec5b7e9009ad88559c186049f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-ca86cd785378531d2b883fd9a151213ec5b7e9009ad88559c186049f*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.10\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24189\t0.31\tray::DefaultActor.run\n",
      "24182\t0.31\tray::DefaultActor.run\n",
      "24186\t0.31\tray::DefaultActor.run\n",
      "24180\t0.31\tray::DefaultActor.run\n",
      "24178\t0.30\tray::DefaultActor.run\n",
      "24184\t0.30\tray::DefaultActor.run\n",
      "24179\t0.30\tray::DefaultActor.run\n",
      "24174\t0.30\tray::DefaultActor.run\n",
      "24185\t0.30\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "ERROR flwr 2024-02-15 18:06:23,529 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: 8080131699fdec87227ce5a101000000, name=DefaultActor.__init__, pid=24181, memory used=0.39GB) was running was 7.07GB / 7.44GB (0.950481), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8f9f4202b7c08e54248e55344f63a4dff0d1bcebccfc900a981c5fe4) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-8f9f4202b7c08e54248e55344f63a4dff0d1bcebccfc900a981c5fe4*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.84\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24171\t0.40\tray::DefaultActor.run\n",
      "24182\t0.40\tray::DefaultActor.run\n",
      "24185\t0.39\tray::DefaultActor.run\n",
      "24176\t0.39\tray::DefaultActor.run\n",
      "24175\t0.39\tray::DefaultActor.run\n",
      "24186\t0.39\tray::DefaultActor.run\n",
      "24181\t0.39\tray::DefaultActor.run\n",
      "24172\t0.39\tray::DefaultActor.run\n",
      "24179\t0.39\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flwr 2024-02-15 18:06:23,531 | ray_client_proxy.py:161 | Traceback (most recent call last):\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 151, in _submit_job\n",
      "    res, updated_context = self.actor_pool.get_client_result(self.cid, timeout)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 425, in get_client_result\n",
      "    return self._fetch_future_result(cid)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 306, in _fetch_future_result\n",
      "    res_cid, res, updated_context = ray.get(\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/ray/_private/worker.py\", line 2526, in get\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: b69812a30d49bad7a23056ae01000000, name=DefaultActor.__init__, pid=24177, memory used=0.36GB) was running was 7.07GB / 7.44GB (0.950478), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: eb87cbf60b1e6dc3a911ba590051b73508b005f44b4692d9a1345e71) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-eb87cbf60b1e6dc3a911ba590051b73508b005f44b4692d9a1345e71*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.13\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24189\t0.37\tray::DefaultActor\n",
      "24182\t0.37\tray::DefaultActor\n",
      "24186\t0.37\tray::DefaultActor.run\n",
      "24185\t0.37\tray::DefaultActor.run\n",
      "24175\t0.37\tray::DefaultActor.run\n",
      "24181\t0.37\tray::DefaultActor\n",
      "24179\t0.37\tray::DefaultActor.run\n",
      "24180\t0.37\tray::DefaultActor\n",
      "24172\t0.36\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "ERROR flwr 2024-02-15 18:06:23,533 | ray_client_proxy.py:162 | Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.30.126.159, ID: 6a6218aaeb17a738f81f0d76aa128037ca9b5ff2352dd37d55158a67) where the task (actor ID: b69812a30d49bad7a23056ae01000000, name=DefaultActor.__init__, pid=24177, memory used=0.36GB) was running was 7.07GB / 7.44GB (0.950478), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: eb87cbf60b1e6dc3a911ba590051b73508b005f44b4692d9a1345e71) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.30.126.159`. To see the logs of the worker, use `ray logs worker-eb87cbf60b1e6dc3a911ba590051b73508b005f44b4692d9a1345e71*out -ip 172.30.126.159. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "23616\t1.13\t/home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/bin/python -m ipykernel_launcher -f /h...\n",
      "24189\t0.37\tray::DefaultActor\n",
      "24182\t0.37\tray::DefaultActor\n",
      "24186\t0.37\tray::DefaultActor.run\n",
      "24185\t0.37\tray::DefaultActor.run\n",
      "24175\t0.37\tray::DefaultActor.run\n",
      "24181\t0.37\tray::DefaultActor\n",
      "24179\t0.37\tray::DefaultActor.run\n",
      "24180\t0.37\tray::DefaultActor\n",
      "24172\t0.36\tray::DefaultActor.run\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "DEBUG flwr 2024-02-15 18:06:23,787 | server.py:187 | evaluate_round 5 received 0 results and 19 failures\n",
      "INFO flwr 2024-02-15 18:06:23,789 | server.py:153 | FL finished in 137.999889382\n",
      "INFO flwr 2024-02-15 18:06:23,790 | app.py:226 | app_fit: losses_distributed []\n",
      "INFO flwr 2024-02-15 18:06:23,792 | app.py:227 | app_fit: metrics_distributed_fit {}\n",
      "INFO flwr 2024-02-15 18:06:23,793 | app.py:228 | app_fit: metrics_distributed {}\n",
      "INFO flwr 2024-02-15 18:06:23,795 | app.py:229 | app_fit: losses_centralized [(0, 0.6753955483436584), (1, 0.7319059371948242), (2, 0.8276991248130798), (3, 0.8657560348510742), (4, 0.9044705033302307), (5, 0.8721485733985901)]\n",
      "INFO flwr 2024-02-15 18:06:23,796 | app.py:230 | app_fit: metrics_centralized {'accuracy': [(0, 0.6329399347305298), (1, 0.7041471600532532), (2, 0.7227329611778259), (3, 0.7227032780647278), (4, 0.7454184293746948), (5, 0.7343499660491943)]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "History (loss, centralized):\n",
       "\tround 0: 0.6753955483436584\n",
       "\tround 1: 0.7319059371948242\n",
       "\tround 2: 0.8276991248130798\n",
       "\tround 3: 0.8657560348510742\n",
       "\tround 4: 0.9044705033302307\n",
       "\tround 5: 0.8721485733985901\n",
       "History (metrics, centralized):\n",
       "{'accuracy': [(0, 0.6329399347305298), (1, 0.7041471600532532), (2, 0.7227329611778259), (3, 0.7227032780647278), (4, 0.7454184293746948), (5, 0.7343499660491943)]}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ray\n",
    "import math\n",
    "# Create an instance of the model and get the parameters\n",
    "\n",
    "# Specify client resources if you need GPU (defaults to 1 CPU and 0 GPU)\n",
    "client_resources = None\n",
    "#if DEVICE.type == \"cuda\":\n",
    "\n",
    "client_resources = {\"num_cpus\": 1}\n",
    "\n",
    "#keras_model = create_keras_model()\n",
    "keras_model.compile(\"adam\", \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "\n",
    "def client_fn(cid) -> FlowerISABELASleepClient:\n",
    "    print(\"starting client: \"+str(cid),type(cid))\n",
    "    #convert client ID to int\n",
    "    clientId = int(cid)\n",
    "    print(\"starting client: \", type(clientId))\n",
    "\n",
    "    data   = clientList[clientId][inputFeatures]\n",
    "    labels = clientList[clientId][outputClasses]\n",
    "    \n",
    "    print(\"Creating client model to client: \"+str(cid))\n",
    "    print(\"Data X: \"+str(len(data)))\n",
    "    print(\"Data Y: \"+str(len(labels)))\n",
    "    \n",
    "    file_global_model = outputFolder+\"/global_model_metrics_after_agregation.csv\"\n",
    "    index_round = 0 \n",
    "    \n",
    "    # get last\n",
    "    if(os.path.isfile(file_global_model)):\n",
    "        dataset = pd.read_csv(file_global_model)\n",
    "        index_round = dataset[\"round\"].max() + 1\n",
    "        del dataset\n",
    "    \n",
    "    # update the index round in the previous checkpoint\n",
    "    if(last_round_checkpoint > -1 and (index_round == last_round_checkpoint)):\n",
    "        index_round = last_round_checkpoint\n",
    "    \n",
    "    print(\"Creating client model to client: \"+str(cid),\"round\",index_round)\n",
    "    # Load and compile a Keras model for CIFAR-10\n",
    "    model = create_keras_model()\n",
    "    model.compile(\"adam\", \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    \n",
    "    proxyClient = FlowerISABELASleepClient(clientId,model,data,labels,index_round)\n",
    "    \n",
    "    return proxyClient\n",
    "\n",
    "strategy = SaveModelStrategy(\n",
    "    min_available_clients=NUMBER_OF_CLIENTS,\n",
    "    evaluate_fn=get_evaluate_fn(keras_model)\n",
    ") # (same arguments as FedAvg here)\n",
    "\n",
    "# load checkpoint\n",
    "if(filename_h5 is not None):\n",
    "    \n",
    "    #npzFile = np.load(filename_np)\n",
    "    keras_model.load_weights(filename_h5)\n",
    "    \n",
    "    initial_parameters = keras_model.get_weights() \n",
    "    # Convert the weights (np.ndarray) to parameters (bytes)\n",
    "    init_param = fl.common.ndarrays_to_parameters(initial_parameters)\n",
    "\n",
    "    strategy = SaveModelStrategy(\n",
    "        min_available_clients=NUMBER_OF_CLIENTS,\n",
    "        evaluate_fn=get_evaluate_fn(keras_model),\n",
    "        initial_parameters = init_param\n",
    "    ) # (same arguments as FedAvg here)\n",
    "\n",
    "# Start simulation\n",
    "fl.simulation.start_simulation(\n",
    "    client_fn=client_fn,\n",
    "    num_clients=NUMBER_OF_CLIENTS,\n",
    "    config=fl.server.ServerConfig(num_rounds=NUMBER_OF_ITERATIONS),  # Just three rounds\n",
    "    client_resources=client_resources,\n",
    "    strategy = strategy\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8f400a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7de2c363",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(DefaultActor pid=24182)\u001b[0m /home/guilherme/cpu-tensorflow-marcelo/nvidia-smi/envs/flower/lib/python3.9/site-packages/flwr/simulation/ray_transport/ray_actor.py:72: DeprecationWarning:  Ensure your client is of type `flwr.client.Client`. Please convert it using the `.to_client()` method before returning it in the `client_fn` you pass to `start_simulation`. We have applied this conversion on your behalf. Not returning a `Client` might trigger an error in future versions of Flower.\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=24182)\u001b[0m   client = check_clientfn_returns_client(client_fn(cid))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activity</th>\n",
       "      <th>location</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>light</th>\n",
       "      <th>phone_lock</th>\n",
       "      <th>proximity</th>\n",
       "      <th>sound</th>\n",
       "      <th>time_to_next_alarm</th>\n",
       "      <th>minutes_day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.982143</td>\n",
       "      <td>0.597637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.982143</td>\n",
       "      <td>0.597637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000583</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.604408</td>\n",
       "      <td>0.982044</td>\n",
       "      <td>0.598332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005117</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.604408</td>\n",
       "      <td>0.982044</td>\n",
       "      <td>0.598332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.601849</td>\n",
       "      <td>0.981944</td>\n",
       "      <td>0.599027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17988</th>\n",
       "      <td>0.25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.586128</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>0.551077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17989</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.572395</td>\n",
       "      <td>0.983234</td>\n",
       "      <td>0.551772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17990</th>\n",
       "      <td>0.75</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.572395</td>\n",
       "      <td>0.983135</td>\n",
       "      <td>0.552467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17991</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.615209</td>\n",
       "      <td>0.983135</td>\n",
       "      <td>0.553162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17992</th>\n",
       "      <td>0.25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.615209</td>\n",
       "      <td>0.983036</td>\n",
       "      <td>0.553857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17993 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       activity  location  day_of_week     light  phone_lock  proximity  \\\n",
       "0          0.00       0.0     0.000000  0.000617         0.0        1.0   \n",
       "1          0.00       0.0     0.000000  0.000617         0.0        1.0   \n",
       "2          0.25       0.5     0.000000  0.000583         0.0        1.0   \n",
       "3          0.25       0.5     0.000000  0.005117         0.0        1.0   \n",
       "4          0.25       0.5     0.000000  0.000700         0.0        1.0   \n",
       "...         ...       ...          ...       ...         ...        ...   \n",
       "17988      0.25       1.0     0.166667  0.000000         1.0        0.0   \n",
       "17989      0.00       1.0     0.166667  0.000000         1.0        0.0   \n",
       "17990      0.75       1.0     0.166667  0.000000         1.0        0.0   \n",
       "17991      0.00       1.0     0.166667  0.000000         1.0        0.0   \n",
       "17992      0.25       1.0     0.166667  0.000000         1.0        0.0   \n",
       "\n",
       "          sound  time_to_next_alarm  minutes_day  \n",
       "0      0.000000            0.982143     0.597637  \n",
       "1      0.000000            0.982143     0.597637  \n",
       "2      0.604408            0.982044     0.598332  \n",
       "3      0.604408            0.982044     0.598332  \n",
       "4      0.601849            0.981944     0.599027  \n",
       "...         ...                 ...          ...  \n",
       "17988  0.586128            0.983333     0.551077  \n",
       "17989  0.572395            0.983234     0.551772  \n",
       "17990  0.572395            0.983135     0.552467  \n",
       "17991  0.615209            0.983135     0.553162  \n",
       "17992  0.615209            0.983036     0.553857  \n",
       "\n",
       "[17993 rows x 9 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clientList[0][inputFeatures]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "4bc76a55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>awake</th>\n",
       "      <th>asleep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17988</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17989</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17990</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17991</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17992</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17993 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       awake  asleep\n",
       "0      False    True\n",
       "1      False    True\n",
       "2      False    True\n",
       "3      False    True\n",
       "4      False    True\n",
       "...      ...     ...\n",
       "17988  False    True\n",
       "17989  False    True\n",
       "17990  False    True\n",
       "17991  False    True\n",
       "17992  False    True\n",
       "\n",
       "[17993 rows x 2 columns]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clientList[0][outputClasses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e25c20d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e2d7f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
